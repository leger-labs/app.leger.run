{
  "cloudModelCount": 130,
  "localModelCount": 27,
  "makerCount": 15,
  "providerCount": 24,
  "cloudModels": [
    {
      "id": "claude-3-haiku",
      "name": "Claude 3 Haiku",
      "maker": "anthropic",
      "providers": [
        {
          "id": "anthropic",
          "litellm_model_name": "anthropic/claude-3-haiku",
          "is_default": true
        },
        {
          "id": "aws-bedrock",
          "litellm_model_name": "bedrock/anthropic.claude-3-haiku",
          "is_default": false
        },
        {
          "id": "vertex-ai",
          "litellm_model_name": "vertex_ai/claude-3-haiku",
          "is_default": false
        }
      ],
      "icon": "@/assets/icons/anthropic.svg",
      "description": "Anthropic's Claude 3 Haiku model with a 200K tokens context window available via Anthropic, AWS Bedrock, Vertex AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.25",
        "output_per_1m": "$1.25",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 200000,
      "max_output": 50000
    },
    {
      "id": "claude-3-opus",
      "name": "Claude 3 Opus",
      "maker": "anthropic",
      "providers": [
        {
          "id": "anthropic",
          "litellm_model_name": "anthropic/claude-3-opus",
          "is_default": true
        },
        {
          "id": "aws-bedrock",
          "litellm_model_name": "bedrock/anthropic.claude-3-opus",
          "is_default": false
        },
        {
          "id": "vertex-ai",
          "litellm_model_name": "vertex_ai/claude-3-opus",
          "is_default": false
        }
      ],
      "icon": "@/assets/icons/anthropic.svg",
      "description": "Anthropic's Claude 3 Opus model with a 200K tokens context window available via Anthropic, AWS Bedrock, Vertex AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$15.00",
        "output_per_1m": "$75.00",
        "tier": "premium"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 200000,
      "max_output": 50000
    },
    {
      "id": "claude-3.5-haiku",
      "name": "Claude 3.5 Haiku",
      "maker": "anthropic",
      "providers": [
        {
          "id": "anthropic",
          "litellm_model_name": "anthropic/claude-3.5-haiku",
          "is_default": true
        },
        {
          "id": "aws-bedrock",
          "litellm_model_name": "bedrock/anthropic.claude-3.5-haiku",
          "is_default": false
        },
        {
          "id": "vertex-ai",
          "litellm_model_name": "vertex_ai/claude-3.5-haiku",
          "is_default": false
        }
      ],
      "icon": "@/assets/icons/anthropic.svg",
      "description": "Anthropic's Claude 3.5 Haiku model with a 200K tokens context window available via Anthropic, AWS Bedrock, Vertex AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.80",
        "output_per_1m": "$4.00",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 200000,
      "max_output": 50000
    },
    {
      "id": "claude-3.5-sonnet",
      "name": "Claude 3.5 Sonnet",
      "maker": "anthropic",
      "providers": [
        {
          "id": "aws-bedrock",
          "litellm_model_name": "bedrock/anthropic.claude-3.5-sonnet",
          "is_default": true
        },
        {
          "id": "vertex-ai",
          "litellm_model_name": "vertex_ai/claude-3.5-sonnet",
          "is_default": false
        }
      ],
      "icon": "@/assets/icons/anthropic.svg",
      "description": "Anthropic's Claude 3.5 Sonnet model with a 200K tokens context window available via AWS Bedrock, Vertex AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$3.00",
        "output_per_1m": "$15.00",
        "tier": "premium"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 200000,
      "max_output": 50000
    },
    {
      "id": "claude-3.5-sonnet-20240620",
      "name": "Claude 3.5 Sonnet 20240620",
      "maker": "anthropic",
      "providers": [
        {
          "id": "aws-bedrock",
          "litellm_model_name": "bedrock/anthropic.claude-3.5-sonnet-20240620",
          "is_default": true
        },
        {
          "id": "vertex-ai",
          "litellm_model_name": "vertex_ai/claude-3.5-sonnet-20240620",
          "is_default": false
        }
      ],
      "icon": "@/assets/icons/anthropic.svg",
      "description": "Anthropic's Claude 3.5 Sonnet 20240620 model with a 200K tokens context window available via AWS Bedrock, Vertex AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$3.00",
        "output_per_1m": "$15.00",
        "tier": "premium"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 200000,
      "max_output": 50000
    },
    {
      "id": "claude-3.7-sonnet",
      "name": "Claude 3.7 Sonnet",
      "maker": "anthropic",
      "providers": [
        {
          "id": "anthropic",
          "litellm_model_name": "anthropic/claude-3.7-sonnet",
          "is_default": true
        },
        {
          "id": "aws-bedrock",
          "litellm_model_name": "bedrock/anthropic.claude-3.7-sonnet",
          "is_default": false
        },
        {
          "id": "vertex-ai",
          "litellm_model_name": "vertex_ai/claude-3.7-sonnet",
          "is_default": false
        }
      ],
      "icon": "@/assets/icons/anthropic.svg",
      "description": "Anthropic's Claude 3.7 Sonnet model with a 200K tokens context window available via Anthropic, AWS Bedrock, Vertex AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$3.00",
        "output_per_1m": "$15.00",
        "tier": "premium"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 200000,
      "max_output": 50000
    },
    {
      "id": "claude-haiku-4.5",
      "name": "Claude Haiku 4.5",
      "maker": "anthropic",
      "providers": [
        {
          "id": "anthropic",
          "litellm_model_name": "anthropic/claude-haiku-4.5",
          "is_default": true
        },
        {
          "id": "aws-bedrock",
          "litellm_model_name": "bedrock/anthropic.claude-haiku-4.5",
          "is_default": false
        },
        {
          "id": "vertex-ai",
          "litellm_model_name": "vertex_ai/claude-haiku-4.5",
          "is_default": false
        }
      ],
      "icon": "@/assets/icons/anthropic.svg",
      "description": "Anthropic's Claude Haiku 4.5 model with a 200K tokens context window available via Anthropic, AWS Bedrock, Vertex AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$1.00",
        "output_per_1m": "$5.00",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 200000,
      "max_output": 50000
    },
    {
      "id": "claude-opus-4",
      "name": "Claude Opus 4",
      "maker": "anthropic",
      "providers": [
        {
          "id": "anthropic",
          "litellm_model_name": "anthropic/claude-opus-4",
          "is_default": true
        },
        {
          "id": "aws-bedrock",
          "litellm_model_name": "bedrock/anthropic.claude-opus-4",
          "is_default": false
        },
        {
          "id": "vertex-ai",
          "litellm_model_name": "vertex_ai/claude-opus-4",
          "is_default": false
        }
      ],
      "icon": "@/assets/icons/anthropic.svg",
      "description": "Anthropic's Claude Opus 4 model with a 200K tokens context window available via Anthropic, AWS Bedrock, Vertex AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$15.00",
        "output_per_1m": "$75.00",
        "tier": "premium"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 200000,
      "max_output": 50000
    },
    {
      "id": "claude-opus-4-1",
      "name": "Claude Opus 4.1",
      "maker": "anthropic",
      "providers": [
        {
          "id": "anthropic",
          "litellm_model_name": "anthropic/claude-opus-4-1-20250805",
          "is_default": true
        },
        {
          "id": "aws-bedrock",
          "litellm_model_name": "bedrock/anthropic.claude-opus-4-1-v2",
          "is_default": false
        },
        {
          "id": "vertex-ai",
          "litellm_model_name": "vertex_ai/claude-opus-4-1",
          "is_default": false
        }
      ],
      "context_window": 200000,
      "max_output": 64000,
      "icon": "@/assets/icons/anthropic.svg",
      "description": "Latest and most powerful Opus model - 7-hour memory span with extended thinking",
      "capabilities": [
        "reasoning",
        "code",
        "chat",
        "tool_calling",
        "vision"
      ],
      "pricing": {
        "input_per_1m": "Premium tier",
        "output_per_1m": "Premium tier",
        "tier": "premium"
      },
      "use_cases": [
        "Most challenging reasoning tasks",
        "Complex software architecture",
        "Advanced research",
        "Strategic planning",
        "Long-form content creation"
      ],
      "release_date": "2025-08-05",
      "notes": "Claude Opus 4.5 does NOT exist - Opus 4.1 is the latest Opus model",
      "features": [
        "7-hour memory span",
        "Extended thinking mode",
        "Improved agentic search",
        "Enhanced coding"
      ],
      "enabled": true
    },
    {
      "id": "claude-sonnet-4",
      "name": "Claude Sonnet 4",
      "maker": "anthropic",
      "providers": [
        {
          "id": "anthropic",
          "litellm_model_name": "anthropic/claude-sonnet-4",
          "is_default": true
        },
        {
          "id": "aws-bedrock",
          "litellm_model_name": "bedrock/anthropic.claude-sonnet-4",
          "is_default": false
        },
        {
          "id": "vertex-ai",
          "litellm_model_name": "vertex_ai/claude-sonnet-4",
          "is_default": false
        }
      ],
      "icon": "@/assets/icons/anthropic.svg",
      "description": "Anthropic's Claude Sonnet 4 model with a 200K tokens context window available via Anthropic, AWS Bedrock, Vertex AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$3.00",
        "output_per_1m": "$15.00",
        "tier": "premium"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 200000,
      "max_output": 50000
    },
    {
      "id": "claude-sonnet-4-5",
      "name": "Claude Sonnet 4.5",
      "maker": "anthropic",
      "providers": [
        {
          "id": "anthropic",
          "litellm_model_name": "anthropic/claude-sonnet-4-5-20250929",
          "is_default": true
        },
        {
          "id": "aws-bedrock",
          "litellm_model_name": "bedrock/anthropic.claude-sonnet-4-5-v2",
          "is_default": false
        },
        {
          "id": "vertex-ai",
          "litellm_model_name": "vertex_ai/claude-sonnet-4-5",
          "is_default": false
        }
      ],
      "context_window": 200000,
      "max_output": 64000,
      "icon": "@/assets/icons/anthropic.svg",
      "description": "Anthropic's most advanced Sonnet model - Hybrid reasoning with 30+ hour autonomy",
      "capabilities": [
        "chat",
        "reasoning",
        "code",
        "tool_calling",
        "vision"
      ],
      "pricing": {
        "input_per_1m": "$3.00",
        "output_per_1m": "$15.00",
        "tier": "standard"
      },
      "use_cases": [
        "Long-running autonomous tasks",
        "Complex multi-step workflows",
        "Advanced code generation",
        "Research and analysis",
        "Content creation at scale"
      ],
      "release_date": "2025-09-29",
      "notes": "Most intelligent and efficient Sonnet model - efficient for everyday use",
      "features": [
        "Hybrid reasoning mode",
        "30+ hour memory span",
        "Enhanced tool use",
        "Improved code generation"
      ],
      "enabled": true
    },
    {
      "id": "codestral",
      "name": "Codestral",
      "maker": "mistral",
      "providers": [
        {
          "id": "mistral",
          "litellm_model_name": "mistral/codestral",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/mistral.svg",
      "description": "Mistral AI's Codestral model with a 256K tokens context window available via Mistral AI",
      "capabilities": [
        "chat",
        "code",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.30",
        "output_per_1m": "$0.90",
        "tier": "standard"
      },
      "use_cases": [
        "Developer copilots and IDE integrations",
        "Automated code review and refactoring",
        "Scripting and workflow automation",
        "Complex analysis and planning tasks",
        "Decision support and strategy"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 256000,
      "max_output": 64000
    },
    {
      "id": "codestral-embed",
      "name": "Codestral Embed",
      "maker": "mistral",
      "providers": [
        {
          "id": "mistral",
          "litellm_model_name": "mistral/codestral-embed",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/mistral.svg",
      "description": "Mistral AI's Codestral Embed model available via Mistral AI",
      "capabilities": [
        "embeddings"
      ],
      "pricing": {
        "input_per_1m": "$0.15",
        "output_per_1m": "—",
        "tier": "standard"
      },
      "use_cases": [
        "Semantic search and retrieval",
        "Vector database indexing",
        "Personalization and recommendations"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true
    },
    {
      "id": "codestral-embed-2505",
      "name": "Codestral Embed 2505",
      "maker": "mistral",
      "providers": [
        {
          "id": "openrouter",
          "litellm_model_name": "openrouter/mistralai/codestral-embed-2505",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/mistral.svg",
      "description": "Mistral AI's Codestral Embed 2505 model available via OpenRouter",
      "capabilities": [
        "embeddings"
      ],
      "pricing": {
        "input_per_1m": "$0.15",
        "output_per_1m": "—",
        "tier": "standard"
      },
      "use_cases": [
        "Semantic search and retrieval",
        "Vector database indexing",
        "Personalization and recommendations"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true
    },
    {
      "id": "command-a",
      "name": "Command A",
      "maker": "cohere",
      "providers": [
        {
          "id": "cohere",
          "litellm_model_name": "cohere/command-a",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/cohere.svg",
      "description": "Cohere's Command A model with a 256K tokens context window available via Cohere",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$2.50",
        "output_per_1m": "$10.00",
        "tier": "premium"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 256000,
      "max_output": 64000
    },
    {
      "id": "deepseek-chat-v3.1",
      "name": "DeepSeek V3.1",
      "maker": "deepseek",
      "providers": [
        {
          "id": "openrouter",
          "litellm_model_name": "openrouter/deepseek/deepseek-chat-v3.1:free",
          "is_default": true
        }
      ],
      "context_window": 128000,
      "max_output": 8192,
      "icon": "@/assets/icons/deepseek.svg",
      "description": "DeepSeek's hybrid reasoning model - 671B params, 37B active, FREE tier",
      "capabilities": [
        "reasoning",
        "chat",
        "code"
      ],
      "pricing": {
        "input_per_1m": "FREE",
        "output_per_1m": "FREE",
        "tier": "free"
      },
      "use_cases": [
        "Budget-conscious reasoning",
        "Experimental workflows",
        "High-volume applications"
      ],
      "release_date": "2025",
      "notes": "Supports reasoning_enabled boolean parameter",
      "parameters": {
        "reasoning_enabled": "boolean"
      },
      "performance": {
        "total_params": "671B",
        "active_params": "37B",
        "architecture": "MoE"
      },
      "enabled": false
    },
    {
      "id": "deepseek-r1",
      "name": "Deepseek R1",
      "maker": "deepseek",
      "providers": [
        {
          "id": "aws-bedrock",
          "litellm_model_name": "bedrock/deepseek.deepseek-r1",
          "is_default": true
        },
        {
          "id": "parasail",
          "litellm_model_name": "parasail/deepseek/deepseek-r1",
          "is_default": false
        }
      ],
      "icon": "@/assets/icons/deepseek.svg",
      "description": "DeepSeek's Deepseek R1 model with a 164K tokens context window available via AWS Bedrock, Parasail",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.79",
        "output_per_1m": "$4.00",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 164000,
      "max_output": 41000
    },
    {
      "id": "deepseek-v3",
      "name": "Deepseek V3",
      "maker": "deepseek",
      "providers": [
        {
          "id": "baseten",
          "litellm_model_name": "baseten/deepseek/deepseek-v3",
          "is_default": true
        },
        {
          "id": "novita",
          "litellm_model_name": "novita/deepseek/deepseek-v3",
          "is_default": false
        }
      ],
      "icon": "@/assets/icons/deepseek.svg",
      "description": "DeepSeek's Deepseek V3 model with a 164K tokens context window available via Baseten, Novita AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.77",
        "output_per_1m": "$0.77",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 164000,
      "max_output": 41000
    },
    {
      "id": "deepseek-v3.1",
      "name": "Deepseek V3.1",
      "maker": "deepseek",
      "providers": [
        {
          "id": "baseten",
          "litellm_model_name": "baseten/deepseek/deepseek-v3.1",
          "is_default": true
        },
        {
          "id": "deepinfra",
          "litellm_model_name": "deepinfra/deepseek/deepseek-v3.1",
          "is_default": false
        },
        {
          "id": "fireworks",
          "litellm_model_name": "fireworks/deepseek/deepseek-v3.1",
          "is_default": false
        },
        {
          "id": "novita",
          "litellm_model_name": "novita/deepseek/deepseek-v3.1",
          "is_default": false
        }
      ],
      "icon": "@/assets/icons/deepseek.svg",
      "description": "DeepSeek's Deepseek V3.1 model with a 164K tokens context window available via Baseten, DeepInfra, Fireworks AI, Novita AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.30",
        "output_per_1m": "$1.00",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 164000,
      "max_output": 41000
    },
    {
      "id": "deepseek-v3.1-terminus",
      "name": "Deepseek V3.1 Terminus",
      "maker": "deepseek",
      "providers": [
        {
          "id": "novita",
          "litellm_model_name": "novita/deepseek/deepseek-v3.1-terminus",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/deepseek.svg",
      "description": "DeepSeek's Deepseek V3.1 Terminus model with a 131K tokens context window available via Novita AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.27",
        "output_per_1m": "$1.00",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 131000,
      "max_output": 32750
    },
    {
      "id": "deepseek-v3.2-exp",
      "name": "Deepseek V3.2 Exp",
      "maker": "deepseek",
      "providers": [
        {
          "id": "deepseek",
          "litellm_model_name": "deepseek/deepseek-v3.2-exp",
          "is_default": true
        },
        {
          "id": "novita",
          "litellm_model_name": "novita/deepseek/deepseek-v3.2-exp",
          "is_default": false
        }
      ],
      "icon": "@/assets/icons/deepseek.svg",
      "description": "DeepSeek's Deepseek V3.2 Exp model with a 164K tokens context window available via DeepSeek, Novita AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.27",
        "output_per_1m": "$0.41",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 164000,
      "max_output": 41000
    },
    {
      "id": "deepseek-v3.2-exp-thinking",
      "name": "Deepseek V3.2 Exp Thinking",
      "maker": "deepseek",
      "providers": [
        {
          "id": "deepseek",
          "litellm_model_name": "deepseek/deepseek-v3.2-exp-thinking",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/deepseek.svg",
      "description": "DeepSeek's Deepseek V3.2 Exp Thinking model with a 164K tokens context window available via DeepSeek",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.28",
        "output_per_1m": "$0.42",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 164000,
      "max_output": 41000
    },
    {
      "id": "devstral-small",
      "name": "Devstral Small",
      "maker": "mistral",
      "providers": [
        {
          "id": "mistral",
          "litellm_model_name": "mistral/devstral-small",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/mistral.svg",
      "description": "Mistral AI's Devstral Small model with a 128K tokens context window available via Mistral AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.10",
        "output_per_1m": "$0.30",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 128000,
      "max_output": 32000
    },
    {
      "id": "embed-v4.0",
      "name": "Embed V4.0",
      "maker": "cohere",
      "providers": [
        {
          "id": "cohere",
          "litellm_model_name": "cohere/embed-v4.0",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/cohere.svg",
      "description": "Cohere's Embed V4.0 model available via Cohere",
      "capabilities": [
        "embeddings"
      ],
      "pricing": {
        "input_per_1m": "$0.12",
        "output_per_1m": "—",
        "tier": "standard"
      },
      "use_cases": [
        "Semantic search and retrieval",
        "Vector database indexing",
        "Personalization and recommendations"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true
    },
    {
      "id": "gemini-2.0-flash",
      "name": "Gemini 2.0 Flash",
      "maker": "google",
      "providers": [
        {
          "id": "vertex-ai",
          "litellm_model_name": "vertex_ai/gemini-2.0-flash",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/google.svg",
      "description": "Google DeepMind's Gemini 2.0 Flash model with a 1M tokens context window available via Vertex AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.10",
        "output_per_1m": "$0.40",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 1000000,
      "max_output": 250000
    },
    {
      "id": "gemini-2.0-flash-lite",
      "name": "Gemini 2.0 Flash Lite",
      "maker": "google",
      "providers": [
        {
          "id": "vertex-ai",
          "litellm_model_name": "vertex_ai/gemini-2.0-flash-lite",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/google.svg",
      "description": "Google DeepMind's Gemini 2.0 Flash Lite model with a 1M tokens context window available via Vertex AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.07",
        "output_per_1m": "$0.30",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 1000000,
      "max_output": 250000
    },
    {
      "id": "gemini-2.5-flash",
      "name": "Gemini 2.5 Flash",
      "maker": "google",
      "providers": [
        {
          "id": "gemini",
          "litellm_model_name": "gemini/gemini-2.5-flash",
          "is_default": true
        },
        {
          "id": "vertex-ai",
          "litellm_model_name": "vertex_ai/gemini-2.5-flash",
          "is_default": false
        }
      ],
      "context_window": 1048576,
      "max_output": 8192,
      "icon": "@/assets/icons/google.svg",
      "description": "Google's fast multimodal model - 1M+ token context with reasoning control",
      "capabilities": [
        "chat",
        "multimodal",
        "vision",
        "code",
        "long-context"
      ],
      "pricing": {
        "input_per_1m": "$0.30",
        "output_per_1m": "$2.50",
        "tier": "budget"
      },
      "use_cases": [
        "Long document analysis",
        "Video understanding",
        "Code repository analysis",
        "Multi-document synthesis",
        "Real-time applications"
      ],
      "release_date": "2025",
      "notes": "Supports reasoning_effort parameter for controlling reasoning depth",
      "parameters": {
        "reasoning_effort": [
          "low",
          "medium",
          "high"
        ]
      },
      "enabled": true
    },
    {
      "id": "gemini-2.5-flash-image",
      "name": "Gemini 2.5 Flash Image",
      "maker": "google",
      "providers": [
        {
          "id": "vertex-ai",
          "litellm_model_name": "vertex_ai/gemini-2.5-flash-image",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/google.svg",
      "description": "Google DeepMind's Gemini 2.5 Flash Image model with a 33K tokens context window available via Vertex AI",
      "capabilities": [
        "chat",
        "vision",
        "multimodal",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.30",
        "output_per_1m": "$2.50",
        "tier": "standard"
      },
      "use_cases": [
        "Multimodal assistants and copilots",
        "Document and image understanding",
        "Visual reasoning and analysis",
        "Complex analysis and planning tasks",
        "Decision support and strategy"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 33000,
      "max_output": 8250
    },
    {
      "id": "gemini-2.5-flash-image-preview",
      "name": "Gemini 2.5 Flash Image Preview",
      "maker": "google",
      "providers": [
        {
          "id": "gemini",
          "litellm_model_name": "gemini/gemini-2.5-flash-image-preview",
          "is_default": true
        },
        {
          "id": "vertex-ai",
          "litellm_model_name": "vertex_ai/gemini-2.5-flash-image-preview",
          "is_default": false
        }
      ],
      "context_window": 33000,
      "max_output": 8250,
      "icon": "@/assets/icons/google.svg",
      "description": "Preview variant of Google's Gemini 2.5 Flash Image with experimental image generation features",
      "capabilities": [
        "chat",
        "vision",
        "multimodal",
        "image",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.30",
        "output_per_1m": "$2.50",
        "tier": "standard"
      },
      "use_cases": [
        "Experimental image generation",
        "Multimodal content creation",
        "Visual prototyping",
        "Preview of upcoming features"
      ],
      "release_date": "2025",
      "notes": "Preview model with experimental image generation capabilities",
      "enabled": true
    },
    {
      "id": "gemini-2.5-flash-lite",
      "name": "Gemini 2.5 Flash Lite",
      "maker": "google",
      "providers": [
        {
          "id": "vertex-ai",
          "litellm_model_name": "vertex_ai/gemini-2.5-flash-lite",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/google.svg",
      "description": "Google DeepMind's Gemini 2.5 Flash Lite model with a 1M tokens context window available via Vertex AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.10",
        "output_per_1m": "$0.40",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 1000000,
      "max_output": 250000
    },
    {
      "id": "gemini-2.5-pro",
      "name": "Gemini 2.5 Pro",
      "maker": "google",
      "providers": [
        {
          "id": "gemini",
          "litellm_model_name": "gemini/gemini-2.5-pro",
          "is_default": true
        },
        {
          "id": "vertex-ai",
          "litellm_model_name": "vertex_ai/gemini-2.5-pro",
          "is_default": false
        }
      ],
      "context_window": 2000000,
      "max_output": 8192,
      "icon": "@/assets/icons/google.svg",
      "description": "Google's most powerful model - 2M token context with thinking budget",
      "capabilities": [
        "reasoning",
        "chat",
        "multimodal",
        "vision",
        "code",
        "long-context"
      ],
      "pricing": {
        "input_per_1m": "Premium tier",
        "output_per_1m": "Premium tier",
        "tier": "premium"
      },
      "use_cases": [
        "Entire codebase analysis",
        "Large-scale research",
        "Multi-source synthesis",
        "Complex reasoning tasks",
        "Advanced code generation"
      ],
      "release_date": "2025",
      "notes": "Supports max_tokens_for_reasoning parameter for thinking budget control",
      "parameters": {
        "reasoning_effort": [
          "low",
          "medium",
          "high"
        ],
        "max_tokens_for_reasoning": "configurable"
      },
      "enabled": true
    },
    {
      "id": "gemini-embedding-001",
      "name": "Gemini Embedding 001",
      "maker": "google",
      "providers": [
        {
          "id": "vertex-ai",
          "litellm_model_name": "vertex_ai/gemini-embedding-001",
          "is_default": true
        },
        {
          "id": "openrouter",
          "litellm_model_name": "openrouter/google/gemini-embedding-001",
          "is_default": false
        }
      ],
      "icon": "@/assets/icons/google.svg",
      "description": "Google DeepMind's Gemini Embedding 001 model available via Vertex AI, OpenRouter",
      "capabilities": [
        "embeddings"
      ],
      "pricing": {
        "input_per_1m": "$0.15",
        "output_per_1m": "—",
        "tier": "standard"
      },
      "use_cases": [
        "Semantic search and retrieval",
        "Vector database indexing",
        "Personalization and recommendations"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true
    },
    {
      "id": "glm-4.5",
      "name": "GLM 4.5",
      "maker": "zai",
      "providers": [
        {
          "id": "novita",
          "litellm_model_name": "novita/zai/glm-4.5",
          "is_default": false
        },
        {
          "id": "zai",
          "litellm_model_name": "zai/glm-4.5",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/zai.svg",
      "description": "Zhipu AI's GLM 4.5 model with a 128K tokens context window available via Novita AI, Z.ai",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.60",
        "output_per_1m": "$2.20",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 128000,
      "max_output": 32000
    },
    {
      "id": "glm-4.5-air",
      "name": "GLM 4.5 Air",
      "maker": "zai",
      "providers": [
        {
          "id": "zai",
          "litellm_model_name": "zai/glm-4.5-air",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/zai.svg",
      "description": "Zhipu AI's GLM 4.5 Air model with a 128K tokens context window available via Z.ai",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.20",
        "output_per_1m": "$1.10",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 128000,
      "max_output": 32000
    },
    {
      "id": "glm-4.5v",
      "name": "GLM 4.5V",
      "maker": "zai",
      "providers": [
        {
          "id": "novita",
          "litellm_model_name": "novita/zai/glm-4.5v",
          "is_default": false
        },
        {
          "id": "zai",
          "litellm_model_name": "zai/glm-4.5v",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/zai.svg",
      "description": "Zhipu AI's GLM 4.5V model with a 66K tokens context window available via Novita AI, Z.ai",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.60",
        "output_per_1m": "$1.80",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 66000,
      "max_output": 16500
    },
    {
      "id": "glm-4.6",
      "name": "GLM 4.6",
      "maker": "zai",
      "providers": [
        {
          "id": "zai",
          "litellm_model_name": "zai/glm-4.6",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/zai.svg",
      "description": "Zhipu AI's GLM 4.6 model with a 200K tokens context window available via Z.ai",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.45",
        "output_per_1m": "$1.80",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 200000,
      "max_output": 50000
    },
    {
      "id": "gpt-3.5-turbo",
      "name": "GPT-3.5 Turbo",
      "maker": "openai",
      "providers": [
        {
          "id": "openai",
          "litellm_model_name": "openai/gpt-3.5-turbo",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/openai.svg",
      "description": "OpenAI's GPT-3.5 Turbo model with a 16K tokens context window available via OpenAI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.50",
        "output_per_1m": "$1.50",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 16000,
      "max_output": 4000
    },
    {
      "id": "gpt-3.5-turbo-instruct",
      "name": "GPT-3.5 Turbo Instruct",
      "maker": "openai",
      "providers": [
        {
          "id": "openai",
          "litellm_model_name": "openai/gpt-3.5-turbo-instruct",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/openai.svg",
      "description": "OpenAI's GPT-3.5 Turbo Instruct model with a 8K tokens context window available via OpenAI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$1.50",
        "output_per_1m": "$2.00",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 8000,
      "max_output": 2000
    },
    {
      "id": "gpt-4-turbo",
      "name": "GPT-4 Turbo",
      "maker": "openai",
      "providers": [
        {
          "id": "openai",
          "litellm_model_name": "openai/gpt-4-turbo",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/openai.svg",
      "description": "OpenAI's GPT-4 Turbo model with a 128K tokens context window available via OpenAI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$10.00",
        "output_per_1m": "$30.00",
        "tier": "premium"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 128000,
      "max_output": 32000
    },
    {
      "id": "gpt-4.1",
      "name": "GPT-4.1",
      "maker": "openai",
      "providers": [
        {
          "id": "azure",
          "litellm_model_name": "azure/gpt-4.1",
          "is_default": false
        },
        {
          "id": "openai",
          "litellm_model_name": "openai/gpt-4.1",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/openai.svg",
      "description": "OpenAI's GPT-4.1 model with a 1M tokens context window available via Azure OpenAI, OpenAI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$2.00",
        "output_per_1m": "$8.00",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 1000000,
      "max_output": 250000
    },
    {
      "id": "gpt-4.1-mini",
      "name": "GPT-4.1 Mini",
      "maker": "openai",
      "providers": [
        {
          "id": "azure",
          "litellm_model_name": "azure/gpt-4.1-mini",
          "is_default": false
        },
        {
          "id": "openai",
          "litellm_model_name": "openai/gpt-4.1-mini",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/openai.svg",
      "description": "OpenAI's GPT-4.1 Mini model with a 1M tokens context window available via Azure OpenAI, OpenAI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.40",
        "output_per_1m": "$1.60",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 1000000,
      "max_output": 250000
    },
    {
      "id": "gpt-4.1-nano",
      "name": "GPT-4.1 Nano",
      "maker": "openai",
      "providers": [
        {
          "id": "azure",
          "litellm_model_name": "azure/gpt-4.1-nano",
          "is_default": false
        },
        {
          "id": "openai",
          "litellm_model_name": "openai/gpt-4.1-nano",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/openai.svg",
      "description": "OpenAI's GPT-4.1 Nano model with a 1M tokens context window available via Azure OpenAI, OpenAI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.10",
        "output_per_1m": "$0.40",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 1000000,
      "max_output": 250000
    },
    {
      "id": "gpt-4o",
      "name": "GPT-4o",
      "maker": "openai",
      "providers": [
        {
          "id": "azure",
          "litellm_model_name": "azure/gpt-4o",
          "is_default": false
        },
        {
          "id": "openai",
          "litellm_model_name": "openai/gpt-4o",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/openai.svg",
      "description": "OpenAI's GPT-4o model with a 128K tokens context window available via Azure OpenAI, OpenAI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$2.50",
        "output_per_1m": "$10.00",
        "tier": "premium"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 128000,
      "max_output": 32000
    },
    {
      "id": "gpt-4o-mini",
      "name": "GPT-4o Mini",
      "maker": "openai",
      "providers": [
        {
          "id": "azure",
          "litellm_model_name": "azure/gpt-4o-mini",
          "is_default": false
        },
        {
          "id": "openai",
          "litellm_model_name": "openai/gpt-4o-mini",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/openai.svg",
      "description": "OpenAI's GPT-4o Mini model with a 128K tokens context window available via Azure OpenAI, OpenAI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.15",
        "output_per_1m": "$0.60",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 128000,
      "max_output": 32000
    },
    {
      "id": "gpt-5",
      "name": "GPT-5",
      "maker": "openai",
      "providers": [
        {
          "id": "openai",
          "litellm_model_name": "openai/gpt-5-2025-08-07",
          "is_default": true
        },
        {
          "id": "azure",
          "litellm_model_name": "azure/gpt-5",
          "is_default": false
        }
      ],
      "context_window": 400000,
      "max_output": 128000,
      "icon": "@/assets/icons/openai.svg",
      "description": "OpenAI's flagship GPT-5 model - Advanced reasoning, multimodal processing, tool calling",
      "capabilities": [
        "chat",
        "reasoning",
        "multimodal",
        "vision",
        "code",
        "tool_calling",
        "function_execution",
        "image"
      ],
      "pricing": {
        "input_per_1m": "$1.25",
        "output_per_1m": "$10.00",
        "tier": "standard"
      },
      "use_cases": [
        "Complex reasoning tasks",
        "Multi-step problem solving",
        "Code generation and debugging",
        "Content creation and editing",
        "Research and analysis"
      ],
      "release_date": "2025-08-07",
      "notes": "Unprecedented capabilities in reasoning, coding, and agentic tasks",
      "enabled": true
    },
    {
      "id": "gpt-5-codex",
      "name": "GPT-5 Codex",
      "maker": "openai",
      "providers": [
        {
          "id": "azure",
          "litellm_model_name": "azure/gpt-5-codex",
          "is_default": false
        },
        {
          "id": "openai",
          "litellm_model_name": "openai/gpt-5-codex",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/openai.svg",
      "description": "OpenAI's GPT-5 Codex model with a 400K tokens context window available via Azure OpenAI, OpenAI",
      "capabilities": [
        "chat",
        "code",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$1.25",
        "output_per_1m": "$10.00",
        "tier": "premium"
      },
      "use_cases": [
        "Developer copilots and IDE integrations",
        "Automated code review and refactoring",
        "Scripting and workflow automation",
        "Complex analysis and planning tasks",
        "Decision support and strategy"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 400000,
      "max_output": 100000
    },
    {
      "id": "gpt-5-mini",
      "name": "GPT-5 Mini",
      "maker": "openai",
      "providers": [
        {
          "id": "openai",
          "litellm_model_name": "openai/gpt-5-mini-2025-08-07",
          "is_default": true
        },
        {
          "id": "azure",
          "litellm_model_name": "azure/gpt-5-mini",
          "is_default": false
        }
      ],
      "context_window": 400000,
      "max_output": 128000,
      "icon": "@/assets/icons/openai.svg",
      "description": "Cost-efficient GPT-5 variant for well-defined tasks",
      "capabilities": [
        "chat",
        "multimodal",
        "vision",
        "code",
        "tool_calling"
      ],
      "pricing": {
        "input_per_1m": "$0.25",
        "output_per_1m": "$2.00",
        "tier": "budget"
      },
      "use_cases": [
        "High-volume API calls",
        "Real-time chat applications",
        "Simple content generation",
        "Code completion",
        "Quick Q&A"
      ],
      "release_date": "2025-08-07",
      "notes": "80% cost reduction vs GPT-5 while maintaining strong performance",
      "enabled": true
    },
    {
      "id": "gpt-5-nano",
      "name": "GPT-5 Nano",
      "maker": "openai",
      "providers": [
        {
          "id": "openai",
          "litellm_model_name": "openai/gpt-5-nano-2025-08-07",
          "is_default": true
        },
        {
          "id": "azure",
          "litellm_model_name": "azure/gpt-5-nano",
          "is_default": false
        }
      ],
      "context_window": 400000,
      "max_output": 128000,
      "icon": "@/assets/icons/openai.svg",
      "description": "Fastest, most cost-efficient GPT-5 for everyday tasks",
      "capabilities": [
        "chat",
        "reasoning",
        "image"
      ],
      "pricing": {
        "input_per_1m": "$0.05",
        "output_per_1m": "$0.40",
        "tier": "budget"
      },
      "use_cases": [
        "High-throughput applications",
        "Simple classification",
        "Basic Q&A",
        "Text summarization",
        "Sentiment analysis"
      ],
      "release_date": "2025-08-07",
      "notes": "96% cost reduction vs GPT-5 - ideal for edge cases and batch processing",
      "enabled": true
    },
    {
      "id": "gpt-5-pro",
      "name": "GPT-5 Pro",
      "maker": "openai",
      "providers": [
        {
          "id": "openai",
          "litellm_model_name": "openai/gpt-5-pro-2025-10-06",
          "is_default": true
        }
      ],
      "context_window": 400000,
      "max_output": 128000,
      "icon": "@/assets/icons/openai.svg",
      "description": "Scaled test-time compute for challenging reasoning tasks",
      "capabilities": [
        "chat",
        "reasoning",
        "code",
        "multimodal",
        "image"
      ],
      "pricing": {
        "input_per_1m": "Premium tier",
        "output_per_1m": "Premium tier",
        "tier": "premium"
      },
      "use_cases": [
        "Mathematical proofs",
        "Complex code refactoring",
        "Research paper analysis",
        "Strategic planning",
        "Advanced debugging"
      ],
      "release_date": "2025-10-06",
      "notes": "Preferred in 67.8% of expert evaluations over GPT-5 Thinking",
      "performance": {
        "expert_preference": "67.8%",
        "vs_model": "GPT-5 Thinking"
      },
      "enabled": false
    },
    {
      "id": "gpt-oss-120b-cloud",
      "name": "GPT-OSS 120B (Cloud)",
      "maker": "openai",
      "providers": [
        {
          "id": "aws-bedrock",
          "litellm_model_name": "bedrock/openai.gpt-oss-120b",
          "is_default": true
        },
        {
          "id": "baseten",
          "litellm_model_name": "baseten/openai/gpt-oss-120b",
          "is_default": false
        },
        {
          "id": "cerebras",
          "litellm_model_name": "cerebras/openai/gpt-oss-120b",
          "is_default": false
        },
        {
          "id": "fireworks",
          "litellm_model_name": "fireworks/openai/gpt-oss-120b",
          "is_default": false
        },
        {
          "id": "groq",
          "litellm_model_name": "groq/openai/gpt-oss-120b",
          "is_default": false
        },
        {
          "id": "parasail",
          "litellm_model_name": "parasail/openai/gpt-oss-120b",
          "is_default": false
        }
      ],
      "icon": "@/assets/icons/openai.svg",
      "description": "OpenAI's GPT-OSS 120B deployments with a 131K token context window available via AWS Bedrock, Baseten, Cerebras, Fireworks AI, Groq, and Parasail",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.10",
        "output_per_1m": "$0.50",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 131000,
      "max_output": 32750
    },
    {
      "id": "gpt-oss-20b-cloud",
      "name": "GPT-OSS 20B (Cloud)",
      "maker": "openai",
      "providers": [
        {
          "id": "aws-bedrock",
          "litellm_model_name": "bedrock/openai.gpt-oss-20b",
          "is_default": true
        },
        {
          "id": "fireworks",
          "litellm_model_name": "fireworks/openai/gpt-oss-20b",
          "is_default": false
        },
        {
          "id": "groq",
          "litellm_model_name": "groq/openai/gpt-oss-20b",
          "is_default": false
        }
      ],
      "icon": "@/assets/icons/openai.svg",
      "description": "OpenAI's GPT-OSS 20B deployments with a 128K token context window available via AWS Bedrock, Fireworks AI, and Groq",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.07",
        "output_per_1m": "$0.30",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 128000,
      "max_output": 32000
    },
    {
      "id": "gpt-oss-safeguard-20b",
      "name": "GPT-OSS Safeguard 20B",
      "maker": "openai",
      "providers": [
        {
          "id": "groq",
          "litellm_model_name": "groq/openai/gpt-oss-safeguard-20b",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/openai.svg",
      "description": "OpenAI's GPT-OSS Safeguard 20B model with a 131K token context window available via Groq",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.07",
        "output_per_1m": "$0.30",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 131000,
      "max_output": 32750
    },
    {
      "id": "grok-2",
      "name": "Grok 2",
      "maker": "xai",
      "providers": [
        {
          "id": "xai",
          "litellm_model_name": "xai/grok-2",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/xai.svg",
      "description": "xAI's Grok 2 model with a 131K tokens context window available via xAI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$2.00",
        "output_per_1m": "$10.00",
        "tier": "premium"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 131000,
      "max_output": 32750
    },
    {
      "id": "grok-2-vision",
      "name": "Grok 2 Vision",
      "maker": "xai",
      "providers": [
        {
          "id": "xai",
          "litellm_model_name": "xai/grok-2-vision",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/xai.svg",
      "description": "xAI's Grok 2 Vision model with a 33K tokens context window available via xAI",
      "capabilities": [
        "chat",
        "vision",
        "multimodal",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$2.00",
        "output_per_1m": "$10.00",
        "tier": "premium"
      },
      "use_cases": [
        "Multimodal assistants and copilots",
        "Document and image understanding",
        "Visual reasoning and analysis",
        "Complex analysis and planning tasks",
        "Decision support and strategy"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 33000,
      "max_output": 8250
    },
    {
      "id": "grok-3",
      "name": "Grok 3",
      "maker": "xai",
      "providers": [
        {
          "id": "xai",
          "litellm_model_name": "xai/grok-3",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/xai.svg",
      "description": "xAI's Grok 3 model with a 131K tokens context window available via xAI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$3.00",
        "output_per_1m": "$15.00",
        "tier": "premium"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 131000,
      "max_output": 32750
    },
    {
      "id": "grok-3-fast",
      "name": "Grok 3 Fast",
      "maker": "xai",
      "providers": [
        {
          "id": "xai",
          "litellm_model_name": "xai/grok-3-fast",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/xai.svg",
      "description": "xAI's Grok 3 Fast model with a 131K tokens context window available via xAI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$5.00",
        "output_per_1m": "$25.00",
        "tier": "premium"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 131000,
      "max_output": 32750
    },
    {
      "id": "grok-3-mini",
      "name": "Grok 3 Mini",
      "maker": "xai",
      "providers": [
        {
          "id": "xai",
          "litellm_model_name": "xai/grok-3-mini",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/xai.svg",
      "description": "xAI's Grok 3 Mini model with a 131K tokens context window available via xAI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.30",
        "output_per_1m": "$0.50",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 131000,
      "max_output": 32750
    },
    {
      "id": "grok-3-mini-fast",
      "name": "Grok 3 Mini Fast",
      "maker": "xai",
      "providers": [
        {
          "id": "xai",
          "litellm_model_name": "xai/grok-3-mini-fast",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/xai.svg",
      "description": "xAI's Grok 3 Mini Fast model with a 131K tokens context window available via xAI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.60",
        "output_per_1m": "$4.00",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 131000,
      "max_output": 32750
    },
    {
      "id": "grok-4",
      "name": "Grok 4",
      "maker": "xai",
      "providers": [
        {
          "id": "xai",
          "litellm_model_name": "xai/grok-4",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/xai.svg",
      "description": "xAI's Grok 4 model with a 256K tokens context window available via xAI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$3.00",
        "output_per_1m": "$15.00",
        "tier": "premium"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 256000,
      "max_output": 64000
    },
    {
      "id": "grok-4-fast",
      "name": "Grok 4 Fast",
      "maker": "xai",
      "providers": [
        {
          "id": "openrouter",
          "litellm_model_name": "openrouter/x-ai/grok-4-fast:free",
          "is_default": true
        }
      ],
      "context_window": 2000000,
      "max_output": 32000,
      "icon": "@/assets/icons/xai.svg",
      "description": "X.AI's fast reasoning model - 2M token context, currently FREE",
      "capabilities": [
        "reasoning",
        "chat",
        "long-context"
      ],
      "pricing": {
        "input_per_1m": "FREE (limited time)",
        "output_per_1m": "FREE (limited time)",
        "tier": "free"
      },
      "use_cases": [
        "Experimental workflows",
        "High-volume testing",
        "Cost-sensitive applications"
      ],
      "release_date": "2025",
      "notes": "Supports reasoning_enabled parameter. FREE promotion may be limited time",
      "parameters": {
        "reasoning_enabled": "boolean"
      },
      "enabled": false
    },
    {
      "id": "grok-4-fast-non-reasoning",
      "name": "Grok 4 Fast Non Reasoning",
      "maker": "xai",
      "providers": [
        {
          "id": "xai",
          "litellm_model_name": "xai/grok-4-fast-non-reasoning",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/xai.svg",
      "description": "xAI's Grok 4 Fast Non Reasoning model with a 2M tokens context window available via xAI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.20",
        "output_per_1m": "$0.50",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 2000000,
      "max_output": 500000
    },
    {
      "id": "grok-4-fast-reasoning",
      "name": "Grok 4 Fast Reasoning",
      "maker": "xai",
      "providers": [
        {
          "id": "xai",
          "litellm_model_name": "xai/grok-4-fast-reasoning",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/xai.svg",
      "description": "xAI's Grok 4 Fast Reasoning model with a 2M tokens context window available via xAI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.20",
        "output_per_1m": "$0.50",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 2000000,
      "max_output": 500000
    },
    {
      "id": "grok-code-fast-1",
      "name": "Grok Code Fast 1",
      "maker": "xai",
      "providers": [
        {
          "id": "xai",
          "litellm_model_name": "xai/grok-code-fast-1",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/xai.svg",
      "description": "xAI's Grok Code Fast 1 model with a 256K tokens context window available via xAI",
      "capabilities": [
        "chat",
        "code",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.20",
        "output_per_1m": "$1.50",
        "tier": "standard"
      },
      "use_cases": [
        "Developer copilots and IDE integrations",
        "Automated code review and refactoring",
        "Scripting and workflow automation",
        "Complex analysis and planning tasks",
        "Decision support and strategy"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 256000,
      "max_output": 64000
    },
    {
      "id": "kimi-k2",
      "name": "Kimi K2",
      "maker": "moonshotai",
      "providers": [
        {
          "id": "deepinfra",
          "litellm_model_name": "deepinfra/moonshotai/kimi-k2",
          "is_default": false
        },
        {
          "id": "fireworks",
          "litellm_model_name": "fireworks/moonshotai/kimi-k2",
          "is_default": false
        },
        {
          "id": "groq",
          "litellm_model_name": "groq/moonshotai/kimi-k2",
          "is_default": false
        },
        {
          "id": "moonshot",
          "litellm_model_name": "moonshotai/kimi-k2",
          "is_default": true
        },
        {
          "id": "novita",
          "litellm_model_name": "novita/moonshotai/kimi-k2",
          "is_default": false
        },
        {
          "id": "parasail",
          "litellm_model_name": "parasail/moonshotai/kimi-k2",
          "is_default": false
        }
      ],
      "icon": "@/assets/icons/moonshot.svg",
      "description": "Moonshot AI's Kimi K2 model with a 131K tokens context window available via DeepInfra, Fireworks AI, Groq, Moonshot AI, Novita AI, Parasail",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.50",
        "output_per_1m": "$2.00",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 131000,
      "max_output": 32750
    },
    {
      "id": "kimi-k2-0905",
      "name": "Kimi K2 0905",
      "maker": "moonshotai",
      "providers": [
        {
          "id": "baseten",
          "litellm_model_name": "baseten/moonshotai/kimi-k2-0905",
          "is_default": false
        },
        {
          "id": "fireworks",
          "litellm_model_name": "fireworks/moonshotai/kimi-k2-0905",
          "is_default": false
        },
        {
          "id": "groq",
          "litellm_model_name": "groq/moonshotai/kimi-k2-0905",
          "is_default": false
        },
        {
          "id": "moonshot",
          "litellm_model_name": "moonshotai/kimi-k2-0905",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/moonshot.svg",
      "description": "Moonshot AI's Kimi K2 0905 model with a 131K tokens context window available via Baseten, Fireworks AI, Groq, Moonshot AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.60",
        "output_per_1m": "$2.50",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 131000,
      "max_output": 32750
    },
    {
      "id": "kimi-k2-turbo",
      "name": "Kimi K2 Turbo",
      "maker": "moonshotai",
      "providers": [
        {
          "id": "moonshot",
          "litellm_model_name": "moonshotai/kimi-k2-turbo",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/moonshot.svg",
      "description": "Moonshot AI's Kimi K2 Turbo model with a 256K tokens context window available via Moonshot AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$2.40",
        "output_per_1m": "$10.00",
        "tier": "premium"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 256000,
      "max_output": 64000
    },
    {
      "id": "llama-3.1-70b",
      "name": "Llama 3.1 70B",
      "maker": "meta",
      "providers": [
        {
          "id": "aws-bedrock",
          "litellm_model_name": "bedrock/meta.llama-3.1-70b",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/meta.svg",
      "description": "Meta AI's Llama 3.1 70B model with a 128K tokens context window available via AWS Bedrock",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.72",
        "output_per_1m": "$0.72",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 128000,
      "max_output": 32000
    },
    {
      "id": "llama-3.1-8b",
      "name": "Llama 3.1 8B",
      "maker": "meta",
      "providers": [
        {
          "id": "aws-bedrock",
          "litellm_model_name": "bedrock/meta.llama-3.1-8b",
          "is_default": true
        },
        {
          "id": "cerebras",
          "litellm_model_name": "cerebras/meta/llama-3.1-8b",
          "is_default": false
        },
        {
          "id": "groq",
          "litellm_model_name": "groq/meta/llama-3.1-8b",
          "is_default": false
        }
      ],
      "icon": "@/assets/icons/meta.svg",
      "description": "Meta AI's Llama 3.1 8B model with a 128K tokens context window available via AWS Bedrock, Cerebras, Groq",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.05",
        "output_per_1m": "$0.08",
        "tier": "budget"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 128000,
      "max_output": 32000
    },
    {
      "id": "llama-3.2-11b",
      "name": "Llama 3.2 11B",
      "maker": "meta",
      "providers": [
        {
          "id": "aws-bedrock",
          "litellm_model_name": "bedrock/meta.llama-3.2-11b",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/meta.svg",
      "description": "Meta AI's Llama 3.2 11B model with a 128K tokens context window available via AWS Bedrock",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.16",
        "output_per_1m": "$0.16",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 128000,
      "max_output": 32000
    },
    {
      "id": "llama-3.2-1b",
      "name": "Llama 3.2 1B",
      "maker": "meta",
      "providers": [
        {
          "id": "aws-bedrock",
          "litellm_model_name": "bedrock/meta.llama-3.2-1b",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/meta.svg",
      "description": "Meta AI's Llama 3.2 1B model with a 128K tokens context window available via AWS Bedrock",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.10",
        "output_per_1m": "$0.10",
        "tier": "budget"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 128000,
      "max_output": 32000
    },
    {
      "id": "llama-3.2-3b",
      "name": "Llama 3.2 3B",
      "maker": "meta",
      "providers": [
        {
          "id": "aws-bedrock",
          "litellm_model_name": "bedrock/meta.llama-3.2-3b",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/meta.svg",
      "description": "Meta AI's Llama 3.2 3B model with a 128K tokens context window available via AWS Bedrock",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.15",
        "output_per_1m": "$0.15",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 128000,
      "max_output": 32000
    },
    {
      "id": "llama-3.2-90b",
      "name": "Llama 3.2 90B",
      "maker": "meta",
      "providers": [
        {
          "id": "aws-bedrock",
          "litellm_model_name": "bedrock/meta.llama-3.2-90b",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/meta.svg",
      "description": "Meta AI's Llama 3.2 90B model with a 128K tokens context window available via AWS Bedrock",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.72",
        "output_per_1m": "$0.72",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 128000,
      "max_output": 32000
    },
    {
      "id": "llama-3.3-70b",
      "name": "Llama 3.3 70B",
      "maker": "meta",
      "providers": [
        {
          "id": "aws-bedrock",
          "litellm_model_name": "bedrock/meta.llama-3.3-70b",
          "is_default": true
        },
        {
          "id": "cerebras",
          "litellm_model_name": "cerebras/meta/llama-3.3-70b",
          "is_default": false
        },
        {
          "id": "groq",
          "litellm_model_name": "groq/meta/llama-3.3-70b",
          "is_default": false
        }
      ],
      "icon": "@/assets/icons/meta.svg",
      "description": "Meta AI's Llama 3.3 70B model with a 128K tokens context window available via AWS Bedrock, Cerebras, Groq",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.72",
        "output_per_1m": "$0.72",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 128000,
      "max_output": 32000
    },
    {
      "id": "llama-4-maverick",
      "name": "Llama 4 Maverick",
      "maker": "meta",
      "providers": [
        {
          "id": "aws-bedrock",
          "litellm_model_name": "bedrock/meta.llama-4-maverick",
          "is_default": true
        },
        {
          "id": "deepinfra",
          "litellm_model_name": "deepinfra/meta/llama-4-maverick",
          "is_default": false
        },
        {
          "id": "vertex-ai",
          "litellm_model_name": "vertex_ai/llama-4-maverick",
          "is_default": false
        }
      ],
      "icon": "@/assets/icons/meta.svg",
      "description": "Meta AI's Llama 4 Maverick model with a 1.3M tokens context window available via AWS Bedrock, DeepInfra, Vertex AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.15",
        "output_per_1m": "$0.60",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 1300000,
      "max_output": 325000
    },
    {
      "id": "llama-4-scout",
      "name": "Llama 4 Scout",
      "maker": "meta",
      "providers": [
        {
          "id": "aws-bedrock",
          "litellm_model_name": "bedrock/meta.llama-4-scout",
          "is_default": true
        },
        {
          "id": "cerebras",
          "litellm_model_name": "cerebras/meta/llama-4-scout",
          "is_default": false
        },
        {
          "id": "deepinfra",
          "litellm_model_name": "deepinfra/meta/llama-4-scout",
          "is_default": false
        },
        {
          "id": "groq",
          "litellm_model_name": "groq/meta/llama-4-scout",
          "is_default": false
        },
        {
          "id": "vertex-ai",
          "litellm_model_name": "vertex_ai/llama-4-scout",
          "is_default": false
        }
      ],
      "icon": "@/assets/icons/meta.svg",
      "description": "Meta AI's Llama 4 Scout model with a 128K tokens context window available via AWS Bedrock, Cerebras, DeepInfra, Groq, Vertex AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.08",
        "output_per_1m": "$0.30",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 128000,
      "max_output": 32000
    },
    {
      "id": "magistral-medium",
      "name": "Magistral Medium",
      "maker": "mistral",
      "providers": [
        {
          "id": "mistral",
          "litellm_model_name": "mistral/magistral-medium",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/mistral.svg",
      "description": "Mistral AI's Magistral Medium model with a 128K tokens context window available via Mistral AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$2.00",
        "output_per_1m": "$5.00",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 128000,
      "max_output": 32000
    },
    {
      "id": "magistral-medium-2506",
      "name": "Magistral Medium 2506",
      "maker": "mistral",
      "providers": [
        {
          "id": "mistral",
          "litellm_model_name": "mistral/magistral-medium-2506",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/mistral.svg",
      "description": "Mistral AI's Magistral Medium 2506 model with a 128K tokens context window available via Mistral AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$2.00",
        "output_per_1m": "$5.00",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 128000,
      "max_output": 32000
    },
    {
      "id": "magistral-small",
      "name": "Magistral Small",
      "maker": "mistral",
      "providers": [
        {
          "id": "mistral",
          "litellm_model_name": "mistral/magistral-small",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/mistral.svg",
      "description": "Mistral AI's Magistral Small model with a 128K tokens context window available via Mistral AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.50",
        "output_per_1m": "$1.50",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 128000,
      "max_output": 32000
    },
    {
      "id": "magistral-small-2506",
      "name": "Magistral Small 2506",
      "maker": "mistral",
      "providers": [
        {
          "id": "mistral",
          "litellm_model_name": "mistral/magistral-small-2506",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/mistral.svg",
      "description": "Mistral AI's Magistral Small 2506 model with a 128K tokens context window available via Mistral AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.50",
        "output_per_1m": "$1.50",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 128000,
      "max_output": 32000
    },
    {
      "id": "ministral-3b",
      "name": "Ministral 3B",
      "maker": "mistral",
      "providers": [
        {
          "id": "mistral",
          "litellm_model_name": "mistral/ministral-3b",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/mistral.svg",
      "description": "Mistral AI's Ministral 3B model with a 128K tokens context window available via Mistral AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.04",
        "output_per_1m": "$0.04",
        "tier": "budget"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 128000,
      "max_output": 32000
    },
    {
      "id": "ministral-8b",
      "name": "Ministral 8B",
      "maker": "mistral",
      "providers": [
        {
          "id": "mistral",
          "litellm_model_name": "mistral/ministral-8b",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/mistral.svg",
      "description": "Mistral AI's Ministral 8B model with a 128K tokens context window available via Mistral AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.10",
        "output_per_1m": "$0.10",
        "tier": "budget"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 128000,
      "max_output": 32000
    },
    {
      "id": "mistral-embed",
      "name": "Mistral Embed",
      "maker": "mistral",
      "providers": [
        {
          "id": "mistral",
          "litellm_model_name": "mistral/mistral-embed",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/mistral.svg",
      "description": "Mistral AI's Mistral Embed model available via Mistral AI",
      "capabilities": [
        "embeddings"
      ],
      "pricing": {
        "input_per_1m": "$0.10",
        "output_per_1m": "—",
        "tier": "budget"
      },
      "use_cases": [
        "Semantic search and retrieval",
        "Vector database indexing",
        "Personalization and recommendations"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true
    },
    {
      "id": "mistral-embed-2312",
      "name": "Mistral Embed 2312",
      "maker": "mistral",
      "providers": [
        {
          "id": "openrouter",
          "litellm_model_name": "openrouter/mistralai/mistral-embed-2312",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/mistral.svg",
      "description": "Mistral AI's Mistral Embed 2312 model available via OpenRouter",
      "capabilities": [
        "embeddings"
      ],
      "pricing": {
        "input_per_1m": "$0.10",
        "output_per_1m": "—",
        "tier": "budget"
      },
      "use_cases": [
        "Semantic search and retrieval",
        "Vector database indexing",
        "Personalization and recommendations"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true
    },
    {
      "id": "mistral-large",
      "name": "Mistral Large",
      "maker": "mistral",
      "providers": [
        {
          "id": "mistral",
          "litellm_model_name": "mistral/mistral-large",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/mistral.svg",
      "description": "Mistral AI's Mistral Large model with a 32K tokens context window available via Mistral AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$2.00",
        "output_per_1m": "$6.00",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 32000,
      "max_output": 8000
    },
    {
      "id": "mistral-medium",
      "name": "Mistral Medium",
      "maker": "mistral",
      "providers": [
        {
          "id": "mistral",
          "litellm_model_name": "mistral/mistral-medium",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/mistral.svg",
      "description": "Mistral AI's Mistral Medium model with a 128K tokens context window available via Mistral AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.40",
        "output_per_1m": "$2.00",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 128000,
      "max_output": 32000
    },
    {
      "id": "mistral-small",
      "name": "Mistral Small",
      "maker": "mistral",
      "providers": [
        {
          "id": "mistral",
          "litellm_model_name": "mistral/mistral-small",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/mistral.svg",
      "description": "Mistral AI's Mistral Small model with a 32K tokens context window available via Mistral AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.10",
        "output_per_1m": "$0.30",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 32000,
      "max_output": 8000
    },
    {
      "id": "mixtral-8x22b-instruct",
      "name": "Mixtral 8X22B Instruct",
      "maker": "mistral",
      "providers": [
        {
          "id": "fireworks",
          "litellm_model_name": "fireworks/mistral/mixtral-8x22b-instruct",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/mistral.svg",
      "description": "Mistral AI's Mixtral 8X22B Instruct model with a 66K tokens context window available via Fireworks AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$1.20",
        "output_per_1m": "$1.20",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 66000,
      "max_output": 16500
    },
    {
      "id": "nova-lite",
      "name": "Nova Lite",
      "maker": "amazon",
      "providers": [
        {
          "id": "aws-bedrock",
          "litellm_model_name": "bedrock/amazon.nova-lite",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/amazon.svg",
      "description": "Amazon's Nova Lite model with a 300K tokens context window available via AWS Bedrock",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.06",
        "output_per_1m": "$0.24",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 300000,
      "max_output": 75000
    },
    {
      "id": "nova-micro",
      "name": "Nova Micro",
      "maker": "amazon",
      "providers": [
        {
          "id": "aws-bedrock",
          "litellm_model_name": "bedrock/amazon.nova-micro",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/amazon.svg",
      "description": "Amazon's Nova Micro model with a 128K tokens context window available via AWS Bedrock",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.04",
        "output_per_1m": "$0.14",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 128000,
      "max_output": 32000
    },
    {
      "id": "nova-pro",
      "name": "Nova Pro",
      "maker": "amazon",
      "providers": [
        {
          "id": "aws-bedrock",
          "litellm_model_name": "bedrock/amazon.nova-pro",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/amazon.svg",
      "description": "Amazon's Nova Pro model with a 300K tokens context window available via AWS Bedrock",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.80",
        "output_per_1m": "$3.20",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 300000,
      "max_output": 75000
    },
    {
      "id": "o1",
      "name": "O1",
      "maker": "openai",
      "providers": [
        {
          "id": "azure",
          "litellm_model_name": "azure/o1",
          "is_default": false
        },
        {
          "id": "openai",
          "litellm_model_name": "openai/o1",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/openai.svg",
      "description": "OpenAI's O1 model with a 200K tokens context window available via Azure OpenAI, OpenAI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$15.00",
        "output_per_1m": "$60.00",
        "tier": "premium"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 200000,
      "max_output": 50000
    },
    {
      "id": "o3",
      "name": "O3",
      "maker": "openai",
      "providers": [
        {
          "id": "openai",
          "litellm_model_name": "openai/o3",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/openai.svg",
      "description": "OpenAI's O3 model with a 200K tokens context window available via OpenAI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$2.00",
        "output_per_1m": "$8.00",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 200000,
      "max_output": 50000
    },
    {
      "id": "o3-deep-research",
      "name": "O3 Deep Research",
      "maker": "openai",
      "providers": [
        {
          "id": "openai",
          "litellm_model_name": "openai/o3-deep-research",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/openai.svg",
      "description": "OpenAI's O3 Deep Research model with a 200K tokens context window available via OpenAI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$10.00",
        "output_per_1m": "$40.00",
        "tier": "premium"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 200000,
      "max_output": 50000
    },
    {
      "id": "o3-mini",
      "name": "O3 Mini",
      "maker": "openai",
      "providers": [
        {
          "id": "azure",
          "litellm_model_name": "azure/o3-mini",
          "is_default": false
        },
        {
          "id": "openai",
          "litellm_model_name": "openai/o3-mini",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/openai.svg",
      "description": "OpenAI's O3 Mini model with a 200K tokens context window available via Azure OpenAI, OpenAI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$1.10",
        "output_per_1m": "$4.40",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 200000,
      "max_output": 50000
    },
    {
      "id": "o4-mini",
      "name": "O4 Mini",
      "maker": "openai",
      "providers": [
        {
          "id": "azure",
          "litellm_model_name": "azure/o4-mini",
          "is_default": false
        },
        {
          "id": "openai",
          "litellm_model_name": "openai/o4-mini",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/openai.svg",
      "description": "OpenAI's O4 Mini model with a 200K tokens context window available via Azure OpenAI, OpenAI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$1.10",
        "output_per_1m": "$4.40",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 200000,
      "max_output": 50000
    },
    {
      "id": "pixtral-12b",
      "name": "Pixtral 12B",
      "maker": "mistral",
      "providers": [
        {
          "id": "mistral",
          "litellm_model_name": "mistral/pixtral-12b",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/mistral.svg",
      "description": "Mistral AI's Pixtral 12B model with a 128K tokens context window available via Mistral AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.15",
        "output_per_1m": "$0.15",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 128000,
      "max_output": 32000
    },
    {
      "id": "pixtral-large",
      "name": "Pixtral Large",
      "maker": "mistral",
      "providers": [
        {
          "id": "mistral",
          "litellm_model_name": "mistral/pixtral-large",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/mistral.svg",
      "description": "Mistral AI's Pixtral Large model with a 128K tokens context window available via Mistral AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$2.00",
        "output_per_1m": "$6.00",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 128000,
      "max_output": 32000
    },
    {
      "id": "qwen-3-14b",
      "name": "Qwen 3 14B",
      "maker": "qwen",
      "providers": [
        {
          "id": "deepinfra",
          "litellm_model_name": "deepinfra/alibaba/qwen-3-14b",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/qwen.svg",
      "description": "Alibaba Qwen's Qwen 3 14B model with a 41K tokens context window available via DeepInfra",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.06",
        "output_per_1m": "$0.24",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 41000,
      "max_output": 10250
    },
    {
      "id": "qwen-3-235b",
      "name": "Qwen 3 235B",
      "maker": "qwen",
      "providers": [
        {
          "id": "baseten",
          "litellm_model_name": "baseten/alibaba/qwen-3-235b",
          "is_default": true
        },
        {
          "id": "deepinfra",
          "litellm_model_name": "deepinfra/alibaba/qwen-3-235b",
          "is_default": false
        },
        {
          "id": "fireworks",
          "litellm_model_name": "fireworks/alibaba/qwen-3-235b",
          "is_default": false
        },
        {
          "id": "novita",
          "litellm_model_name": "novita/alibaba/qwen-3-235b",
          "is_default": false
        }
      ],
      "icon": "@/assets/icons/qwen.svg",
      "description": "Alibaba Qwen's Qwen 3 235B model with a 262K tokens context window available via Baseten, DeepInfra, Fireworks AI, Novita AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.13",
        "output_per_1m": "$0.60",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 262000,
      "max_output": 65500
    },
    {
      "id": "qwen-3-30b",
      "name": "Qwen 3 30B",
      "maker": "qwen",
      "providers": [
        {
          "id": "deepinfra",
          "litellm_model_name": "deepinfra/alibaba/qwen-3-30b",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/qwen.svg",
      "description": "Alibaba Qwen's Qwen 3 30B model with a 41K tokens context window available via DeepInfra",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.08",
        "output_per_1m": "$0.29",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 41000,
      "max_output": 10250
    },
    {
      "id": "qwen-3-32b",
      "name": "Qwen 3 32B",
      "maker": "qwen",
      "providers": [
        {
          "id": "aws-bedrock",
          "litellm_model_name": "bedrock/alibaba.qwen-3-32b",
          "is_default": true
        },
        {
          "id": "cerebras",
          "litellm_model_name": "cerebras/alibaba/qwen-3-32b",
          "is_default": false
        },
        {
          "id": "deepinfra",
          "litellm_model_name": "deepinfra/alibaba/qwen-3-32b",
          "is_default": false
        },
        {
          "id": "groq",
          "litellm_model_name": "groq/alibaba/qwen-3-32b",
          "is_default": false
        }
      ],
      "icon": "@/assets/icons/qwen.svg",
      "description": "Alibaba Qwen's Qwen 3 32B model with a 128K tokens context window available via AWS Bedrock, Cerebras, DeepInfra, Groq",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.10",
        "output_per_1m": "$0.30",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 128000,
      "max_output": 32000
    },
    {
      "id": "qwen3-coder",
      "name": "Qwen3 Coder",
      "maker": "qwen",
      "providers": [
        {
          "id": "baseten",
          "litellm_model_name": "baseten/alibaba/qwen3-coder",
          "is_default": true
        },
        {
          "id": "cerebras",
          "litellm_model_name": "cerebras/alibaba/qwen3-coder",
          "is_default": false
        },
        {
          "id": "deepinfra",
          "litellm_model_name": "deepinfra/alibaba/qwen3-coder",
          "is_default": false
        },
        {
          "id": "novita",
          "litellm_model_name": "novita/alibaba/qwen3-coder",
          "is_default": false
        }
      ],
      "icon": "@/assets/icons/qwen.svg",
      "description": "Alibaba Qwen's Qwen3 Coder model with a 131K tokens context window available via Baseten, Cerebras, DeepInfra, Novita AI",
      "capabilities": [
        "chat",
        "code",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.40",
        "output_per_1m": "$1.60",
        "tier": "standard"
      },
      "use_cases": [
        "Developer copilots and IDE integrations",
        "Automated code review and refactoring",
        "Scripting and workflow automation",
        "Complex analysis and planning tasks",
        "Decision support and strategy"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 131000,
      "max_output": 32750
    },
    {
      "id": "qwen3-coder-30b-a3b",
      "name": "Qwen3 Coder 30B A3b",
      "maker": "qwen",
      "providers": [
        {
          "id": "aws-bedrock",
          "litellm_model_name": "bedrock/alibaba.qwen3-coder-30b-a3b",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/qwen.svg",
      "description": "Alibaba Qwen's Qwen3 Coder 30B A3b model with a 262K tokens context window available via AWS Bedrock",
      "capabilities": [
        "chat",
        "code",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.15",
        "output_per_1m": "$0.60",
        "tier": "standard"
      },
      "use_cases": [
        "Developer copilots and IDE integrations",
        "Automated code review and refactoring",
        "Scripting and workflow automation",
        "Complex analysis and planning tasks",
        "Decision support and strategy"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 262000,
      "max_output": 65500
    },
    {
      "id": "qwen3-coder-plus",
      "name": "Qwen3 Coder Plus",
      "maker": "qwen",
      "providers": [
        {
          "id": "alibabacloud",
          "litellm_model_name": "alibabacloud/alibaba/qwen3-coder-plus",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/qwen.svg",
      "description": "Alibaba Qwen's Qwen3 Coder Plus model with a 1M tokens context window available via Alibaba Cloud",
      "capabilities": [
        "chat",
        "code",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$1.00",
        "output_per_1m": "$5.00",
        "tier": "standard"
      },
      "use_cases": [
        "Developer copilots and IDE integrations",
        "Automated code review and refactoring",
        "Scripting and workflow automation",
        "Complex analysis and planning tasks",
        "Decision support and strategy"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 1000000,
      "max_output": 250000
    },
    {
      "id": "qwen3-embedding-0.6b",
      "name": "Qwen3 Embedding 0.6B",
      "maker": "qwen",
      "providers": [
        {
          "id": "openrouter",
          "litellm_model_name": "openrouter/qwen/qwen-3-embedding-0.6b",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/qwen.svg",
      "description": "Qwen's Qwen3 Embedding 0.6B model available via OpenRouter",
      "capabilities": [
        "embeddings"
      ],
      "pricing": {
        "input_per_1m": "$0.01",
        "output_per_1m": "—",
        "tier": "budget"
      },
      "use_cases": [
        "Semantic search and retrieval",
        "Vector database indexing",
        "Personalization and recommendations"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true
    },
    {
      "id": "qwen3-embedding-4b",
      "name": "Qwen3 Embedding 4B",
      "maker": "qwen",
      "providers": [
        {
          "id": "openrouter",
          "litellm_model_name": "openrouter/qwen/qwen-3-embedding-4b",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/qwen.svg",
      "description": "Qwen's Qwen3 Embedding 4B model available via OpenRouter",
      "capabilities": [
        "embeddings"
      ],
      "pricing": {
        "input_per_1m": "$0.02",
        "output_per_1m": "—",
        "tier": "budget"
      },
      "use_cases": [
        "Semantic search and retrieval",
        "Vector database indexing",
        "Personalization and recommendations"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true
    },
    {
      "id": "qwen3-embedding-8b",
      "name": "Qwen3 Embedding 8B",
      "maker": "qwen",
      "providers": [
        {
          "id": "openrouter",
          "litellm_model_name": "openrouter/qwen/qwen-3-embedding-8b",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/qwen.svg",
      "description": "Qwen's Qwen3 Embedding 8B model available via OpenRouter",
      "capabilities": [
        "embeddings"
      ],
      "pricing": {
        "input_per_1m": "$0.01",
        "output_per_1m": "—",
        "tier": "budget"
      },
      "use_cases": [
        "Semantic search and retrieval",
        "Vector database indexing",
        "Personalization and recommendations"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true
    },
    {
      "id": "qwen3-max",
      "name": "Qwen3 Max",
      "maker": "qwen",
      "providers": [
        {
          "id": "alibabacloud",
          "litellm_model_name": "alibabacloud/alibaba/qwen3-max",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/qwen.svg",
      "description": "Alibaba Qwen's Qwen3 Max model with a 262K tokens context window available via Alibaba Cloud",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$1.20",
        "output_per_1m": "$6.00",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 262000,
      "max_output": 65500
    },
    {
      "id": "qwen3-max-preview",
      "name": "Qwen3 Max Preview",
      "maker": "qwen",
      "providers": [
        {
          "id": "alibabacloud",
          "litellm_model_name": "alibabacloud/alibaba/qwen3-max-preview",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/qwen.svg",
      "description": "Alibaba Qwen's Qwen3 Max Preview model with a 262K tokens context window available via Alibaba Cloud",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$1.20",
        "output_per_1m": "$6.00",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 262000,
      "max_output": 65500
    },
    {
      "id": "qwen3-next-80b-a3b-instruct",
      "name": "Qwen3 Next 80B A3b Instruct",
      "maker": "qwen",
      "providers": [
        {
          "id": "alibabacloud",
          "litellm_model_name": "alibabacloud/alibaba/qwen3-next-80b-a3b-instruct",
          "is_default": true
        },
        {
          "id": "novita",
          "litellm_model_name": "novita/alibaba/qwen3-next-80b-a3b-instruct",
          "is_default": false
        }
      ],
      "icon": "@/assets/icons/qwen.svg",
      "description": "Alibaba Qwen's Qwen3 Next 80B A3b Instruct model with a 131K tokens context window available via Alibaba Cloud, Novita AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.15",
        "output_per_1m": "$1.50",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 131000,
      "max_output": 32750
    },
    {
      "id": "qwen3-next-80b-a3b-thinking",
      "name": "Qwen3 Next 80B A3b Thinking",
      "maker": "qwen",
      "providers": [
        {
          "id": "alibabacloud",
          "litellm_model_name": "alibabacloud/alibaba/qwen3-next-80b-a3b-thinking",
          "is_default": true
        },
        {
          "id": "novita",
          "litellm_model_name": "novita/alibaba/qwen3-next-80b-a3b-thinking",
          "is_default": false
        }
      ],
      "icon": "@/assets/icons/qwen.svg",
      "description": "Alibaba Qwen's Qwen3 Next 80B A3b Thinking model with a 131K tokens context window available via Alibaba Cloud, Novita AI",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.15",
        "output_per_1m": "$1.50",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 131000,
      "max_output": 32750
    },
    {
      "id": "qwen3-vl-instruct",
      "name": "Qwen3 VL Instruct",
      "maker": "qwen",
      "providers": [
        {
          "id": "alibabacloud",
          "litellm_model_name": "alibabacloud/alibaba/qwen3-vl-instruct",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/qwen.svg",
      "description": "Alibaba Qwen's Qwen3 VL Instruct model with a 131K tokens context window available via Alibaba Cloud",
      "capabilities": [
        "chat",
        "vision",
        "multimodal",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.70",
        "output_per_1m": "$2.80",
        "tier": "standard"
      },
      "use_cases": [
        "Multimodal assistants and copilots",
        "Document and image understanding",
        "Visual reasoning and analysis",
        "Complex analysis and planning tasks",
        "Decision support and strategy"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 131000,
      "max_output": 32750
    },
    {
      "id": "qwen3-vl-thinking",
      "name": "Qwen3 VL Thinking",
      "maker": "qwen",
      "providers": [
        {
          "id": "alibabacloud",
          "litellm_model_name": "alibabacloud/alibaba/qwen3-vl-thinking",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/qwen.svg",
      "description": "Alibaba Qwen's Qwen3 VL Thinking model with a 131K tokens context window available via Alibaba Cloud",
      "capabilities": [
        "chat",
        "vision",
        "multimodal",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$0.70",
        "output_per_1m": "$8.40",
        "tier": "standard"
      },
      "use_cases": [
        "Multimodal assistants and copilots",
        "Document and image understanding",
        "Visual reasoning and analysis",
        "Complex analysis and planning tasks",
        "Decision support and strategy"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 131000,
      "max_output": 32750
    },
    {
      "id": "sonar",
      "name": "Sonar",
      "maker": "perplexity",
      "providers": [
        {
          "id": "perplexity",
          "litellm_model_name": "perplexity/sonar",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/perplexity.svg",
      "description": "Perplexity AI's Sonar model with a 127K tokens context window available via Perplexity",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$1.00",
        "output_per_1m": "$1.00",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 127000,
      "max_output": 31750
    },
    {
      "id": "sonar-pro",
      "name": "Sonar Pro",
      "maker": "perplexity",
      "providers": [
        {
          "id": "perplexity",
          "litellm_model_name": "perplexity/sonar-pro",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/perplexity.svg",
      "description": "Perplexity AI's Sonar Pro model with a 200K tokens context window available via Perplexity",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$3.00",
        "output_per_1m": "$15.00",
        "tier": "premium"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 200000,
      "max_output": 50000
    },
    {
      "id": "sonar-reasoning",
      "name": "Sonar Reasoning",
      "maker": "perplexity",
      "providers": [
        {
          "id": "perplexity",
          "litellm_model_name": "perplexity/sonar-reasoning",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/perplexity.svg",
      "description": "Perplexity AI's Sonar Reasoning model with a 127K tokens context window available via Perplexity",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$1.00",
        "output_per_1m": "$5.00",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 127000,
      "max_output": 31750
    },
    {
      "id": "sonar-reasoning-pro",
      "name": "Sonar Reasoning Pro",
      "maker": "perplexity",
      "providers": [
        {
          "id": "perplexity",
          "litellm_model_name": "perplexity/sonar-reasoning-pro",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/perplexity.svg",
      "description": "Perplexity AI's Sonar Reasoning Pro model with a 127K tokens context window available via Perplexity",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "pricing": {
        "input_per_1m": "$2.00",
        "output_per_1m": "$8.00",
        "tier": "standard"
      },
      "use_cases": [
        "Complex analysis and planning tasks",
        "Decision support and strategy",
        "Conversational AI assistants",
        "Knowledge base Q&A",
        "Content generation and summarization"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true,
      "context_window": 127000,
      "max_output": 31750
    },
    {
      "id": "text-embedding-005",
      "name": "Text Embedding 005",
      "maker": "google",
      "providers": [
        {
          "id": "vertex-ai",
          "litellm_model_name": "vertex_ai/text-embedding-005",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/google.svg",
      "description": "Google DeepMind's Text Embedding 005 model available via Vertex AI",
      "capabilities": [
        "embeddings"
      ],
      "pricing": {
        "input_per_1m": "$0.03",
        "output_per_1m": "—",
        "tier": "budget"
      },
      "use_cases": [
        "Semantic search and retrieval",
        "Vector database indexing",
        "Personalization and recommendations"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true
    },
    {
      "id": "text-embedding-3-large",
      "name": "Text Embedding 3 Large",
      "maker": "openai",
      "providers": [
        {
          "id": "azure",
          "litellm_model_name": "azure/text-embedding-3-large",
          "is_default": false
        },
        {
          "id": "openai",
          "litellm_model_name": "openai/text-embedding-3-large",
          "is_default": true
        },
        {
          "id": "openrouter",
          "litellm_model_name": "openrouter/openai/text-embedding-3-large",
          "is_default": false
        }
      ],
      "icon": "@/assets/icons/openai.svg",
      "description": "OpenAI's Text Embedding 3 Large model available via Azure OpenAI, OpenAI, OpenRouter",
      "capabilities": [
        "embeddings"
      ],
      "pricing": {
        "input_per_1m": "$0.13",
        "output_per_1m": "—",
        "tier": "standard"
      },
      "use_cases": [
        "Semantic search and retrieval",
        "Vector database indexing",
        "Personalization and recommendations"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true
    },
    {
      "id": "text-embedding-3-small",
      "name": "Text Embedding 3 Small",
      "maker": "openai",
      "providers": [
        {
          "id": "azure",
          "litellm_model_name": "azure/text-embedding-3-small",
          "is_default": false
        },
        {
          "id": "openai",
          "litellm_model_name": "openai/text-embedding-3-small",
          "is_default": true
        },
        {
          "id": "openrouter",
          "litellm_model_name": "openrouter/openai/text-embedding-3-small",
          "is_default": false
        }
      ],
      "icon": "@/assets/icons/openai.svg",
      "description": "OpenAI's Text Embedding 3 Small model available via Azure OpenAI, OpenAI, OpenRouter",
      "capabilities": [
        "embeddings"
      ],
      "pricing": {
        "input_per_1m": "$0.02",
        "output_per_1m": "—",
        "tier": "budget"
      },
      "use_cases": [
        "Semantic search and retrieval",
        "Vector database indexing",
        "Personalization and recommendations"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true
    },
    {
      "id": "text-embedding-ada-002",
      "name": "Text Embedding Ada 002",
      "maker": "openai",
      "providers": [
        {
          "id": "azure",
          "litellm_model_name": "azure/text-embedding-ada-002",
          "is_default": false
        },
        {
          "id": "openai",
          "litellm_model_name": "openai/text-embedding-ada-002",
          "is_default": true
        },
        {
          "id": "openrouter",
          "litellm_model_name": "openrouter/openai/text-embedding-ada-002",
          "is_default": false
        }
      ],
      "icon": "@/assets/icons/openai.svg",
      "description": "OpenAI's Text Embedding Ada 002 model available via Azure OpenAI, OpenAI, OpenRouter",
      "capabilities": [
        "embeddings"
      ],
      "pricing": {
        "input_per_1m": "$0.10",
        "output_per_1m": "—",
        "tier": "budget"
      },
      "use_cases": [
        "Semantic search and retrieval",
        "Vector database indexing",
        "Personalization and recommendations"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true
    },
    {
      "id": "text-multilingual-embedding-002",
      "name": "Text Multilingual Embedding 002",
      "maker": "google",
      "providers": [
        {
          "id": "vertex-ai",
          "litellm_model_name": "vertex_ai/text-multilingual-embedding-002",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/google.svg",
      "description": "Google DeepMind's Text Multilingual Embedding 002 model available via Vertex AI",
      "capabilities": [
        "embeddings"
      ],
      "pricing": {
        "input_per_1m": "$0.03",
        "output_per_1m": "—",
        "tier": "budget"
      },
      "use_cases": [
        "Semantic search and retrieval",
        "Vector database indexing",
        "Personalization and recommendations"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true
    },
    {
      "id": "titan-embed-text-v2",
      "name": "Titan Embed Text V2",
      "maker": "amazon",
      "providers": [
        {
          "id": "aws-bedrock",
          "litellm_model_name": "bedrock/amazon.titan-embed-text-v2",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/amazon.svg",
      "description": "Amazon's Titan Embed Text V2 model available via AWS Bedrock",
      "capabilities": [
        "embeddings"
      ],
      "pricing": {
        "input_per_1m": "$0.02",
        "output_per_1m": "—",
        "tier": "budget"
      },
      "use_cases": [
        "Semantic search and retrieval",
        "Vector database indexing",
        "Personalization and recommendations"
      ],
      "notes": "Pricing and availability aggregated from provider catalogs.",
      "enabled": true
    },
    {
      "id": "voyage-3-large",
      "name": "Voyage 3 Large",
      "maker": "voyage",
      "providers": [
        {
          "id": "voyage",
          "litellm_model_name": "voyage/voyage-3-large",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/voyage.svg",
      "description": "High-capacity embedding model with 10B parameters for premium retrieval tasks",
      "capabilities": [
        "embeddings"
      ],
      "pricing": {
        "input_per_1m": "$0.18",
        "output_per_1m": "—",
        "tier": "standard"
      },
      "use_cases": [
        "High fidelity semantic search",
        "Enterprise RAG workloads",
        "Large document indexing",
        "Advanced similarity ranking"
      ],
      "enabled": true,
      "embedding_dimension": 1536,
      "notes": "Premium embedding quality for demanding retrieval tasks"
    },
    {
      "id": "voyage-3.5",
      "name": "Voyage 3.5",
      "maker": "voyage",
      "providers": [
        {
          "id": "voyage",
          "litellm_model_name": "voyage/voyage-3.5",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/voyage.svg",
      "description": "Balanced embedding model for general-purpose semantic search and retrieval",
      "capabilities": [
        "embeddings"
      ],
      "pricing": {
        "input_per_1m": "$0.06",
        "output_per_1m": "—",
        "tier": "standard"
      },
      "use_cases": [
        "General semantic search",
        "RAG applications",
        "Document similarity",
        "Content recommendation"
      ],
      "enabled": true
    },
    {
      "id": "voyage-3.5-lite",
      "name": "Voyage 3.5 Lite",
      "maker": "voyage",
      "providers": [
        {
          "id": "voyage",
          "litellm_model_name": "voyage/voyage-3.5-lite",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/voyage.svg",
      "description": "Lightweight embedding model optimized for speed and cost efficiency",
      "capabilities": [
        "embeddings"
      ],
      "pricing": {
        "input_per_1m": "$0.02",
        "output_per_1m": "—",
        "tier": "budget"
      },
      "use_cases": [
        "High-volume semantic search",
        "Budget-conscious RAG pipelines",
        "Real-time retrieval",
        "Vector database indexing"
      ],
      "enabled": true
    },
    {
      "id": "voyage-code-2",
      "name": "Voyage Code 2",
      "maker": "voyage",
      "providers": [
        {
          "id": "voyage",
          "litellm_model_name": "voyage/voyage-code-2",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/voyage.svg",
      "description": "Previous generation code embedding model with balanced performance and cost",
      "capabilities": [
        "embeddings"
      ],
      "pricing": {
        "input_per_1m": "$0.12",
        "output_per_1m": "—",
        "tier": "standard"
      },
      "use_cases": [
        "Code search",
        "Codebase navigation",
        "Function similarity",
        "API documentation retrieval"
      ],
      "enabled": true,
      "notes": "Predecessor to Voyage Code 3, still recommended for cost-sensitive applications"
    },
    {
      "id": "voyage-code-3",
      "name": "Voyage Code 3",
      "maker": "voyage",
      "providers": [
        {
          "id": "voyage",
          "litellm_model_name": "voyage/voyage-code-3",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/voyage.svg",
      "description": "Advanced code embedding model for semantic code search and repository analysis",
      "capabilities": [
        "embeddings"
      ],
      "pricing": {
        "input_per_1m": "$0.18",
        "output_per_1m": "—",
        "tier": "standard"
      },
      "use_cases": [
        "Code semantic search",
        "Repository indexing",
        "Code similarity detection",
        "Developer copilot features"
      ],
      "enabled": true,
      "notes": "Trained on code repositories with support for 100+ programming languages"
    },
    {
      "id": "voyage-finance-2",
      "name": "Voyage Finance 2",
      "maker": "voyage",
      "providers": [
        {
          "id": "voyage",
          "litellm_model_name": "voyage/voyage-finance-2",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/voyage.svg",
      "description": "Domain-specific embedding model optimized for financial documents and analysis",
      "capabilities": [
        "embeddings"
      ],
      "pricing": {
        "input_per_1m": "$0.12",
        "output_per_1m": "—",
        "tier": "standard"
      },
      "use_cases": [
        "Financial document retrieval",
        "Earnings report analysis",
        "Market research search",
        "Regulatory compliance"
      ],
      "enabled": true,
      "notes": "Fine-tuned on financial terminology, reports, and market data"
    },
    {
      "id": "voyage-law-2",
      "name": "Voyage Law 2",
      "maker": "voyage",
      "providers": [
        {
          "id": "voyage",
          "litellm_model_name": "voyage/voyage-law-2",
          "is_default": true
        }
      ],
      "icon": "@/assets/icons/voyage.svg",
      "description": "Specialized embedding model fine-tuned for legal document retrieval and analysis",
      "capabilities": [
        "embeddings"
      ],
      "pricing": {
        "input_per_1m": "$0.12",
        "output_per_1m": "—",
        "tier": "standard"
      },
      "use_cases": [
        "Legal document search",
        "Case law retrieval",
        "Contract analysis",
        "Legal research assistants"
      ],
      "enabled": true,
      "notes": "Optimized for legal domain terminology and document structures"
    }
  ],
  "localModels": [
    {
      "id": "gemma-3-12b-it",
      "name": "Gemma 3 12B Instruct",
      "maker": "google",
      "providers": [
        {
          "id": "llama-cpp",
          "model_uri": "huggingface://google/gemma-3-12b-it-GGUF/gemma-3-12b-it-Q8_K_XL.gguf",
          "is_default": true
        }
      ],
      "quantization": "Q8_K_XL",
      "ram_required_gb": 28,
      "context_window": 8192,
      "group": "reasoning",
      "icon": "@/assets/icons/gemma.svg",
      "description": "Balanced Gemma 3 instruction model for higher-quality on-device reasoning",
      "family": "gemma",
      "capabilities": [
        "chat",
        "code",
        "reasoning"
      ],
      "ctx_size": 8192,
      "ttl": 900,
      "hf_repo": "google/gemma-3-12b-it-GGUF",
      "hf_file": "gemma-3-12b-it-Q8_K_XL.gguf",
      "vulkan_driver": "AMDVLK",
      "flash_attn": true,
      "enabled": false,
      "use_cases": [
        "Productivity copilots",
        "Analytical writing",
        "Intermediate software design",
        "Multilingual chat"
      ],
      "quantization_options": [
        {
          "level": "Q8_K_XL",
          "hf_file": "gemma-3-12b-it-Q8_K_XL.gguf",
          "ram_required_gb": 28,
          "recommended_backend": "vulkan_amdvlk"
        }
      ],
      "benchmarks": {
        "prompt_processing": {
          "context": "pp512",
          "metric": "tokens_per_second",
          "winner": "rocm6_4_4-rocwmma",
          "backends": [
            {
              "id": "rocm6_4_4-rocwmma",
              "throughput": 809.2,
              "stddev": 0.75
            },
            {
              "id": "rocm7_rc-rocwmma",
              "throughput": 799.43,
              "stddev": 1.14
            },
            {
              "id": "vulkan_amdvlk",
              "throughput": 648.34,
              "stddev": 0.61
            },
            {
              "id": "vulkan_radv",
              "throughput": 532.11,
              "stddev": 3.15
            }
          ]
        },
        "text_generation": {
          "context": "tg128",
          "metric": "tokens_per_second",
          "winner": "vulkan_amdvlk",
          "backends": [
            {
              "id": "rocm6_4_4-rocwmma",
              "throughput": 14.11,
              "stddev": 0
            },
            {
              "id": "rocm7_rc-rocwmma",
              "throughput": 14.11,
              "stddev": 0
            },
            {
              "id": "vulkan_amdvlk",
              "throughput": 14.52,
              "stddev": 0
            },
            {
              "id": "vulkan_radv",
              "throughput": 13.98,
              "stddev": 0
            }
          ]
        }
      },
      "notes": "Consistent throughput across ROCm and Vulkan backends; data collected with pp512/tg128 workloads."
    },
    {
      "id": "gemma-3-27b-it",
      "name": "Gemma 3 27B Instruct",
      "maker": "google",
      "providers": [
        {
          "id": "llama-cpp",
          "model_uri": "huggingface://google/gemma-3-27b-it-GGUF/gemma-3-27b-it-BF16.gguf",
          "is_default": true
        }
      ],
      "quantization": "BF16",
      "ram_required_gb": 64,
      "context_window": 8192,
      "group": "heavy",
      "icon": "@/assets/icons/gemma.svg",
      "description": "Largest Gemma 3 instruction variant for top-tier quality and reasoning",
      "family": "gemma",
      "capabilities": [
        "chat",
        "code",
        "reasoning"
      ],
      "ctx_size": 8192,
      "ttl": 1200,
      "hf_repo": "google/gemma-3-27b-it-GGUF",
      "hf_file": "gemma-3-27b-it-BF16.gguf",
      "vulkan_driver": "AMDVLK",
      "flash_attn": true,
      "enabled": false,
      "use_cases": [
        "Enterprise copilots",
        "Complex analytical workflows",
        "High-accuracy summarization",
        "Advanced planning and coding"
      ],
      "quantization_options": [
        {
          "level": "BF16",
          "hf_file": "gemma-3-27b-it-BF16.gguf",
          "ram_required_gb": 64,
          "recommended_backend": "rocm6_4_4-rocwmma"
        }
      ],
      "benchmarks": {
        "prompt_processing": {
          "context": "pp512",
          "metric": "tokens_per_second",
          "winner": "rocm6_4_4-rocwmma",
          "backends": [
            {
              "id": "rocm6_4_4-rocwmma",
              "throughput": 468.87,
              "stddev": 0.84
            },
            {
              "id": "rocm7_rc-rocwmma",
              "throughput": 464.58,
              "stddev": 0.58
            },
            {
              "id": "vulkan_amdvlk",
              "throughput": null,
              "stddev": null,
              "status": "load_error"
            },
            {
              "id": "vulkan_radv",
              "throughput": 107.33,
              "stddev": 1
            }
          ]
        },
        "text_generation": {
          "context": "tg128",
          "metric": "tokens_per_second",
          "winner": "rocm6_4_4-rocwmma",
          "backends": [
            {
              "id": "rocm6_4_4-rocwmma",
              "throughput": 4.11,
              "stddev": 0
            },
            {
              "id": "rocm7_rc-rocwmma",
              "throughput": 4.11,
              "stddev": 0
            },
            {
              "id": "vulkan_amdvlk",
              "throughput": null,
              "stddev": null,
              "status": "load_error"
            },
            {
              "id": "vulkan_radv",
              "throughput": 3.91,
              "stddev": 0
            }
          ]
        }
      },
      "notes": "Requires high-end hardware (BF16) but delivers flagship Gemma quality; Vulkan AMDVLK backend currently fails to load."
    },
    {
      "id": "gemma-3-4b-it",
      "name": "Gemma 3 4B Instruct",
      "maker": "google",
      "providers": [
        {
          "id": "llama-cpp",
          "model_uri": "huggingface://google/gemma-3-4b-it-GGUF/gemma-3-4b-it-Q3_K_S.gguf",
          "is_default": true
        }
      ],
      "quantization": "Q3_K_S",
      "ram_required_gb": 5,
      "context_window": 8192,
      "group": "task",
      "icon": "@/assets/icons/gemma.svg",
      "description": "Compact Gemma 3 assistant fine-tune for fast on-device reasoning and chat",
      "family": "gemma",
      "capabilities": [
        "chat",
        "code"
      ],
      "ctx_size": 8192,
      "ttl": 600,
      "hf_repo": "google/gemma-3-4b-it-GGUF",
      "hf_file": "gemma-3-4b-it-Q3_K_S.gguf",
      "vulkan_driver": "RADV",
      "flash_attn": true,
      "enabled": false,
      "use_cases": [
        "Snappy personal assistants",
        "Edge deployments with tight VRAM budgets",
        "Lightweight code comprehension",
        "Conversational agents for quick answers"
      ],
      "quantization_options": [
        {
          "level": "Q3_K_S",
          "hf_file": "gemma-3-4b-it-Q3_K_S.gguf",
          "ram_required_gb": 5,
          "recommended_backend": "rocm6_4_4-rocwmma"
        }
      ],
      "benchmarks": {
        "prompt_processing": {
          "context": "pp512",
          "metric": "tokens_per_second",
          "winner": "rocm6_4_4-rocwmma",
          "backends": [
            {
              "id": "rocm6_4_4-rocwmma",
              "throughput": 2278.78,
              "stddev": 8.79
            },
            {
              "id": "rocm7_rc-rocwmma",
              "throughput": 2249.97,
              "stddev": 8.38
            },
            {
              "id": "vulkan_amdvlk",
              "throughput": 1193.42,
              "stddev": 154
            },
            {
              "id": "vulkan_radv",
              "throughput": 1150.84,
              "stddev": 174.29
            }
          ]
        },
        "text_generation": {
          "context": "tg128",
          "metric": "tokens_per_second",
          "winner": "vulkan_radv",
          "backends": [
            {
              "id": "rocm6_4_4-rocwmma",
              "throughput": 76.94,
              "stddev": 0.01
            },
            {
              "id": "rocm7_rc-rocwmma",
              "throughput": 77.23,
              "stddev": 0.02
            },
            {
              "id": "vulkan_amdvlk",
              "throughput": 82.87,
              "stddev": 1.37
            },
            {
              "id": "vulkan_radv",
              "throughput": 85.89,
              "stddev": 0.11
            }
          ]
        }
      },
      "notes": "Benchmark data captured from pp512/tg128 tests; ideal for ultra-fast responses with ROCm or RADV backends."
    },
    {
      "id": "glm-4.5-air",
      "name": "GLM 4.5 Air",
      "maker": "zai",
      "providers": [
        {
          "id": "llama-cpp",
          "model_uri": "huggingface://ZhipuAI/glm-4.5-air-GGUF/glm-4.5-air-Q4_K_XL.gguf",
          "is_default": true
        }
      ],
      "quantization": "Q4_K_XL",
      "ram_required_gb": 72,
      "context_window": 131072,
      "group": "heavy",
      "icon": "@/assets/icons/zai.svg",
      "description": "Zhipu AI's flagship GLM 4.5 Air model tuned for fast local inference",
      "family": "glm",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "ctx_size": 65536,
      "ttl": 1200,
      "hf_repo": "ZhipuAI/glm-4.5-air-GGUF",
      "hf_file": "glm-4.5-air-Q4_K_XL.gguf",
      "vulkan_driver": "RADV",
      "flash_attn": true,
      "enabled": false,
      "use_cases": [
        "Enterprise research assistants",
        "Deep analytical reasoning",
        "Large-context summarization",
        "Planning and strategy agents"
      ],
      "quantization_options": [
        {
          "level": "Q4_K_XL",
          "hf_file": "glm-4.5-air-Q4_K_XL.gguf",
          "ram_required_gb": 72,
          "recommended_backend": "vulkan_radv",
          "benchmarks": {
            "prompt_processing": {
              "context": "pp512",
              "metric": "tokens_per_second",
              "winner": "vulkan_radv",
              "backends": [
                {
                  "id": "rocm6_4_4-rocwmma",
                  "throughput": 134.2,
                  "stddev": 0.28
                },
                {
                  "id": "rocm7_rc-rocwmma",
                  "throughput": 67.08,
                  "stddev": 0.15
                },
                {
                  "id": "vulkan_amdvlk",
                  "throughput": 218.68,
                  "stddev": 0.54
                },
                {
                  "id": "vulkan_radv",
                  "throughput": 236.02,
                  "stddev": 2.6
                }
              ]
            },
            "text_generation": {
              "context": "tg128",
              "metric": "tokens_per_second",
              "winner": "vulkan_radv",
              "backends": [
                {
                  "id": "rocm6_4_4-rocwmma",
                  "throughput": 21.04,
                  "stddev": 0.01
                },
                {
                  "id": "rocm7_rc-rocwmma",
                  "throughput": 21.07,
                  "stddev": 0
                },
                {
                  "id": "vulkan_amdvlk",
                  "throughput": 24.27,
                  "stddev": 0.01
                },
                {
                  "id": "vulkan_radv",
                  "throughput": 24.51,
                  "stddev": 0.01
                }
              ]
            }
          }
        },
        {
          "level": "Q6_K_XL",
          "hf_file": "glm-4.5-air-Q6_K_XL.gguf",
          "ram_required_gb": 96,
          "recommended_backend": "vulkan_amdvlk",
          "benchmarks": {
            "prompt_processing": {
              "context": "pp512",
              "metric": "tokens_per_second",
              "winner": "vulkan_amdvlk",
              "backends": [
                {
                  "id": "rocm6_4_4-rocwmma",
                  "throughput": 128.51,
                  "stddev": 0.51
                },
                {
                  "id": "rocm7_rc-rocwmma",
                  "throughput": 69.91,
                  "stddev": 0.44
                },
                {
                  "id": "vulkan_amdvlk",
                  "throughput": 262.18,
                  "stddev": 1.19
                },
                {
                  "id": "vulkan_radv",
                  "throughput": 233.21,
                  "stddev": 6.28
                }
              ]
            },
            "text_generation": {
              "context": "tg128",
              "metric": "tokens_per_second",
              "winner": "vulkan_radv",
              "backends": [
                {
                  "id": "rocm6_4_4-rocwmma",
                  "throughput": 16.13,
                  "stddev": 0.16
                },
                {
                  "id": "rocm7_rc-rocwmma",
                  "throughput": 16.13,
                  "stddev": 0.11
                },
                {
                  "id": "vulkan_amdvlk",
                  "throughput": 17.3,
                  "stddev": 0.01
                },
                {
                  "id": "vulkan_radv",
                  "throughput": 17.65,
                  "stddev": 0.01
                }
              ]
            }
          }
        }
      ],
      "benchmarks": {
        "context": "pp512/tg128",
        "quantization": "Q4_K_XL",
        "prompt_processing": {
          "context": "pp512",
          "metric": "tokens_per_second",
          "winner": "vulkan_radv",
          "backends": [
            {
              "id": "rocm6_4_4-rocwmma",
              "throughput": 134.2,
              "stddev": 0.28
            },
            {
              "id": "rocm7_rc-rocwmma",
              "throughput": 67.08,
              "stddev": 0.15
            },
            {
              "id": "vulkan_amdvlk",
              "throughput": 218.68,
              "stddev": 0.54
            },
            {
              "id": "vulkan_radv",
              "throughput": 236.02,
              "stddev": 2.6
            }
          ]
        },
        "text_generation": {
          "context": "tg128",
          "metric": "tokens_per_second",
          "winner": "vulkan_radv",
          "backends": [
            {
              "id": "rocm6_4_4-rocwmma",
              "throughput": 21.04,
              "stddev": 0.01
            },
            {
              "id": "rocm7_rc-rocwmma",
              "throughput": 21.07,
              "stddev": 0
            },
            {
              "id": "vulkan_amdvlk",
              "throughput": 24.27,
              "stddev": 0.01
            },
            {
              "id": "vulkan_radv",
              "throughput": 24.51,
              "stddev": 0.01
            }
          ]
        }
      },
      "notes": "Dual quantization coverage with ROCm and Vulkan tuning; Q6_K_XL delivers higher quality if VRAM allows."
    },
    {
      "id": "gpt-oss-120b",
      "name": "GPT-OSS 120B",
      "maker": "openai",
      "providers": [
        {
          "id": "llama-cpp",
          "model_uri": "huggingface://openai/gpt-oss-120b-GGUF/gpt-oss-120b-mxfp4.gguf",
          "is_default": true
        }
      ],
      "quantization": "MXFP4",
      "ram_required_gb": 80,
      "context_window": 8192,
      "group": "heavy",
      "icon": "@/assets/icons/openai.svg",
      "description": "OpenAI's 120B parameter model - complex reasoning with MXFP4 throughput",
      "family": "gpt-oss",
      "capabilities": [
        "chat",
        "code",
        "reasoning"
      ],
      "ctx_size": 8192,
      "ttl": 900,
      "hf_repo": "openai/gpt-oss-120b-GGUF",
      "hf_file": "gpt-oss-120b-mxfp4.gguf",
      "vulkan_driver": "AMDVLK",
      "flash_attn": true,
      "enabled": true,
      "use_cases": [
        "Complex reasoning tasks",
        "Advanced code generation",
        "Research and analysis",
        "Long-form content"
      ],
      "quantization_options": [
        {
          "level": "MXFP4",
          "hf_file": "gpt-oss-120b-mxfp4.gguf",
          "ram_required_gb": 80,
          "recommended_backend": "vulkan_amdvlk",
          "benchmarks": {
            "prompt_processing": {
              "context": "pp512",
              "metric": "tokens_per_second",
              "winner": "vulkan_amdvlk",
              "backends": [
                {
                  "id": "rocm6_4_4-rocwmma",
                  "throughput": 767.28,
                  "stddev": 2.81
                },
                {
                  "id": "rocm7_rc-rocwmma",
                  "throughput": 756.58,
                  "stddev": 4.67
                },
                {
                  "id": "vulkan_amdvlk",
                  "throughput": 788.46,
                  "stddev": 4.36
                },
                {
                  "id": "vulkan_radv",
                  "throughput": 526.13,
                  "stddev": 3.2
                }
              ]
            },
            "text_generation": {
              "context": "tg128",
              "metric": "tokens_per_second",
              "winner": "vulkan_radv",
              "backends": [
                {
                  "id": "rocm6_4_4-rocwmma",
                  "throughput": 47.63,
                  "stddev": 0.01
                },
                {
                  "id": "rocm7_rc-rocwmma",
                  "throughput": 47.62,
                  "stddev": 0.01
                },
                {
                  "id": "vulkan_amdvlk",
                  "throughput": 50.32,
                  "stddev": 0.03
                },
                {
                  "id": "vulkan_radv",
                  "throughput": 52.9,
                  "stddev": 0.05
                }
              ]
            }
          }
        },
        {
          "level": "F16",
          "hf_file": "gpt-oss-120b-f16.gguf",
          "ram_required_gb": 128,
          "recommended_backend": "rocm6_4_4-rocwmma",
          "benchmarks": {
            "prompt_processing": {
              "context": "pp512",
              "metric": "tokens_per_second",
              "winner": "rocm6_4_4-rocwmma",
              "backends": [
                {
                  "id": "rocm6_4_4-rocwmma",
                  "throughput": 786.49,
                  "stddev": 4.02
                },
                {
                  "id": "rocm7_rc-rocwmma",
                  "throughput": 770.55,
                  "stddev": 4.47
                },
                {
                  "id": "vulkan_amdvlk",
                  "throughput": 719.39,
                  "stddev": 2.63
                },
                {
                  "id": "vulkan_radv",
                  "throughput": 481.71,
                  "stddev": 2.11
                }
              ]
            },
            "text_generation": {
              "context": "tg128",
              "metric": "tokens_per_second",
              "winner": "rocm6_4_4-rocwmma",
              "backends": [
                {
                  "id": "rocm6_4_4-rocwmma",
                  "throughput": 35.16,
                  "stddev": 0
                },
                {
                  "id": "rocm7_rc-rocwmma",
                  "throughput": 35.07,
                  "stddev": 0
                },
                {
                  "id": "vulkan_amdvlk",
                  "throughput": 34.71,
                  "stddev": 0.02
                },
                {
                  "id": "vulkan_radv",
                  "throughput": 34.46,
                  "stddev": 0.02
                }
              ]
            }
          }
        },
        {
          "level": "Q4_K_M",
          "hf_file": "gpt-oss-120b-q4_k_m.gguf",
          "ram_required_gb": 64,
          "recommended_backend": "vulkan_amdvlk",
          "notes": "Legacy quantization retained for compatibility"
        }
      ],
      "benchmarks": {
        "prompt_processing": {
          "context": "pp512",
          "metric": "tokens_per_second",
          "winner": "vulkan_amdvlk",
          "backends": [
            {
              "id": "rocm6_4_4-rocwmma",
              "throughput": 767.28,
              "stddev": 2.81
            },
            {
              "id": "rocm7_rc-rocwmma",
              "throughput": 756.58,
              "stddev": 4.67
            },
            {
              "id": "vulkan_amdvlk",
              "throughput": 788.46,
              "stddev": 4.36
            },
            {
              "id": "vulkan_radv",
              "throughput": 526.13,
              "stddev": 3.2
            }
          ]
        },
        "text_generation": {
          "context": "tg128",
          "metric": "tokens_per_second",
          "winner": "vulkan_radv",
          "backends": [
            {
              "id": "rocm6_4_4-rocwmma",
              "throughput": 47.63,
              "stddev": 0.01
            },
            {
              "id": "rocm7_rc-rocwmma",
              "throughput": 47.62,
              "stddev": 0.01
            },
            {
              "id": "vulkan_amdvlk",
              "throughput": 50.32,
              "stddev": 0.03
            },
            {
              "id": "vulkan_radv",
              "throughput": 52.9,
              "stddev": 0.05
            }
          ]
        }
      },
      "notes": "MXFP4 unlocks high throughput on AMDVLK; keep F16 for maximum fidelity if memory allows."
    },
    {
      "id": "gpt-oss-20b",
      "name": "GPT-OSS 20B",
      "maker": "openai",
      "providers": [
        {
          "id": "llama-cpp",
          "model_uri": "huggingface://openai/gpt-oss-20b-GGUF/gpt-oss-20b-mxfp4.gguf",
          "is_default": true
        }
      ],
      "quantization": "MXFP4",
      "ram_required_gb": 24,
      "context_window": 8192,
      "group": "heavy",
      "icon": "@/assets/icons/openai.svg",
      "description": "OpenAI's 20B parameter model - fast general tasks with MXFP4 precision",
      "family": "gpt-oss",
      "capabilities": [
        "chat",
        "code",
        "reasoning"
      ],
      "ctx_size": 8192,
      "ttl": 600,
      "hf_repo": "openai/gpt-oss-20b-GGUF",
      "hf_file": "gpt-oss-20b-mxfp4.gguf",
      "aliases": [
        "gpt-oss"
      ],
      "vulkan_driver": "RADV",
      "flash_attn": true,
      "enabled": true,
      "use_cases": [
        "General conversation",
        "Code generation",
        "Problem solving",
        "Content creation"
      ],
      "quantization_options": [
        {
          "level": "MXFP4",
          "hf_file": "gpt-oss-20b-mxfp4.gguf",
          "ram_required_gb": 24,
          "recommended_backend": "vulkan_radv",
          "benchmarks": {
            "prompt_processing": {
              "context": "pp512",
              "metric": "tokens_per_second",
              "winner": "vulkan_amdvlk",
              "backends": [
                {
                  "id": "rocm6_4_4-rocwmma",
                  "throughput": 1529.26,
                  "stddev": 3.68
                },
                {
                  "id": "rocm7_rc-rocwmma",
                  "throughput": 1506.28,
                  "stddev": 15.62
                },
                {
                  "id": "vulkan_amdvlk",
                  "throughput": 1908.57,
                  "stddev": 17.12
                },
                {
                  "id": "vulkan_radv",
                  "throughput": 1340.77,
                  "stddev": 10.85
                }
              ]
            },
            "text_generation": {
              "context": "tg128",
              "metric": "tokens_per_second",
              "winner": "vulkan_radv",
              "backends": [
                {
                  "id": "rocm6_4_4-rocwmma",
                  "throughput": 68.05,
                  "stddev": 0.01
                },
                {
                  "id": "rocm7_rc-rocwmma",
                  "throughput": 67.98,
                  "stddev": 0.03
                },
                {
                  "id": "vulkan_amdvlk",
                  "throughput": 72.91,
                  "stddev": 0.04
                },
                {
                  "id": "vulkan_radv",
                  "throughput": 75.19,
                  "stddev": 0.11
                }
              ]
            }
          }
        },
        {
          "level": "F32",
          "hf_file": "gpt-oss-20b-f32.gguf",
          "ram_required_gb": 48,
          "recommended_backend": "rocm6_4_4-rocwmma",
          "benchmarks": {
            "prompt_processing": {
              "context": "pp512",
              "metric": "tokens_per_second",
              "winner": "rocm6_4_4-rocwmma",
              "backends": [
                {
                  "id": "rocm6_4_4-rocwmma",
                  "throughput": 1493.11,
                  "stddev": 16.19
                },
                {
                  "id": "rocm7_rc-rocwmma",
                  "throughput": 1470.86,
                  "stddev": 14.39
                },
                {
                  "id": "vulkan_amdvlk",
                  "throughput": 609.37,
                  "stddev": 2.58
                },
                {
                  "id": "vulkan_radv",
                  "throughput": 451.11,
                  "stddev": 2.96
                }
              ]
            },
            "text_generation": {
              "context": "tg128",
              "metric": "tokens_per_second",
              "winner": "rocm6_4_4-rocwmma",
              "backends": [
                {
                  "id": "rocm6_4_4-rocwmma",
                  "throughput": 27.3,
                  "stddev": 0
                },
                {
                  "id": "rocm7_rc-rocwmma",
                  "throughput": 27.29,
                  "stddev": 0
                },
                {
                  "id": "vulkan_amdvlk",
                  "throughput": 18.25,
                  "stddev": 0.01
                },
                {
                  "id": "vulkan_radv",
                  "throughput": 16.83,
                  "stddev": 0.01
                }
              ]
            }
          }
        },
        {
          "level": "Q4_K_M",
          "hf_file": "gpt-oss-20b-q4_k_m.gguf",
          "ram_required_gb": 16,
          "recommended_backend": "vulkan_radv",
          "notes": "Legacy quantization retained for compatibility"
        }
      ],
      "benchmarks": {
        "prompt_processing": {
          "context": "pp512",
          "metric": "tokens_per_second",
          "winner": "vulkan_amdvlk",
          "backends": [
            {
              "id": "rocm6_4_4-rocwmma",
              "throughput": 1529.26,
              "stddev": 3.68
            },
            {
              "id": "rocm7_rc-rocwmma",
              "throughput": 1506.28,
              "stddev": 15.62
            },
            {
              "id": "vulkan_amdvlk",
              "throughput": 1908.57,
              "stddev": 17.12
            },
            {
              "id": "vulkan_radv",
              "throughput": 1340.77,
              "stddev": 10.85
            }
          ]
        },
        "text_generation": {
          "context": "tg128",
          "metric": "tokens_per_second",
          "winner": "vulkan_radv",
          "backends": [
            {
              "id": "rocm6_4_4-rocwmma",
              "throughput": 68.05,
              "stddev": 0.01
            },
            {
              "id": "rocm7_rc-rocwmma",
              "throughput": 67.98,
              "stddev": 0.03
            },
            {
              "id": "vulkan_amdvlk",
              "throughput": 72.91,
              "stddev": 0.04
            },
            {
              "id": "vulkan_radv",
              "throughput": 75.19,
              "stddev": 0.11
            }
          ]
        }
      },
      "notes": "Auto-swaps after 10 minutes of inactivity. MXFP4 delivers the best balance of quality and throughput on Vulkan."
    },
    {
      "id": "granite-4.0-h-micro",
      "name": "Granite 4.0 H-Micro",
      "maker": "ibm",
      "providers": [
        {
          "id": "llama-cpp",
          "model_uri": "huggingface://unsloth/granite-4.0-h-micro-GGUF/granite-4.0-h-micro-UD-Q4_K_XL.gguf",
          "is_default": true
        }
      ],
      "quantization": "Q4_K_XL",
      "ram_required_gb": 3,
      "context_window": 131072,
      "group": "task",
      "icon": "@/assets/icons/ibm.svg",
      "description": "IBM's 3B enterprise model - Fast, efficient for edge use",
      "family": "granite",
      "capabilities": [
        "chat",
        "code"
      ],
      "ctx_size": 8192,
      "ttl": 600,
      "hf_repo": "unsloth/granite-4.0-h-micro-GGUF",
      "hf_file": "granite-4.0-h-micro-UD-Q4_K_XL.gguf",
      "aliases": [
        "granite-micro",
        "granite-3b"
      ],
      "shortname": "granite-micro",
      "vulkan_driver": "RADV",
      "flash_attn": true,
      "enabled": false,
      "use_cases": [
        "Enterprise workflows",
        "Edge deployment",
        "Fast task completion",
        "Business applications"
      ],
      "notes": "IBM's enterprise-focused model with strong business use case support"
    },
    {
      "id": "granite-4.0-h-small",
      "name": "Granite 4.0 H-Small",
      "maker": "ibm",
      "providers": [
        {
          "id": "llama-cpp",
          "model_uri": "huggingface://unsloth/granite-4.0-h-small-GGUF/granite-4.0-h-small-UD-Q4_K_XL.gguf",
          "is_default": true
        }
      ],
      "quantization": "Q4_K_XL",
      "ram_required_gb": 22,
      "context_window": 131072,
      "group": "heavy",
      "icon": "@/assets/icons/ibm.svg",
      "description": "IBM's 32B MoE model (9B active) - Enterprise workhorse",
      "family": "granite",
      "capabilities": [
        "chat",
        "code"
      ],
      "ctx_size": 16384,
      "ttl": 900,
      "hf_repo": "unsloth/granite-4.0-h-small-GGUF",
      "hf_file": "granite-4.0-h-small-UD-Q4_K_XL.gguf",
      "aliases": [
        "granite-small",
        "granite-32b"
      ],
      "shortname": "granite-small",
      "vulkan_driver": "AMDVLK",
      "flash_attn": true,
      "enabled": false,
      "use_cases": [
        "Enterprise-grade assistants",
        "Structured document analysis",
        "Production code support",
        "Business workflow automation"
      ],
      "notes": "MoE architecture delivers strong quality with efficient active parameters"
    },
    {
      "id": "llama-2-7b",
      "name": "Llama 2 7B",
      "maker": "meta",
      "providers": [
        {
          "id": "llama-cpp",
          "model_uri": "huggingface://meta-llama/Llama-2-7b-chat-GGUF/llama-2-7b-chat-q4_0.gguf",
          "is_default": true
        }
      ],
      "quantization": "Q4_0",
      "ram_required_gb": 8,
      "context_window": 4096,
      "group": "task",
      "icon": "@/assets/icons/meta.svg",
      "description": "Classic Llama 2 7B chat model for lightweight assistants",
      "family": "llama",
      "capabilities": [
        "chat",
        "code"
      ],
      "ctx_size": 4096,
      "ttl": 600,
      "hf_repo": "meta-llama/Llama-2-7b-chat-GGUF",
      "hf_file": "llama-2-7b-chat-q4_0.gguf",
      "vulkan_driver": "AMDVLK",
      "flash_attn": true,
      "enabled": false,
      "use_cases": [
        "Legacy compatibility",
        "Fast draft generation",
        "Conversational prototyping",
        "Education and tutoring"
      ],
      "quantization_options": [
        {
          "level": "Q4_0",
          "hf_file": "llama-2-7b-chat-q4_0.gguf",
          "ram_required_gb": 8,
          "recommended_backend": "vulkan_amdvlk"
        }
      ],
      "benchmarks": {
        "prompt_processing": {
          "context": "pp512",
          "metric": "tokens_per_second",
          "winner": "vulkan_amdvlk",
          "backends": [
            {
              "id": "rocm6_4_4-rocwmma",
              "throughput": 1101.41,
              "stddev": 1.79
            },
            {
              "id": "rocm7_rc-rocwmma",
              "throughput": 1092.96,
              "stddev": 3
            },
            {
              "id": "vulkan_amdvlk",
              "throughput": 1376.09,
              "stddev": 0.77
            },
            {
              "id": "vulkan_radv",
              "throughput": 1096.08,
              "stddev": 2.8
            }
          ]
        },
        "text_generation": {
          "context": "tg128",
          "metric": "tokens_per_second",
          "winner": "vulkan_radv",
          "backends": [
            {
              "id": "rocm6_4_4-rocwmma",
              "throughput": 49.92,
              "stddev": 0.01
            },
            {
              "id": "rocm7_rc-rocwmma",
              "throughput": 49.81,
              "stddev": 0.01
            },
            {
              "id": "vulkan_amdvlk",
              "throughput": 53.23,
              "stddev": 0.06
            },
            {
              "id": "vulkan_radv",
              "throughput": 53.86,
              "stddev": 0.02
            }
          ]
        }
      },
      "notes": "Still a great baseline for low-power setups; shines with Vulkan RADV for generation throughput."
    },
    {
      "id": "llama-3.3-70b-instruct",
      "name": "Llama 3.3 70B Instruct",
      "maker": "meta",
      "providers": [
        {
          "id": "llama-cpp",
          "model_uri": "huggingface://meta-llama/Llama-3.3-70B-Instruct-GGUF/llama-3.3-70b-instruct-q8_k_xl.gguf",
          "is_default": true
        }
      ],
      "quantization": "Q8_K_XL",
      "ram_required_gb": 72,
      "context_window": 8192,
      "group": "heavy",
      "icon": "@/assets/icons/meta.svg",
      "description": "Latest 70B Llama 3.3 instruct tuned for premium local assistants",
      "family": "llama",
      "capabilities": [
        "chat",
        "code",
        "reasoning"
      ],
      "ctx_size": 8192,
      "ttl": 1200,
      "hf_repo": "meta-llama/Llama-3.3-70B-Instruct-GGUF",
      "hf_file": "llama-3.3-70b-instruct-q8_k_xl.gguf",
      "vulkan_driver": "AMDVLK",
      "flash_attn": true,
      "enabled": false,
      "use_cases": [
        "Executive copilots",
        "Strategic planning",
        "Advanced development workflows",
        "High-quality multilingual chat"
      ],
      "quantization_options": [
        {
          "level": "Q8_K_XL",
          "hf_file": "llama-3.3-70b-instruct-q8_k_xl.gguf",
          "ram_required_gb": 72,
          "recommended_backend": "rocm6_4_4-rocwmma"
        }
      ],
      "benchmarks": {
        "prompt_processing": {
          "context": "pp512",
          "metric": "tokens_per_second",
          "winner": "rocm6_4_4-rocwmma",
          "backends": [
            {
              "id": "rocm6_4_4-rocwmma",
              "throughput": 103.65,
              "stddev": 0.17
            },
            {
              "id": "rocm7_rc-rocwmma",
              "throughput": 103.05,
              "stddev": 0.09
            },
            {
              "id": "vulkan_amdvlk",
              "throughput": 98.46,
              "stddev": 0.54
            },
            {
              "id": "vulkan_radv",
              "throughput": 86.06,
              "stddev": 1.83
            }
          ]
        },
        "text_generation": {
          "context": "tg128",
          "metric": "tokens_per_second",
          "winner": "rocm6_4_4-rocwmma",
          "backends": [
            {
              "id": "rocm6_4_4-rocwmma",
              "throughput": 2.79,
              "stddev": 0
            },
            {
              "id": "rocm7_rc-rocwmma",
              "throughput": 2.78,
              "stddev": 0.01
            },
            {
              "id": "vulkan_amdvlk",
              "throughput": 2.8,
              "stddev": 0
            },
            {
              "id": "vulkan_radv",
              "throughput": 2.78,
              "stddev": 0
            }
          ]
        }
      },
      "notes": "All backends perform similarly for generation; ROCm edges out for prompt processing."
    },
    {
      "id": "llama-4-scout-17b",
      "name": "Llama 4 Scout 17B",
      "maker": "meta",
      "providers": [
        {
          "id": "llama-cpp",
          "model_uri": "huggingface://unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/Llama-4-Scout-17B-16E-Instruct-UD-Q4_K_XL.gguf",
          "is_default": true
        }
      ],
      "quantization": "Q4_K_XL",
      "ram_required_gb": 32,
      "context_window": 10485760,
      "group": "heavy",
      "icon": "@/assets/icons/meta.svg",
      "description": "Extended-context Llama 4 Scout 17B (16E) tuned for huge transcripts and strong coding",
      "family": "llama",
      "capabilities": [
        "chat",
        "long-context",
        "code"
      ],
      "ctx_size": 65536,
      "ttl": 900,
      "hf_repo": "unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF",
      "hf_file": "Llama-4-Scout-17B-16E-Instruct-UD-Q4_K_XL.gguf",
      "vulkan_driver": "AMDVLK",
      "flash_attn": true,
      "enabled": false,
      "use_cases": [
        "Entire codebase analysis",
        "Very long document processing",
        "Multi-document synthesis",
        "Extended conversations"
      ],
      "quantization_options": [
        {
          "level": "Q4_K_XL",
          "hf_file": "Llama-4-Scout-17B-16E-Instruct-UD-Q4_K_XL.gguf",
          "ram_required_gb": 32,
          "recommended_backend": "rocm6_4_4-rocwmma",
          "benchmarks": {
            "prompt_processing": {
              "context": "pp512",
              "metric": "tokens_per_second",
              "winner": "rocm6_4_4-rocwmma (tie with rocm7_rc-rocwmma)",
              "backends": [
                {
                  "id": "rocm6_4_4-rocwmma",
                  "throughput": 305.77,
                  "stddev": 1.56
                },
                {
                  "id": "rocm7_rc-rocwmma",
                  "throughput": 305.36,
                  "stddev": 1.78
                },
                {
                  "id": "vulkan_amdvlk",
                  "throughput": 194.33,
                  "stddev": 1.56
                },
                {
                  "id": "vulkan_radv",
                  "throughput": 228.13,
                  "stddev": 3.26
                }
              ]
            },
            "text_generation": {
              "context": "tg128",
              "metric": "tokens_per_second",
              "winner": "vulkan_radv (close tie with vulkan_amdvlk)",
              "backends": [
                {
                  "id": "rocm6_4_4-rocwmma",
                  "throughput": 17.97,
                  "stddev": 0
                },
                {
                  "id": "rocm7_rc-rocwmma",
                  "throughput": 17.97,
                  "stddev": 0
                },
                {
                  "id": "vulkan_amdvlk",
                  "throughput": 20.64,
                  "stddev": 0.01
                },
                {
                  "id": "vulkan_radv",
                  "throughput": 20.88,
                  "stddev": 0
                }
              ]
            }
          }
        },
        {
          "level": "Q6_K",
          "hf_file": "Llama-4-Scout-17B-16E-Instruct-UD-Q6_K.gguf",
          "ram_required_gb": 40,
          "recommended_backend": "rocm7_rc-rocwmma",
          "benchmarks": {
            "prompt_processing": {
              "context": "pp512",
              "metric": "tokens_per_second",
              "winner": "rocm7_rc-rocwmma (tie with rocm6_4_4-rocwmma)",
              "backends": [
                {
                  "id": "rocm6_4_4-rocwmma",
                  "throughput": 282.95,
                  "stddev": 5.18
                },
                {
                  "id": "rocm7_rc-rocwmma",
                  "throughput": 288.92,
                  "stddev": 3.51
                },
                {
                  "id": "vulkan_amdvlk",
                  "throughput": 224.57,
                  "stddev": 3.64
                },
                {
                  "id": "vulkan_radv",
                  "throughput": 212.38,
                  "stddev": 2.39
                }
              ]
            },
            "text_generation": {
              "context": "tg128",
              "metric": "tokens_per_second",
              "winner": "vulkan_amdvlk (tie with vulkan_radv)",
              "backends": [
                {
                  "id": "rocm6_4_4-rocwmma",
                  "throughput": 14.77,
                  "stddev": 0.06
                },
                {
                  "id": "rocm7_rc-rocwmma",
                  "throughput": 14.81,
                  "stddev": 0
                },
                {
                  "id": "vulkan_amdvlk",
                  "throughput": 15.76,
                  "stddev": 0.01
                },
                {
                  "id": "vulkan_radv",
                  "throughput": 15.76,
                  "stddev": 0.01
                }
              ]
            }
          }
        },
        {
          "level": "Q8_0",
          "hf_file": "Llama-4-Scout-17B-16E-Instruct-UD-Q8_0.gguf",
          "ram_required_gb": 56,
          "recommended_backend": "vulkan_amdvlk",
          "benchmarks": {
            "prompt_processing": {
              "context": "pp512",
              "metric": "tokens_per_second",
              "winner": "vulkan_amdvlk",
              "backends": [
                {
                  "id": "rocm6_4_4-rocwmma",
                  "throughput": 293.68,
                  "stddev": 3.72
                },
                {
                  "id": "rocm7_rc-rocwmma",
                  "throughput": 296.04,
                  "stddev": 2.16
                },
                {
                  "id": "vulkan_amdvlk",
                  "throughput": 346.93,
                  "stddev": 1.5
                },
                {
                  "id": "vulkan_radv",
                  "throughput": 280.38,
                  "stddev": 1.48
                }
              ]
            },
            "text_generation": {
              "context": "tg128",
              "metric": "tokens_per_second",
              "winner": "vulkan_radv",
              "backends": [
                {
                  "id": "rocm6_4_4-rocwmma",
                  "throughput": 11.96,
                  "stddev": 0.02
                },
                {
                  "id": "rocm7_rc-rocwmma",
                  "throughput": 11.98,
                  "stddev": 0
                },
                {
                  "id": "vulkan_amdvlk",
                  "throughput": 12.44,
                  "stddev": 0
                },
                {
                  "id": "vulkan_radv",
                  "throughput": 12.58,
                  "stddev": 0
                }
              ]
            }
          }
        }
      ],
      "benchmarks": {
        "prompt_processing": {
          "context": "pp512",
          "metric": "tokens_per_second",
          "winner": "rocm6_4_4-rocwmma (tie with rocm7_rc-rocwmma)",
          "backends": [
            {
              "id": "rocm6_4_4-rocwmma",
              "throughput": 305.77,
              "stddev": 1.56
            },
            {
              "id": "rocm7_rc-rocwmma",
              "throughput": 305.36,
              "stddev": 1.78
            },
            {
              "id": "vulkan_amdvlk",
              "throughput": 194.33,
              "stddev": 1.56
            },
            {
              "id": "vulkan_radv",
              "throughput": 228.13,
              "stddev": 3.26
            }
          ]
        },
        "text_generation": {
          "context": "tg128",
          "metric": "tokens_per_second",
          "winner": "vulkan_radv (close tie with vulkan_amdvlk)",
          "backends": [
            {
              "id": "rocm6_4_4-rocwmma",
              "throughput": 17.97,
              "stddev": 0
            },
            {
              "id": "rocm7_rc-rocwmma",
              "throughput": 17.97,
              "stddev": 0
            },
            {
              "id": "vulkan_amdvlk",
              "throughput": 20.64,
              "stddev": 0.01
            },
            {
              "id": "vulkan_radv",
              "throughput": 20.88,
              "stddev": 0
            }
          ]
        }
      },
      "notes": "Can theoretically do 10M context; use Q4_K_XL for VRAM savings, Q6_K for balance, or Q8_0 for maximum quality."
    },
    {
      "id": "nomic-embed-text-v1.5",
      "name": "Nomic Embed Text v1.5",
      "maker": "nomic",
      "providers": [
        {
          "id": "llama-cpp",
          "model_uri": "huggingface://nomic-ai/nomic-embed-text-v1.5-GGUF/nomic-embed-text-v1.5.Q8_0.gguf",
          "is_default": true
        }
      ],
      "quantization": "Q8_0",
      "ram_required_gb": 1,
      "context_window": 8192,
      "group": "embeddings",
      "icon": "@/assets/icons/huggingface.svg",
      "description": "OpenAI-compatible embedding (text-embedding-3-small alternative)",
      "family": "nomic",
      "capabilities": [
        "embeddings"
      ],
      "ctx_size": 8192,
      "ttl": 0,
      "hf_repo": "nomic-ai/nomic-embed-text-v1.5-GGUF",
      "hf_file": "nomic-embed-text-v1.5.Q8_0.gguf",
      "aliases": [
        "text-embedding-3-small"
      ],
      "embedding_dimension": 768,
      "vulkan_driver": "RADV",
      "flash_attn": false,
      "enabled": false,
      "use_cases": [
        "Drop-in OpenAI embedding replacement",
        "Lightweight semantic search",
        "Metadata enrichment",
        "Vector database experimentation"
      ],
      "notes": "Ideal when you want OpenAI-compatible embeddings without leaving the stack"
    },
    {
      "id": "qwen3-0.6b",
      "name": "Qwen3 0.6B",
      "maker": "qwen",
      "providers": [
        {
          "id": "llama-cpp",
          "model_uri": "huggingface://Qwen/Qwen3-0.6B-GGUF/qwen3-0.6b-q4_k_m.gguf",
          "is_default": true
        }
      ],
      "quantization": "Q4_K_M",
      "ram_required_gb": 1,
      "context_window": 32768,
      "group": "task",
      "icon": "@/assets/icons/qwen.svg",
      "description": "Ultra-fast task model - title generation",
      "family": "qwen",
      "capabilities": [
        "chat"
      ],
      "ctx_size": 4096,
      "ttl": 0,
      "hf_repo": "Qwen/Qwen3-0.6B-GGUF",
      "hf_file": "qwen3-0.6b-q4_k_m.gguf",
      "shortname": "qwen3-tiny",
      "vulkan_driver": "RADV",
      "flash_attn": true,
      "enabled": true,
      "use_cases": [
        "Title generation",
        "Quick summaries",
        "Simple classification"
      ],
      "notes": "Never unloads - always ready for instant response"
    },
    {
      "id": "qwen3-1.7b",
      "name": "Qwen3 1.7B",
      "maker": "qwen",
      "providers": [
        {
          "id": "llama-cpp",
          "model_uri": "huggingface://Qwen/Qwen3-1.7B-GGUF/qwen3-1.7b-q4_k_m.gguf",
          "is_default": true
        }
      ],
      "quantization": "Q4_K_M",
      "ram_required_gb": 2,
      "context_window": 32768,
      "group": "task",
      "icon": "@/assets/icons/qwen.svg",
      "description": "Fast task model - metadata tagging, summarization",
      "family": "qwen",
      "capabilities": [
        "chat"
      ],
      "ctx_size": 8192,
      "ttl": 600,
      "hf_repo": "Qwen/Qwen3-1.7B-GGUF",
      "hf_file": "qwen3-1.7b-q4_k_m.gguf",
      "shortname": "qwen3-small",
      "vulkan_driver": "RADV",
      "flash_attn": true,
      "enabled": false,
      "use_cases": [
        "Metadata tagging",
        "Document summarization",
        "Lightweight automation",
        "Fast chat responses"
      ],
      "notes": "Enable for lightweight tasks that need more context than the tiny model"
    },
    {
      "id": "qwen3-14b",
      "name": "Qwen3 14B",
      "maker": "qwen",
      "providers": [
        {
          "id": "llama-cpp",
          "model_uri": "huggingface://Qwen/Qwen3-14B-GGUF/qwen3-14b-q4_k_m.gguf",
          "is_default": true
        }
      ],
      "quantization": "Q4_K_M",
      "ram_required_gb": 10,
      "context_window": 32768,
      "group": "task",
      "icon": "@/assets/icons/qwen.svg",
      "description": "Premium task model - advanced reasoning",
      "family": "qwen",
      "capabilities": [
        "chat",
        "code",
        "reasoning"
      ],
      "ctx_size": 8192,
      "ttl": 600,
      "hf_repo": "Qwen/Qwen3-14B-GGUF",
      "hf_file": "qwen3-14b-q4_k_m.gguf",
      "shortname": "qwen3-xlarge",
      "vulkan_driver": "RADV",
      "flash_attn": true,
      "enabled": false,
      "use_cases": [
        "Advanced reasoning tasks",
        "Complex code generation",
        "Detailed analysis",
        "High-quality content"
      ],
      "notes": "Auto-unloads after 10 minutes when not in use"
    },
    {
      "id": "qwen3-235b",
      "name": "Qwen3 235B",
      "maker": "qwen",
      "providers": [
        {
          "id": "llama-cpp",
          "model_uri": "huggingface://unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF/Qwen3-235B-A22B-Instruct-2507-UD-Q3_K_XL.gguf",
          "is_default": true
        }
      ],
      "quantization": "Q3_K_XL",
      "ram_required_gb": 97,
      "context_window": 131072,
      "group": "heavy",
      "icon": "@/assets/icons/qwen.svg",
      "description": "Flagship model - incredible capability, fits in 128GB",
      "family": "qwen",
      "capabilities": [
        "chat",
        "code",
        "reasoning",
        "long-context"
      ],
      "ctx_size": 65536,
      "ttl": 1800,
      "hf_repo": "unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF",
      "hf_file": "Qwen3-235B-A22B-Instruct-2507-UD-Q3_K_XL.gguf",
      "vulkan_driver": "AMDVLK",
      "flash_attn": true,
      "batch_size": 2048,
      "enabled": false,
      "use_cases": [
        "Most challenging reasoning tasks",
        "Complex software architecture",
        "Research-grade analysis",
        "Long-form content creation"
      ],
      "quantization_options": [
        {
          "level": "Q3_K_XL",
          "hf_file": "Qwen3-235B-A22B-Instruct-2507-UD-Q3_K_XL.gguf",
          "ram_required_gb": 97,
          "recommended_backend": "rocm6_4_4-rocwmma"
        }
      ],
      "benchmarks": {
        "prompt_processing": {
          "context": "pp512",
          "metric": "tokens_per_second",
          "winner": "rocm6_4_4-rocwmma",
          "backends": [
            {
              "id": "rocm6_4_4-rocwmma",
              "throughput": 142.3,
              "stddev": 0.82
            },
            {
              "id": "rocm7_rc-rocwmma",
              "throughput": 142.6,
              "stddev": 0.56
            },
            {
              "id": "vulkan_amdvlk",
              "throughput": 135.43,
              "stddev": 4.81
            },
            {
              "id": "vulkan_radv",
              "throughput": 125.48,
              "stddev": 4.53
            }
          ]
        },
        "text_generation": {
          "context": "tg128",
          "metric": "tokens_per_second",
          "winner": "vulkan_radv",
          "backends": [
            {
              "id": "rocm6_4_4-rocwmma",
              "throughput": 14.9,
              "stddev": 0.12
            },
            {
              "id": "rocm7_rc-rocwmma",
              "throughput": 14.9,
              "stddev": 0.09
            },
            {
              "id": "vulkan_amdvlk",
              "throughput": 17.14,
              "stddev": 0.02
            },
            {
              "id": "vulkan_radv",
              "throughput": 18.02,
              "stddev": 0.01
            }
          ]
        }
      },
      "notes": "Fits tightly into 128GB systems with Q3_K_XL; expect slower inference but unmatched quality. Conservative 65k context (130k max)."
    },
    {
      "id": "qwen3-30b-a3b",
      "name": "Qwen3 30B A3B",
      "maker": "qwen",
      "providers": [
        {
          "id": "llama-cpp",
          "model_uri": "huggingface://Qwen/Qwen3-30B-A3B-GGUF/Qwen3-30B-A3B-BF16.gguf",
          "is_default": true
        }
      ],
      "quantization": "BF16",
      "ram_required_gb": 64,
      "context_window": 32768,
      "group": "heavy",
      "icon": "@/assets/icons/qwen.svg",
      "description": "Full-precision Qwen3 30B base for highest quality local inference",
      "family": "qwen",
      "capabilities": [
        "chat",
        "code",
        "reasoning"
      ],
      "ctx_size": 32768,
      "ttl": 900,
      "hf_repo": "Qwen/Qwen3-30B-A3B-GGUF",
      "hf_file": "Qwen3-30B-A3B-BF16.gguf",
      "vulkan_driver": "AMDVLK",
      "flash_attn": true,
      "enabled": false,
      "use_cases": [
        "High-accuracy copilots",
        "Complex research tasks",
        "Advanced planning",
        "Large-context analysis"
      ],
      "quantization_options": [
        {
          "level": "BF16",
          "hf_file": "Qwen3-30B-A3B-BF16.gguf",
          "ram_required_gb": 64,
          "recommended_backend": "rocm6_4_4-rocwmma"
        }
      ],
      "benchmarks": {
        "prompt_processing": {
          "context": "pp512",
          "metric": "tokens_per_second",
          "winner": "rocm6_4_4-rocwmma",
          "backends": [
            {
              "id": "rocm6_4_4-rocwmma",
              "throughput": 480.95,
              "stddev": 4.32
            },
            {
              "id": "rocm7_rc-rocwmma",
              "throughput": 477.05,
              "stddev": 5.97
            },
            {
              "id": "vulkan_amdvlk",
              "throughput": 216.46,
              "stddev": 0.31
            },
            {
              "id": "vulkan_radv",
              "throughput": 166.05,
              "stddev": 0.25
            }
          ]
        },
        "text_generation": {
          "context": "tg128",
          "metric": "tokens_per_second",
          "winner": "rocm7_rc-rocwmma",
          "backends": [
            {
              "id": "rocm6_4_4-rocwmma",
              "throughput": 25.82,
              "stddev": 0
            },
            {
              "id": "rocm7_rc-rocwmma",
              "throughput": 25.84,
              "stddev": 0
            },
            {
              "id": "vulkan_amdvlk",
              "throughput": 10,
              "stddev": 0.01
            },
            {
              "id": "vulkan_radv",
              "throughput": 9.29,
              "stddev": 0.02
            }
          ]
        }
      },
      "notes": "BF16 delivers maximum fidelity; pair with ROCm for best performance."
    },
    {
      "id": "qwen3-30b-a3b-instruct-2507",
      "name": "Qwen3 30B A3B Instruct 2507",
      "maker": "qwen",
      "providers": [
        {
          "id": "llama-cpp",
          "model_uri": "huggingface://Qwen/Qwen3-30B-A3B-Instruct-2507-GGUF/Qwen3-30B-A3B-Instruct-2507-Q6_K_XL.gguf",
          "is_default": true
        }
      ],
      "quantization": "Q6_K_XL",
      "ram_required_gb": 48,
      "context_window": 32768,
      "group": "heavy",
      "icon": "@/assets/icons/qwen.svg",
      "description": "Instruction-tuned Qwen3 30B (July 2025) optimized for high-throughput local inference",
      "family": "qwen",
      "capabilities": [
        "chat",
        "code",
        "reasoning"
      ],
      "ctx_size": 32768,
      "ttl": 900,
      "hf_repo": "Qwen/Qwen3-30B-A3B-Instruct-2507-GGUF",
      "hf_file": "Qwen3-30B-A3B-Instruct-2507-Q6_K_XL.gguf",
      "vulkan_driver": "AMDVLK",
      "flash_attn": true,
      "enabled": true,
      "use_cases": [
        "Powerful local assistant",
        "Agentic reasoning",
        "Large document summarization",
        "Application copilots"
      ],
      "quantization_options": [
        {
          "level": "Q6_K_XL",
          "hf_file": "Qwen3-30B-A3B-Instruct-2507-Q6_K_XL.gguf",
          "ram_required_gb": 48,
          "recommended_backend": "vulkan_amdvlk"
        }
      ],
      "benchmarks": {
        "prompt_processing": {
          "context": "pp512",
          "metric": "tokens_per_second",
          "winner": "vulkan_amdvlk",
          "backends": [
            {
              "id": "rocm6_4_4-rocwmma",
              "throughput": 626.72,
              "stddev": 6.27
            },
            {
              "id": "rocm7_rc-rocwmma",
              "throughput": 623.1,
              "stddev": 4.22
            },
            {
              "id": "vulkan_amdvlk",
              "throughput": 1020.41,
              "stddev": 5.76
            },
            {
              "id": "vulkan_radv",
              "throughput": 857.47,
              "stddev": 4.38
            }
          ]
        },
        "text_generation": {
          "context": "tg128",
          "metric": "tokens_per_second",
          "winner": "vulkan_radv",
          "backends": [
            {
              "id": "rocm6_4_4-rocwmma",
              "throughput": 57.04,
              "stddev": 0.01
            },
            {
              "id": "rocm7_rc-rocwmma",
              "throughput": 56.95,
              "stddev": 0.01
            },
            {
              "id": "vulkan_amdvlk",
              "throughput": 59.42,
              "stddev": 0.04
            },
            {
              "id": "vulkan_radv",
              "throughput": 63.41,
              "stddev": 0.08
            }
          ]
        }
      },
      "notes": "Q6_K_XL balances VRAM footprint and throughput; AMDVLK excels for prompt ingestion while RADV leads generation."
    },
    {
      "id": "qwen3-4b",
      "name": "Qwen3 4B",
      "maker": "qwen",
      "providers": [
        {
          "id": "llama-cpp",
          "model_uri": "huggingface://Qwen/Qwen3-4B-GGUF/qwen3-4b-q4_k_m.gguf",
          "is_default": true
        }
      ],
      "quantization": "Q4_K_M",
      "ram_required_gb": 3,
      "context_window": 32768,
      "group": "task",
      "icon": "@/assets/icons/qwen.svg",
      "description": "Balanced task model - tags, queries, analysis",
      "family": "qwen",
      "capabilities": [
        "chat",
        "code"
      ],
      "ctx_size": 8192,
      "ttl": 0,
      "hf_repo": "Qwen/Qwen3-4B-GGUF",
      "hf_file": "qwen3-4b-q4_k_m.gguf",
      "vulkan_driver": "RADV",
      "flash_attn": true,
      "shortname": "qwen3-medium",
      "use_cases": [
        "Tag generation",
        "Query rewriting",
        "Text analysis",
        "Simple coding tasks"
      ],
      "notes": "Never unloads - always ready for multi-step operations",
      "enabled": true
    },
    {
      "id": "qwen3-8b",
      "name": "Qwen3 8B",
      "maker": "qwen",
      "providers": [
        {
          "id": "llama-cpp",
          "model_uri": "huggingface://Qwen/Qwen3-8B-GGUF/qwen3-8b-q4_k_m.gguf",
          "is_default": true
        }
      ],
      "quantization": "Q4_K_M",
      "ram_required_gb": 6,
      "context_window": 32768,
      "group": "task",
      "icon": "@/assets/icons/qwen.svg",
      "description": "High-quality task model - complex reasoning",
      "family": "qwen",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "ctx_size": 8192,
      "ttl": 600,
      "hf_repo": "Qwen/Qwen3-8B-GGUF",
      "hf_file": "qwen3-8b-q4_k_m.gguf",
      "shortname": "qwen3-large",
      "vulkan_driver": "RADV",
      "flash_attn": true,
      "enabled": false,
      "use_cases": [
        "Complex task automation",
        "Advanced summarization",
        "Multi-step reasoning",
        "Higher quality chat interactions"
      ],
      "notes": "Enable when you need stronger reasoning for on-device workloads"
    },
    {
      "id": "qwen3-coder-30b",
      "name": "Qwen3 Coder 30B",
      "maker": "qwen",
      "providers": [
        {
          "id": "llama-cpp",
          "model_uri": "huggingface://unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf",
          "is_default": true
        }
      ],
      "quantization": "Q4_K_M",
      "ram_required_gb": 32,
      "context_window": 8192,
      "group": "heavy",
      "icon": "@/assets/icons/qwen.svg",
      "description": "Advanced code model - great for complex refactoring",
      "family": "qwen",
      "capabilities": [
        "code",
        "chat"
      ],
      "ctx_size": 8192,
      "ttl": 900,
      "hf_repo": "unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF",
      "hf_file": "Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf",
      "vulkan_driver": "RADV",
      "flash_attn": true,
      "enabled": false,
      "use_cases": [
        "Complex code refactoring",
        "Software architecture design",
        "Code review and analysis",
        "Algorithm implementation"
      ],
      "quantization_options": [
        {
          "level": "Q4_K_M",
          "hf_file": "Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf",
          "ram_required_gb": 32,
          "recommended_backend": "vulkan_radv",
          "benchmarks": {
            "prompt_processing": {
              "context": "pp512",
              "metric": "tokens_per_second",
              "winner": "vulkan_radv",
              "backends": [
                {
                  "id": "rocm6_4_4-rocwmma",
                  "throughput": 662.07,
                  "stddev": 2.63
                },
                {
                  "id": "rocm7_rc-rocwmma",
                  "throughput": 652.79,
                  "stddev": 5.72
                },
                {
                  "id": "vulkan_amdvlk",
                  "throughput": 773.64,
                  "stddev": 3.79
                },
                {
                  "id": "vulkan_radv",
                  "throughput": 832.99,
                  "stddev": 3.06
                }
              ]
            },
            "text_generation": {
              "context": "tg128",
              "metric": "tokens_per_second",
              "winner": "vulkan_radv",
              "backends": [
                {
                  "id": "rocm6_4_4-rocwmma",
                  "throughput": 68.4,
                  "stddev": 0.02
                },
                {
                  "id": "rocm7_rc-rocwmma",
                  "throughput": 68.7,
                  "stddev": 0.02
                },
                {
                  "id": "vulkan_amdvlk",
                  "throughput": 77.82,
                  "stddev": 0.06
                },
                {
                  "id": "vulkan_radv",
                  "throughput": 81.4,
                  "stddev": 0.12
                }
              ]
            }
          }
        },
        {
          "level": "BF16",
          "hf_file": "Qwen3-Coder-30B-A3B-Instruct-BF16.gguf",
          "ram_required_gb": 60,
          "recommended_backend": "rocm6_4_4-rocwmma",
          "notes": "Full-precision variant for maximum quality"
        }
      ],
      "benchmarks": {
        "prompt_processing": {
          "context": "pp512",
          "metric": "tokens_per_second",
          "winner": "vulkan_radv",
          "backends": [
            {
              "id": "rocm6_4_4-rocwmma",
              "throughput": 662.07,
              "stddev": 2.63
            },
            {
              "id": "rocm7_rc-rocwmma",
              "throughput": 652.79,
              "stddev": 5.72
            },
            {
              "id": "vulkan_amdvlk",
              "throughput": 773.64,
              "stddev": 3.79
            },
            {
              "id": "vulkan_radv",
              "throughput": 832.99,
              "stddev": 3.06
            }
          ]
        },
        "text_generation": {
          "context": "tg128",
          "metric": "tokens_per_second",
          "winner": "vulkan_radv",
          "backends": [
            {
              "id": "rocm6_4_4-rocwmma",
              "throughput": 68.4,
              "stddev": 0.02
            },
            {
              "id": "rocm7_rc-rocwmma",
              "throughput": 68.7,
              "stddev": 0.02
            },
            {
              "id": "vulkan_amdvlk",
              "throughput": 77.82,
              "stddev": 0.06
            },
            {
              "id": "vulkan_radv",
              "throughput": 81.4,
              "stddev": 0.12
            }
          ]
        }
      },
      "notes": "Specialized for coding tasks. Default Q4_K_M variant aligns with latest benchmark results."
    },
    {
      "id": "qwen3-embedding-0.6b",
      "name": "Qwen3 Embedding 0.6B",
      "maker": "qwen",
      "providers": [
        {
          "id": "llama-cpp",
          "model_uri": "huggingface://Qwen/Qwen3-Embedding-0.6B-GGUF/qwen3-embedding-0.6b-q8_0.gguf",
          "is_default": true
        }
      ],
      "quantization": "Q8_0",
      "ram_required_gb": 1,
      "context_window": 8192,
      "group": "embeddings",
      "icon": "@/assets/icons/qwen.svg",
      "description": "Ultra-fast embedding model - 0.6B parameters",
      "family": "qwen",
      "capabilities": [
        "embeddings"
      ],
      "ctx_size": 8192,
      "ttl": 0,
      "hf_repo": "Qwen/Qwen3-Embedding-0.6B-GGUF",
      "hf_file": "qwen3-embedding-0.6b-q8_0.gguf",
      "shortname": "qwen3-emb-tiny",
      "embedding_dimension": 1024,
      "vulkan_driver": "RADV",
      "flash_attn": false,
      "enabled": false,
      "use_cases": [
        "Low-latency RAG",
        "Fast semantic search",
        "Edge deployments",
        "Bulk document ingestion"
      ],
      "notes": "Default choice when you need the smallest embedding footprint"
    },
    {
      "id": "qwen3-embedding-4b",
      "name": "Qwen3 Embedding 4B",
      "maker": "qwen",
      "providers": [
        {
          "id": "llama-cpp",
          "model_uri": "huggingface://Qwen/Qwen3-Embedding-4B-GGUF/qwen3-embedding-4b-q8_0.gguf",
          "is_default": true
        }
      ],
      "quantization": "Q8_0",
      "ram_required_gb": 5,
      "context_window": 8192,
      "group": "embeddings",
      "icon": "@/assets/icons/qwen.svg",
      "description": "Balanced embedding model - 4B parameters",
      "family": "qwen",
      "capabilities": [
        "embeddings"
      ],
      "ctx_size": 8192,
      "ttl": 0,
      "hf_repo": "Qwen/Qwen3-Embedding-4B-GGUF",
      "hf_file": "qwen3-embedding-4b-q8_0.gguf",
      "shortname": "qwen3-emb-medium",
      "embedding_dimension": 2048,
      "vulkan_driver": "RADV",
      "flash_attn": false,
      "enabled": false,
      "use_cases": [
        "Balanced RAG pipelines",
        "Semantic clustering",
        "Knowledge base indexing",
        "Multilingual search"
      ],
      "notes": "Great middle ground between speed and embedding fidelity"
    },
    {
      "id": "qwen3-embedding-8b",
      "name": "Qwen3 Embedding 8B",
      "maker": "qwen",
      "providers": [
        {
          "id": "llama-cpp",
          "model_uri": "huggingface://Qwen/Qwen3-Embedding-8B-GGUF/qwen3-embedding-8b-q8_0.gguf",
          "is_default": true
        }
      ],
      "quantization": "Q8_0",
      "ram_required_gb": 9,
      "context_window": 8192,
      "group": "embeddings",
      "icon": "@/assets/icons/qwen.svg",
      "description": "High-quality embedding model - 8B parameters (DEFAULT)",
      "family": "qwen",
      "capabilities": [
        "embeddings"
      ],
      "ctx_size": 8192,
      "ttl": 0,
      "hf_repo": "Qwen/Qwen3-Embedding-8B-GGUF",
      "hf_file": "qwen3-embedding-8b-q8_0.gguf",
      "aliases": [
        "qwen-embed",
        "default-embedding"
      ],
      "shortname": "qwen3-emb-large",
      "embedding_dimension": 4096,
      "vulkan_driver": "RADV",
      "flash_attn": false,
      "enabled": true,
      "use_cases": [
        "RAG document embedding",
        "Semantic search",
        "Document similarity",
        "Vector database indexing"
      ],
      "notes": "Default embedding model - never unloads, always ready"
    },
    {
      "id": "qwen3-thinking-30b",
      "name": "Qwen3 Thinking 30B",
      "maker": "qwen",
      "providers": [
        {
          "id": "llama-cpp",
          "model_uri": "huggingface://unsloth/Qwen3-30B-A3B-Thinking-2507-GGUF/Qwen3-30B-A3B-Thinking-2507-UD-Q4_K_XL.gguf",
          "is_default": true
        }
      ],
      "quantization": "Q4_K_XL",
      "ram_required_gb": 20,
      "context_window": 8192,
      "group": "heavy",
      "icon": "@/assets/icons/qwen.svg",
      "description": "Deep reasoning specialist - thinks through problems",
      "family": "qwen",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "ctx_size": 8192,
      "ttl": 900,
      "hf_repo": "unsloth/Qwen3-30B-A3B-Thinking-2507-GGUF",
      "hf_file": "Qwen3-30B-A3B-Thinking-2507-UD-Q4_K_XL.gguf",
      "aliases": [
        "thinking",
        "qwen-thinking"
      ],
      "vulkan_driver": "AMDVLK",
      "flash_attn": true,
      "enabled": false,
      "use_cases": [
        "Complex reasoning tasks",
        "In-depth analysis",
        "Strategic planning",
        "Challenging problem solving"
      ],
      "notes": "Enable when you need extended reasoning with thoughtful responses"
    },
    {
      "id": "qwq-32b",
      "name": "QwQ 32B",
      "maker": "qwen",
      "providers": [
        {
          "id": "llama-cpp",
          "model_uri": "huggingface://bartowski/Qwen_QwQ-32B-GGUF/Qwen_QwQ-32B-Q4_K_M.gguf",
          "is_default": true
        }
      ],
      "quantization": "Q4_K_M",
      "ram_required_gb": 20,
      "context_window": 32768,
      "group": "heavy",
      "icon": "@/assets/icons/qwen.svg",
      "description": "Question-answering specialist with reasoning",
      "family": "qwen",
      "capabilities": [
        "chat",
        "reasoning"
      ],
      "ctx_size": 32768,
      "ttl": 900,
      "hf_repo": "bartowski/Qwen_QwQ-32B-GGUF",
      "hf_file": "Qwen_QwQ-32B-Q4_K_M.gguf",
      "vulkan_driver": "AMDVLK",
      "flash_attn": true,
      "enabled": false,
      "use_cases": [
        "High-accuracy Q&A",
        "Reasoning with citations",
        "Technical troubleshooting",
        "Educational explanations"
      ],
      "notes": "Optimized for question answering with visible reasoning traces"
    },
    {
      "id": "voyage-3-large",
      "name": "Voyage 3 Large",
      "maker": "voyage",
      "providers": [
        {
          "id": "llama-cpp",
          "model_uri": "huggingface://voyageai/voyage-3-large-GGUF/voyage-3-large-q8_0.gguf",
          "is_default": true
        }
      ],
      "quantization": "Q8_0",
      "ram_required_gb": 11,
      "context_window": 8192,
      "group": "embeddings",
      "icon": "@/assets/icons/voyage.svg",
      "description": "High-capacity embedding - 10B parameters",
      "family": "voyage",
      "capabilities": [
        "embeddings"
      ],
      "ctx_size": 8192,
      "ttl": 0,
      "hf_repo": "voyageai/voyage-3-large-GGUF",
      "hf_file": "voyage-3-large-q8_0.gguf",
      "embedding_dimension": 1536,
      "vulkan_driver": "RADV",
      "flash_attn": false,
      "enabled": false,
      "use_cases": [
        "High fidelity semantic search",
        "Enterprise RAG workloads",
        "Large document indexing",
        "Advanced similarity ranking"
      ],
      "notes": "Premium embedding quality for demanding retrieval tasks"
    }
  ],
  "makers": [
    {
      "id": "amazon",
      "name": "Amazon",
      "icon": "@/assets/icons/amazon.svg",
      "website": "https://aws.amazon.com/bedrock/nova/",
      "description": "Amazon Nova and Titan foundation models",
      "models": [
        "amazon/nova-lite",
        "amazon/nova-micro",
        "amazon/nova-pro",
        "amazon/titan-embed-text-v2"
      ]
    },
    {
      "id": "anthropic",
      "name": "Anthropic",
      "icon": "@/assets/icons/anthropic.svg",
      "website": "https://www.anthropic.com/",
      "description": "AI safety company creating Claude models",
      "models": [
        "anthropic/claude-3-haiku",
        "anthropic/claude-3-opus",
        "anthropic/claude-3.5-haiku",
        "anthropic/claude-3.5-sonnet",
        "anthropic/claude-3.5-sonnet-20240620",
        "anthropic/claude-3.7-sonnet",
        "anthropic/claude-haiku-4.5",
        "anthropic/claude-opus-4",
        "anthropic/claude-opus-4.1",
        "anthropic/claude-sonnet-4",
        "anthropic/claude-sonnet-4.5"
      ]
    },
    {
      "id": "cohere",
      "name": "Cohere",
      "icon": "@/assets/icons/cohere.svg",
      "website": "https://cohere.com/",
      "description": "Enterprise AI company providing Command and Embed models",
      "models": [
        "cohere/command-a",
        "cohere/embed-v4.0"
      ]
    },
    {
      "id": "deepseek",
      "name": "DeepSeek",
      "icon": "@/assets/icons/deepseek.svg",
      "website": "https://www.deepseek.com/",
      "description": "DeepSeek research lab focused on large language models",
      "models": [
        "deepseek/deepseek-r1",
        "deepseek/deepseek-v3",
        "deepseek/deepseek-v3.1",
        "deepseek/deepseek-v3.1-terminus",
        "deepseek/deepseek-v3.2-exp",
        "deepseek/deepseek-v3.2-exp-thinking"
      ]
    },
    {
      "id": "google",
      "name": "Google DeepMind",
      "icon": "@/assets/icons/google.svg",
      "website": "https://ai.google.dev/",
      "description": "Google Gemini model platform",
      "models": [
        "google/gemini-2.0-flash",
        "google/gemini-2.0-flash-lite",
        "google/gemini-2.5-flash",
        "google/gemini-2.5-flash-image",
        "google/gemini-2.5-flash-image-preview",
        "google/gemini-2.5-flash-lite",
        "google/gemini-2.5-pro",
        "google/gemini-embedding-001",
        "google/text-embedding-005",
        "google/text-multilingual-embedding-002"
      ]
    },
    {
      "id": "ibm",
      "name": "IBM",
      "icon": "@/assets/icons/ibm.svg",
      "website": "https://www.ibm.com/granite",
      "description": "Creator of Granite enterprise models"
    },
    {
      "id": "meta",
      "name": "Meta AI",
      "icon": "@/assets/icons/meta.svg",
      "website": "https://ai.meta.com/",
      "description": "Meta AI research organization behind Llama models",
      "models": [
        "meta/llama-3.1-70b",
        "meta/llama-3.1-8b",
        "meta/llama-3.2-11b",
        "meta/llama-3.2-1b",
        "meta/llama-3.2-3b",
        "meta/llama-3.2-90b",
        "meta/llama-3.3-70b",
        "meta/llama-4-maverick",
        "meta/llama-4-scout"
      ]
    },
    {
      "id": "mistral",
      "name": "Mistral AI",
      "icon": "@/assets/icons/mistral.svg",
      "website": "https://mistral.ai/",
      "description": "European AI company creating efficient language models",
      "models": [
        "mistral/codestral",
        "mistral/codestral-embed",
        "mistral/devstral-small",
        "mistral/magistral-medium",
        "mistral/magistral-medium-2506",
        "mistral/magistral-small",
        "mistral/magistral-small-2506",
        "mistral/ministral-3b",
        "mistral/ministral-8b",
        "mistral/mistral-embed",
        "mistral/mistral-large",
        "mistral/mistral-medium",
        "mistral/mistral-small",
        "mistral/mixtral-8x22b-instruct",
        "mistral/pixtral-12b",
        "mistral/pixtral-large"
      ]
    },
    {
      "id": "moonshotai",
      "name": "Moonshot AI",
      "icon": "@/assets/icons/moonshot.svg",
      "website": "https://moonshot.cn/",
      "description": "Chinese AI startup known for Kimi assistants",
      "models": [
        "moonshotai/kimi-k2",
        "moonshotai/kimi-k2-0905",
        "moonshotai/kimi-k2-turbo"
      ]
    },
    {
      "id": "openai",
      "name": "OpenAI",
      "icon": "@/assets/icons/openai.svg",
      "website": "https://openai.com/",
      "description": "OpenAI research lab and API provider",
      "models": [
        "openai/gpt-3.5-turbo",
        "openai/gpt-3.5-turbo-instruct",
        "openai/gpt-4-turbo",
        "openai/gpt-4.1",
        "openai/gpt-4.1-mini",
        "openai/gpt-4.1-nano",
        "openai/gpt-4o",
        "openai/gpt-4o-mini",
        "openai/gpt-5",
        "openai/gpt-5-codex",
        "openai/gpt-5-mini",
        "openai/gpt-5-nano",
        "openai/gpt-5-pro",
        "openai/gpt-oss-120b",
        "openai/gpt-oss-20b",
        "openai/gpt-oss-safeguard-20b",
        "openai/o1",
        "openai/o3",
        "openai/o3-deep-research",
        "openai/o3-mini",
        "openai/o4-mini",
        "openai/text-embedding-3-large",
        "openai/text-embedding-3-small",
        "openai/text-embedding-ada-002"
      ]
    },
    {
      "id": "perplexity",
      "name": "Perplexity AI",
      "icon": "@/assets/icons/perplexity.svg",
      "website": "https://www.perplexity.ai/",
      "description": "AI search company offering Sonar reasoning models",
      "models": [
        "perplexity/sonar",
        "perplexity/sonar-pro",
        "perplexity/sonar-reasoning",
        "perplexity/sonar-reasoning-pro"
      ]
    },
    {
      "id": "qwen",
      "name": "Alibaba Qwen",
      "icon": "@/assets/icons/qwen.svg",
      "website": "https://qwenlm.github.io/",
      "description": "Alibaba's Qwen model family",
      "models": [
        "alibaba/qwen-3-14b",
        "alibaba/qwen-3-235b",
        "alibaba/qwen-3-30b",
        "alibaba/qwen-3-32b",
        "alibaba/qwen3-coder",
        "alibaba/qwen3-coder-30b-a3b",
        "alibaba/qwen3-coder-plus",
        "alibaba/qwen3-max",
        "alibaba/qwen3-max-preview",
        "alibaba/qwen3-next-80b-a3b-instruct",
        "alibaba/qwen3-next-80b-a3b-thinking",
        "alibaba/qwen3-vl-instruct",
        "alibaba/qwen3-vl-thinking"
      ]
    },
    {
      "id": "voyage",
      "name": "Voyage AI",
      "icon": "@/assets/icons/voyage.svg",
      "website": "https://www.voyageai.com/",
      "description": "Specialized embedding models for domain-specific retrieval",
      "models": [
        "voyage/voyage-3.5-lite",
        "voyage/voyage-3.5",
        "voyage/voyage-3-large",
        "voyage/voyage-law-2",
        "voyage/voyage-code-3",
        "voyage/voyage-code-2",
        "voyage/voyage-finance-2"
      ]
    },
    {
      "id": "xai",
      "name": "xAI",
      "icon": "@/assets/icons/xai.svg",
      "website": "https://x.ai/",
      "description": "xAI research company building Grok models",
      "models": [
        "xai/grok-2",
        "xai/grok-2-vision",
        "xai/grok-3",
        "xai/grok-3-fast",
        "xai/grok-3-mini",
        "xai/grok-3-mini-fast",
        "xai/grok-4",
        "xai/grok-4-fast-non-reasoning",
        "xai/grok-4-fast-reasoning",
        "xai/grok-code-fast-1"
      ]
    },
    {
      "id": "zai",
      "name": "Zhipu AI",
      "icon": "@/assets/icons/zai.svg",
      "website": "https://www.zhipuai.cn/",
      "description": "Zhipu AI GLM model family",
      "models": [
        "zai/glm-4.5",
        "zai/glm-4.5-air",
        "zai/glm-4.5v",
        "zai/glm-4.6"
      ]
    }
  ],
  "providers": [
    {
      "id": "alibabacloud",
      "name": "Alibaba Cloud",
      "icon": "@/assets/icons/alibabacloud.svg",
      "website": "https://www.alibabacloud.com/",
      "requires_api_key": "ALIBABACLOUD_ACCESS_KEY_ID",
      "api_key_register_url": "https://account.alibabacloud.com/register",
      "description": "Alibaba Cloud's AI and model hosting services",
      "provider_type": "cloud",
      "models": [
        {
          "id": "alibaba/qwen3-coder-plus",
          "maker": "qwen",
          "context": 1000000,
          "input_cost_per_million": 1,
          "output_cost_per_million": 5
        },
        {
          "id": "alibaba/qwen3-max",
          "maker": "qwen",
          "context": 262000,
          "input_cost_per_million": 1.2,
          "output_cost_per_million": 6
        },
        {
          "id": "alibaba/qwen3-max-preview",
          "maker": "qwen",
          "context": 262000,
          "input_cost_per_million": 1.2,
          "output_cost_per_million": 6
        },
        {
          "id": "alibaba/qwen3-next-80b-a3b-instruct",
          "maker": "qwen",
          "context": 131000,
          "input_cost_per_million": 0.15,
          "output_cost_per_million": 1.5
        },
        {
          "id": "alibaba/qwen3-next-80b-a3b-thinking",
          "maker": "qwen",
          "context": 131000,
          "input_cost_per_million": 0.15,
          "output_cost_per_million": 1.5
        },
        {
          "id": "alibaba/qwen3-vl-instruct",
          "maker": "qwen",
          "context": 131000,
          "input_cost_per_million": 0.7,
          "output_cost_per_million": 2.8
        },
        {
          "id": "alibaba/qwen3-vl-thinking",
          "maker": "qwen",
          "context": 131000,
          "input_cost_per_million": 0.7,
          "output_cost_per_million": 8.4
        }
      ]
    },
    {
      "id": "anthropic",
      "name": "Anthropic",
      "icon": "@/assets/icons/anthropic.svg",
      "website": "https://console.anthropic.com/",
      "requires_api_key": "ANTHROPIC_API_KEY",
      "api_key_register_url": "https://console.anthropic.com/",
      "description": "Official Anthropic API for Claude models",
      "provider_type": "api",
      "models": [
        {
          "id": "anthropic/claude-3-haiku",
          "maker": "anthropic",
          "context": 200000,
          "input_cost_per_million": 0.25,
          "output_cost_per_million": 1.25
        },
        {
          "id": "anthropic/claude-3-opus",
          "maker": "anthropic",
          "context": 200000,
          "input_cost_per_million": 15,
          "output_cost_per_million": 75
        },
        {
          "id": "anthropic/claude-3.5-haiku",
          "maker": "anthropic",
          "context": 200000,
          "input_cost_per_million": 0.8,
          "output_cost_per_million": 4
        },
        {
          "id": "anthropic/claude-3.7-sonnet",
          "maker": "anthropic",
          "context": 200000,
          "input_cost_per_million": 3,
          "output_cost_per_million": 15
        },
        {
          "id": "anthropic/claude-haiku-4.5",
          "maker": "anthropic",
          "context": 200000,
          "input_cost_per_million": 1,
          "output_cost_per_million": 5
        },
        {
          "id": "anthropic/claude-opus-4",
          "maker": "anthropic",
          "context": 200000,
          "input_cost_per_million": 15,
          "output_cost_per_million": 75
        },
        {
          "id": "anthropic/claude-opus-4.1",
          "maker": "anthropic",
          "context": 200000,
          "input_cost_per_million": 15,
          "output_cost_per_million": 75
        },
        {
          "id": "anthropic/claude-sonnet-4",
          "maker": "anthropic",
          "context": 200000,
          "input_cost_per_million": 3,
          "output_cost_per_million": 15
        },
        {
          "id": "anthropic/claude-sonnet-4.5",
          "maker": "anthropic",
          "context": 200000,
          "input_cost_per_million": 3,
          "output_cost_per_million": 15
        }
      ]
    },
    {
      "id": "aws-bedrock",
      "name": "AWS Bedrock",
      "icon": "@/assets/icons/aws.svg",
      "website": "https://aws.amazon.com/bedrock/",
      "requires_api_key": "AWS_ACCESS_KEY_ID",
      "api_key_register_url": "https://aws.amazon.com/bedrock/",
      "description": "Amazon's managed AI service with multiple models",
      "provider_type": "cloud",
      "models": [
        {
          "id": "alibaba/qwen-3-32b",
          "maker": "qwen",
          "context": 128000,
          "input_cost_per_million": 0.1,
          "output_cost_per_million": 0.3
        },
        {
          "id": "alibaba/qwen3-coder-30b-a3b",
          "maker": "qwen",
          "context": 262000,
          "input_cost_per_million": 0.15,
          "output_cost_per_million": 0.6
        },
        {
          "id": "amazon/nova-lite",
          "maker": "amazon",
          "context": 300000,
          "input_cost_per_million": 0.06,
          "output_cost_per_million": 0.24
        },
        {
          "id": "amazon/nova-micro",
          "maker": "amazon",
          "context": 128000,
          "input_cost_per_million": 0.04,
          "output_cost_per_million": 0.14
        },
        {
          "id": "amazon/nova-pro",
          "maker": "amazon",
          "context": 300000,
          "input_cost_per_million": 0.8,
          "output_cost_per_million": 3.2
        },
        {
          "id": "amazon/titan-embed-text-v2",
          "maker": "amazon",
          "context": null,
          "input_cost_per_million": 0.02,
          "output_cost_per_million": null
        },
        {
          "id": "anthropic/claude-3-haiku",
          "maker": "anthropic",
          "context": 200000,
          "input_cost_per_million": 0.25,
          "output_cost_per_million": 1.25
        },
        {
          "id": "anthropic/claude-3-opus",
          "maker": "anthropic",
          "context": 200000,
          "input_cost_per_million": 15,
          "output_cost_per_million": 75
        },
        {
          "id": "anthropic/claude-3.5-haiku",
          "maker": "anthropic",
          "context": 200000,
          "input_cost_per_million": 0.8,
          "output_cost_per_million": 4
        },
        {
          "id": "anthropic/claude-3.5-sonnet",
          "maker": "anthropic",
          "context": 200000,
          "input_cost_per_million": 3,
          "output_cost_per_million": 15
        },
        {
          "id": "anthropic/claude-3.5-sonnet-20240620",
          "maker": "anthropic",
          "context": 200000,
          "input_cost_per_million": 3,
          "output_cost_per_million": 15
        },
        {
          "id": "anthropic/claude-3.7-sonnet",
          "maker": "anthropic",
          "context": 200000,
          "input_cost_per_million": 3,
          "output_cost_per_million": 15
        },
        {
          "id": "anthropic/claude-haiku-4.5",
          "maker": "anthropic",
          "context": 200000,
          "input_cost_per_million": 1,
          "output_cost_per_million": 5
        },
        {
          "id": "anthropic/claude-opus-4",
          "maker": "anthropic",
          "context": 200000,
          "input_cost_per_million": 15,
          "output_cost_per_million": 75
        },
        {
          "id": "anthropic/claude-opus-4.1",
          "maker": "anthropic",
          "context": 200000,
          "input_cost_per_million": 15,
          "output_cost_per_million": 75
        },
        {
          "id": "anthropic/claude-sonnet-4",
          "maker": "anthropic",
          "context": 200000,
          "input_cost_per_million": 3,
          "output_cost_per_million": 15
        },
        {
          "id": "anthropic/claude-sonnet-4.5",
          "maker": "anthropic",
          "context": 200000,
          "input_cost_per_million": 3,
          "output_cost_per_million": 15
        },
        {
          "id": "deepseek/deepseek-r1",
          "maker": "deepseek",
          "context": 164000,
          "input_cost_per_million": 0.79,
          "output_cost_per_million": 4
        },
        {
          "id": "meta/llama-3.1-70b",
          "maker": "meta",
          "context": 128000,
          "input_cost_per_million": 0.72,
          "output_cost_per_million": 0.72
        },
        {
          "id": "meta/llama-3.1-8b",
          "maker": "meta",
          "context": 128000,
          "input_cost_per_million": 0.05,
          "output_cost_per_million": 0.08
        },
        {
          "id": "meta/llama-3.2-11b",
          "maker": "meta",
          "context": 128000,
          "input_cost_per_million": 0.16,
          "output_cost_per_million": 0.16
        },
        {
          "id": "meta/llama-3.2-1b",
          "maker": "meta",
          "context": 128000,
          "input_cost_per_million": 0.1,
          "output_cost_per_million": 0.1
        },
        {
          "id": "meta/llama-3.2-3b",
          "maker": "meta",
          "context": 128000,
          "input_cost_per_million": 0.15,
          "output_cost_per_million": 0.15
        },
        {
          "id": "meta/llama-3.2-90b",
          "maker": "meta",
          "context": 128000,
          "input_cost_per_million": 0.72,
          "output_cost_per_million": 0.72
        },
        {
          "id": "meta/llama-3.3-70b",
          "maker": "meta",
          "context": 128000,
          "input_cost_per_million": 0.72,
          "output_cost_per_million": 0.72
        },
        {
          "id": "meta/llama-4-maverick",
          "maker": "meta",
          "context": 1300000,
          "input_cost_per_million": 0.15,
          "output_cost_per_million": 0.6
        },
        {
          "id": "meta/llama-4-scout",
          "maker": "meta",
          "context": 128000,
          "input_cost_per_million": 0.08,
          "output_cost_per_million": 0.3
        },
        {
          "id": "openai/gpt-oss-120b",
          "maker": "openai",
          "context": 131000,
          "input_cost_per_million": 0.1,
          "output_cost_per_million": 0.5
        },
        {
          "id": "openai/gpt-oss-20b",
          "maker": "openai",
          "context": 128000,
          "input_cost_per_million": 0.07,
          "output_cost_per_million": 0.3
        }
      ]
    },
    {
      "id": "azure",
      "name": "Azure OpenAI",
      "icon": "@/assets/icons/azure.svg",
      "website": "https://azure.microsoft.com/en-us/products/ai-services/openai-service",
      "requires_api_key": "AZURE_OPENAI_API_KEY",
      "api_key_register_url": "https://portal.azure.com/",
      "description": "Microsoft's managed OpenAI service on Azure",
      "provider_type": "cloud",
      "models": [
        {
          "id": "openai/gpt-4.1",
          "maker": "openai",
          "context": 1000000,
          "input_cost_per_million": 2,
          "output_cost_per_million": 8
        },
        {
          "id": "openai/gpt-4.1-mini",
          "maker": "openai",
          "context": 1000000,
          "input_cost_per_million": 0.4,
          "output_cost_per_million": 1.6
        },
        {
          "id": "openai/gpt-4.1-nano",
          "maker": "openai",
          "context": 1000000,
          "input_cost_per_million": 0.1,
          "output_cost_per_million": 0.4
        },
        {
          "id": "openai/gpt-4o",
          "maker": "openai",
          "context": 128000,
          "input_cost_per_million": 2.5,
          "output_cost_per_million": 10
        },
        {
          "id": "openai/gpt-4o-mini",
          "maker": "openai",
          "context": 128000,
          "input_cost_per_million": 0.15,
          "output_cost_per_million": 0.6
        },
        {
          "id": "openai/gpt-5",
          "maker": "openai",
          "context": 400000,
          "input_cost_per_million": 1.25,
          "output_cost_per_million": 10
        },
        {
          "id": "openai/gpt-5-codex",
          "maker": "openai",
          "context": 400000,
          "input_cost_per_million": 1.25,
          "output_cost_per_million": 10
        },
        {
          "id": "openai/gpt-5-mini",
          "maker": "openai",
          "context": 400000,
          "input_cost_per_million": 0.25,
          "output_cost_per_million": 2
        },
        {
          "id": "openai/gpt-5-nano",
          "maker": "openai",
          "context": 400000,
          "input_cost_per_million": 0.05,
          "output_cost_per_million": 0.4
        },
        {
          "id": "openai/o1",
          "maker": "openai",
          "context": 200000,
          "input_cost_per_million": 15,
          "output_cost_per_million": 60
        },
        {
          "id": "openai/o3-mini",
          "maker": "openai",
          "context": 200000,
          "input_cost_per_million": 1.1,
          "output_cost_per_million": 4.4
        },
        {
          "id": "openai/o4-mini",
          "maker": "openai",
          "context": 200000,
          "input_cost_per_million": 1.1,
          "output_cost_per_million": 4.4
        },
        {
          "id": "openai/text-embedding-3-large",
          "maker": "openai",
          "context": null,
          "input_cost_per_million": 0.13,
          "output_cost_per_million": null
        },
        {
          "id": "openai/text-embedding-3-small",
          "maker": "openai",
          "context": null,
          "input_cost_per_million": 0.02,
          "output_cost_per_million": null
        },
        {
          "id": "openai/text-embedding-ada-002",
          "maker": "openai",
          "context": null,
          "input_cost_per_million": 0.1,
          "output_cost_per_million": null
        }
      ]
    },
    {
      "id": "baseten",
      "name": "Baseten",
      "icon": "@/assets/icons/baseten.svg",
      "website": "https://www.baseten.co/",
      "requires_api_key": "BASETEN_API_KEY",
      "api_key_register_url": "https://www.baseten.co/",
      "description": "Managed platform for deploying and scaling AI models",
      "provider_type": "api",
      "models": [
        {
          "id": "alibaba/qwen-3-235b",
          "maker": "qwen",
          "context": 262000,
          "input_cost_per_million": 0.13,
          "output_cost_per_million": 0.6
        },
        {
          "id": "alibaba/qwen3-coder",
          "maker": "qwen",
          "context": 131000,
          "input_cost_per_million": 0.4,
          "output_cost_per_million": 1.6
        },
        {
          "id": "deepseek/deepseek-v3",
          "maker": "deepseek",
          "context": 164000,
          "input_cost_per_million": 0.77,
          "output_cost_per_million": 0.77
        },
        {
          "id": "deepseek/deepseek-v3.1",
          "maker": "deepseek",
          "context": 164000,
          "input_cost_per_million": 0.3,
          "output_cost_per_million": 1
        },
        {
          "id": "moonshotai/kimi-k2-0905",
          "maker": "moonshotai",
          "context": 131000,
          "input_cost_per_million": 0.6,
          "output_cost_per_million": 2.5
        },
        {
          "id": "openai/gpt-oss-120b",
          "maker": "openai",
          "context": 131000,
          "input_cost_per_million": 0.1,
          "output_cost_per_million": 0.5
        }
      ]
    },
    {
      "id": "cerebras",
      "name": "Cerebras",
      "icon": "@/assets/icons/cerebras.svg",
      "website": "https://inference.cerebras.ai/",
      "requires_api_key": "CEREBRAS_API_KEY",
      "api_key_register_url": "https://inference.cerebras.ai/signup",
      "description": "High-performance inference on Cerebras CS hardware",
      "provider_type": "api",
      "models": [
        {
          "id": "alibaba/qwen-3-32b",
          "maker": "qwen",
          "context": 128000,
          "input_cost_per_million": 0.1,
          "output_cost_per_million": 0.3
        },
        {
          "id": "alibaba/qwen3-coder",
          "maker": "qwen",
          "context": 131000,
          "input_cost_per_million": 0.4,
          "output_cost_per_million": 1.6
        },
        {
          "id": "meta/llama-3.1-8b",
          "maker": "meta",
          "context": 128000,
          "input_cost_per_million": 0.05,
          "output_cost_per_million": 0.08
        },
        {
          "id": "meta/llama-3.3-70b",
          "maker": "meta",
          "context": 128000,
          "input_cost_per_million": 0.72,
          "output_cost_per_million": 0.72
        },
        {
          "id": "meta/llama-4-scout",
          "maker": "meta",
          "context": 128000,
          "input_cost_per_million": 0.08,
          "output_cost_per_million": 0.3
        },
        {
          "id": "openai/gpt-oss-120b",
          "maker": "openai",
          "context": 131000,
          "input_cost_per_million": 0.1,
          "output_cost_per_million": 0.5
        }
      ]
    },
    {
      "id": "cohere",
      "name": "Cohere",
      "icon": "@/assets/icons/cohere.svg",
      "website": "https://cohere.com/",
      "requires_api_key": "COHERE_API_KEY",
      "api_key_register_url": "https://dashboard.cohere.com/signup",
      "description": "Enterprise-grade language model APIs from Cohere",
      "provider_type": "api",
      "models": [
        {
          "id": "cohere/command-a",
          "maker": "cohere",
          "context": 256000,
          "input_cost_per_million": 2.5,
          "output_cost_per_million": 10
        },
        {
          "id": "cohere/embed-v4.0",
          "maker": "cohere",
          "context": null,
          "input_cost_per_million": 0.12,
          "output_cost_per_million": null
        }
      ]
    },
    {
      "id": "deepinfra",
      "name": "DeepInfra",
      "icon": "@/assets/icons/deepinfra.svg",
      "website": "https://deepinfra.com/",
      "requires_api_key": "DEEPINFRA_API_KEY",
      "api_key_register_url": "https://deepinfra.com/signup",
      "description": "Serverless GPU inference for open-source models",
      "provider_type": "api",
      "models": [
        {
          "id": "alibaba/qwen-3-14b",
          "maker": "qwen",
          "context": 41000,
          "input_cost_per_million": 0.06,
          "output_cost_per_million": 0.24
        },
        {
          "id": "alibaba/qwen-3-235b",
          "maker": "qwen",
          "context": 262000,
          "input_cost_per_million": 0.13,
          "output_cost_per_million": 0.6
        },
        {
          "id": "alibaba/qwen-3-30b",
          "maker": "qwen",
          "context": 41000,
          "input_cost_per_million": 0.08,
          "output_cost_per_million": 0.29
        },
        {
          "id": "alibaba/qwen-3-32b",
          "maker": "qwen",
          "context": 128000,
          "input_cost_per_million": 0.1,
          "output_cost_per_million": 0.3
        },
        {
          "id": "alibaba/qwen3-coder",
          "maker": "qwen",
          "context": 131000,
          "input_cost_per_million": 0.4,
          "output_cost_per_million": 1.6
        },
        {
          "id": "deepseek/deepseek-v3.1",
          "maker": "deepseek",
          "context": 164000,
          "input_cost_per_million": 0.3,
          "output_cost_per_million": 1
        },
        {
          "id": "meta/llama-4-maverick",
          "maker": "meta",
          "context": 1300000,
          "input_cost_per_million": 0.15,
          "output_cost_per_million": 0.6
        },
        {
          "id": "meta/llama-4-scout",
          "maker": "meta",
          "context": 128000,
          "input_cost_per_million": 0.08,
          "output_cost_per_million": 0.3
        },
        {
          "id": "moonshotai/kimi-k2",
          "maker": "moonshotai",
          "context": 131000,
          "input_cost_per_million": 0.5,
          "output_cost_per_million": 2
        }
      ]
    },
    {
      "id": "deepseek",
      "name": "DeepSeek",
      "icon": "@/assets/icons/deepseek.svg",
      "website": "https://platform.deepseek.com/",
      "requires_api_key": "DEEPSEEK_API_KEY",
      "api_key_register_url": "https://platform.deepseek.com/",
      "description": "Official DeepSeek API",
      "provider_type": "api",
      "models": [
        {
          "id": "deepseek/deepseek-v3.2-exp",
          "maker": "deepseek",
          "context": 164000,
          "input_cost_per_million": 0.27,
          "output_cost_per_million": 0.41
        },
        {
          "id": "deepseek/deepseek-v3.2-exp-thinking",
          "maker": "deepseek",
          "context": 164000,
          "input_cost_per_million": 0.28,
          "output_cost_per_million": 0.42
        }
      ]
    },
    {
      "id": "fireworks",
      "name": "Fireworks AI",
      "icon": "@/assets/icons/fireworks.svg",
      "website": "https://fireworks.ai/",
      "requires_api_key": "FIREWORKS_API_KEY",
      "api_key_register_url": "https://app.fireworks.ai/",
      "description": "High-performance inference APIs for open and proprietary models",
      "provider_type": "api",
      "models": [
        {
          "id": "alibaba/qwen-3-235b",
          "maker": "qwen",
          "context": 262000,
          "input_cost_per_million": 0.13,
          "output_cost_per_million": 0.6
        },
        {
          "id": "deepseek/deepseek-v3.1",
          "maker": "deepseek",
          "context": 164000,
          "input_cost_per_million": 0.3,
          "output_cost_per_million": 1
        },
        {
          "id": "mistral/mixtral-8x22b-instruct",
          "maker": "mistral",
          "context": 66000,
          "input_cost_per_million": 1.2,
          "output_cost_per_million": 1.2
        },
        {
          "id": "moonshotai/kimi-k2",
          "maker": "moonshotai",
          "context": 131000,
          "input_cost_per_million": 0.5,
          "output_cost_per_million": 2
        },
        {
          "id": "moonshotai/kimi-k2-0905",
          "maker": "moonshotai",
          "context": 131000,
          "input_cost_per_million": 0.6,
          "output_cost_per_million": 2.5
        },
        {
          "id": "openai/gpt-oss-120b",
          "maker": "openai",
          "context": 131000,
          "input_cost_per_million": 0.1,
          "output_cost_per_million": 0.5
        },
        {
          "id": "openai/gpt-oss-20b",
          "maker": "openai",
          "context": 128000,
          "input_cost_per_million": 0.07,
          "output_cost_per_million": 0.3
        }
      ]
    },
    {
      "id": "gemini",
      "name": "Google Gemini",
      "icon": "@/assets/icons/gemini.svg",
      "website": "https://ai.google.dev/",
      "requires_api_key": "GEMINI_API_KEY",
      "api_key_register_url": "https://makersuite.google.com/app/apikey",
      "description": "Official Google Gemini API",
      "provider_type": "api"
    },
    {
      "id": "groq",
      "name": "Groq",
      "icon": "@/assets/icons/groq.svg",
      "website": "https://console.groq.com/",
      "requires_api_key": "GROQ_API_KEY",
      "api_key_register_url": "https://console.groq.com/keys",
      "description": "Ultra-fast inference on Groq LPU hardware",
      "provider_type": "api",
      "models": [
        {
          "id": "alibaba/qwen-3-32b",
          "maker": "qwen",
          "context": 128000,
          "input_cost_per_million": 0.1,
          "output_cost_per_million": 0.3
        },
        {
          "id": "meta/llama-3.1-8b",
          "maker": "meta",
          "context": 128000,
          "input_cost_per_million": 0.05,
          "output_cost_per_million": 0.08
        },
        {
          "id": "meta/llama-3.3-70b",
          "maker": "meta",
          "context": 128000,
          "input_cost_per_million": 0.72,
          "output_cost_per_million": 0.72
        },
        {
          "id": "meta/llama-4-scout",
          "maker": "meta",
          "context": 128000,
          "input_cost_per_million": 0.08,
          "output_cost_per_million": 0.3
        },
        {
          "id": "moonshotai/kimi-k2",
          "maker": "moonshotai",
          "context": 131000,
          "input_cost_per_million": 0.5,
          "output_cost_per_million": 2
        },
        {
          "id": "moonshotai/kimi-k2-0905",
          "maker": "moonshotai",
          "context": 131000,
          "input_cost_per_million": 0.6,
          "output_cost_per_million": 2.5
        },
        {
          "id": "openai/gpt-oss-120b",
          "maker": "openai",
          "context": 131000,
          "input_cost_per_million": 0.1,
          "output_cost_per_million": 0.5
        },
        {
          "id": "openai/gpt-oss-20b",
          "maker": "openai",
          "context": 128000,
          "input_cost_per_million": 0.07,
          "output_cost_per_million": 0.3
        },
        {
          "id": "openai/gpt-oss-safeguard-20b",
          "maker": "openai",
          "context": 131000,
          "input_cost_per_million": 0.07,
          "output_cost_per_million": 0.3
        }
      ]
    },
    {
      "id": "llama-cpp",
      "name": "llama.cpp",
      "icon": "@/assets/icons/llama-cpp.svg",
      "website": "https://github.com/ggerganov/llama.cpp",
      "description": "Local inference engine for GGUF models",
      "provider_type": "local"
    },
    {
      "id": "mistral",
      "name": "Mistral AI",
      "icon": "@/assets/icons/mistral.svg",
      "website": "https://console.mistral.ai/",
      "requires_api_key": "MISTRAL_API_KEY",
      "api_key_register_url": "https://console.mistral.ai/",
      "description": "Official Mistral AI API",
      "provider_type": "api",
      "models": [
        {
          "id": "mistral/codestral",
          "maker": "mistral",
          "context": 256000,
          "input_cost_per_million": 0.3,
          "output_cost_per_million": 0.9
        },
        {
          "id": "mistral/codestral-embed",
          "maker": "mistral",
          "context": null,
          "input_cost_per_million": 0.15,
          "output_cost_per_million": null
        },
        {
          "id": "mistral/devstral-small",
          "maker": "mistral",
          "context": 128000,
          "input_cost_per_million": 0.1,
          "output_cost_per_million": 0.3
        },
        {
          "id": "mistral/magistral-medium",
          "maker": "mistral",
          "context": 128000,
          "input_cost_per_million": 2,
          "output_cost_per_million": 5
        },
        {
          "id": "mistral/magistral-medium-2506",
          "maker": "mistral",
          "context": 128000,
          "input_cost_per_million": 2,
          "output_cost_per_million": 5
        },
        {
          "id": "mistral/magistral-small",
          "maker": "mistral",
          "context": 128000,
          "input_cost_per_million": 0.5,
          "output_cost_per_million": 1.5
        },
        {
          "id": "mistral/magistral-small-2506",
          "maker": "mistral",
          "context": 128000,
          "input_cost_per_million": 0.5,
          "output_cost_per_million": 1.5
        },
        {
          "id": "mistral/ministral-3b",
          "maker": "mistral",
          "context": 128000,
          "input_cost_per_million": 0.04,
          "output_cost_per_million": 0.04
        },
        {
          "id": "mistral/ministral-8b",
          "maker": "mistral",
          "context": 128000,
          "input_cost_per_million": 0.1,
          "output_cost_per_million": 0.1
        },
        {
          "id": "mistral/mistral-embed",
          "maker": "mistral",
          "context": null,
          "input_cost_per_million": 0.1,
          "output_cost_per_million": null
        },
        {
          "id": "mistral/mistral-large",
          "maker": "mistral",
          "context": 32000,
          "input_cost_per_million": 2,
          "output_cost_per_million": 6
        },
        {
          "id": "mistral/mistral-medium",
          "maker": "mistral",
          "context": 128000,
          "input_cost_per_million": 0.4,
          "output_cost_per_million": 2
        },
        {
          "id": "mistral/mistral-small",
          "maker": "mistral",
          "context": 32000,
          "input_cost_per_million": 0.1,
          "output_cost_per_million": 0.3
        },
        {
          "id": "mistral/pixtral-12b",
          "maker": "mistral",
          "context": 128000,
          "input_cost_per_million": 0.15,
          "output_cost_per_million": 0.15
        },
        {
          "id": "mistral/pixtral-large",
          "maker": "mistral",
          "context": 128000,
          "input_cost_per_million": 2,
          "output_cost_per_million": 6
        }
      ]
    },
    {
      "id": "moonshot",
      "name": "Moonshot AI",
      "icon": "@/assets/icons/moonshot.svg",
      "website": "https://moonshot.cn/",
      "requires_api_key": "MOONSHOT_API_KEY",
      "api_key_register_url": "https://moonshot.cn/",
      "description": "Access to Moonshot AI's Kimi large language models",
      "provider_type": "api",
      "models": [
        {
          "id": "moonshotai/kimi-k2",
          "maker": "moonshotai",
          "context": 131000,
          "input_cost_per_million": 0.5,
          "output_cost_per_million": 2
        },
        {
          "id": "moonshotai/kimi-k2-0905",
          "maker": "moonshotai",
          "context": 131000,
          "input_cost_per_million": 0.6,
          "output_cost_per_million": 2.5
        },
        {
          "id": "moonshotai/kimi-k2-turbo",
          "maker": "moonshotai",
          "context": 256000,
          "input_cost_per_million": 2.4,
          "output_cost_per_million": 10
        }
      ]
    },
    {
      "id": "novita",
      "name": "Novita AI",
      "icon": "@/assets/icons/novita.svg",
      "website": "https://novita.ai/",
      "requires_api_key": "NOVITA_API_KEY",
      "api_key_register_url": "https://novita.ai/",
      "description": "Pay-as-you-go APIs for popular open-source models",
      "provider_type": "api",
      "models": [
        {
          "id": "alibaba/qwen-3-235b",
          "maker": "qwen",
          "context": 262000,
          "input_cost_per_million": 0.13,
          "output_cost_per_million": 0.6
        },
        {
          "id": "alibaba/qwen3-coder",
          "maker": "qwen",
          "context": 131000,
          "input_cost_per_million": 0.4,
          "output_cost_per_million": 1.6
        },
        {
          "id": "alibaba/qwen3-next-80b-a3b-instruct",
          "maker": "qwen",
          "context": 131000,
          "input_cost_per_million": 0.15,
          "output_cost_per_million": 1.5
        },
        {
          "id": "alibaba/qwen3-next-80b-a3b-thinking",
          "maker": "qwen",
          "context": 131000,
          "input_cost_per_million": 0.15,
          "output_cost_per_million": 1.5
        },
        {
          "id": "deepseek/deepseek-v3",
          "maker": "deepseek",
          "context": 164000,
          "input_cost_per_million": 0.77,
          "output_cost_per_million": 0.77
        },
        {
          "id": "deepseek/deepseek-v3.1",
          "maker": "deepseek",
          "context": 164000,
          "input_cost_per_million": 0.3,
          "output_cost_per_million": 1
        },
        {
          "id": "deepseek/deepseek-v3.1-terminus",
          "maker": "deepseek",
          "context": 131000,
          "input_cost_per_million": 0.27,
          "output_cost_per_million": 1
        },
        {
          "id": "deepseek/deepseek-v3.2-exp",
          "maker": "deepseek",
          "context": 164000,
          "input_cost_per_million": 0.27,
          "output_cost_per_million": 0.41
        },
        {
          "id": "moonshotai/kimi-k2",
          "maker": "moonshotai",
          "context": 131000,
          "input_cost_per_million": 0.5,
          "output_cost_per_million": 2
        },
        {
          "id": "zai/glm-4.5",
          "maker": "zai",
          "context": 128000,
          "input_cost_per_million": 0.6,
          "output_cost_per_million": 2.2
        },
        {
          "id": "zai/glm-4.5v",
          "maker": "zai",
          "context": 66000,
          "input_cost_per_million": 0.6,
          "output_cost_per_million": 1.8
        }
      ]
    },
    {
      "id": "openai",
      "name": "OpenAI",
      "icon": "@/assets/icons/openai.svg",
      "website": "https://platform.openai.com/",
      "requires_api_key": "OPENAI_API_KEY",
      "api_key_register_url": "https://platform.openai.com/signup",
      "description": "Official OpenAI API for GPT models",
      "provider_type": "api",
      "models": [
        {
          "id": "openai/gpt-3.5-turbo",
          "maker": "openai",
          "context": 16000,
          "input_cost_per_million": 0.5,
          "output_cost_per_million": 1.5
        },
        {
          "id": "openai/gpt-3.5-turbo-instruct",
          "maker": "openai",
          "context": 8000,
          "input_cost_per_million": 1.5,
          "output_cost_per_million": 2
        },
        {
          "id": "openai/gpt-4-turbo",
          "maker": "openai",
          "context": 128000,
          "input_cost_per_million": 10,
          "output_cost_per_million": 30
        },
        {
          "id": "openai/gpt-4.1",
          "maker": "openai",
          "context": 1000000,
          "input_cost_per_million": 2,
          "output_cost_per_million": 8
        },
        {
          "id": "openai/gpt-4.1-mini",
          "maker": "openai",
          "context": 1000000,
          "input_cost_per_million": 0.4,
          "output_cost_per_million": 1.6
        },
        {
          "id": "openai/gpt-4.1-nano",
          "maker": "openai",
          "context": 1000000,
          "input_cost_per_million": 0.1,
          "output_cost_per_million": 0.4
        },
        {
          "id": "openai/gpt-4o",
          "maker": "openai",
          "context": 128000,
          "input_cost_per_million": 2.5,
          "output_cost_per_million": 10
        },
        {
          "id": "openai/gpt-4o-mini",
          "maker": "openai",
          "context": 128000,
          "input_cost_per_million": 0.15,
          "output_cost_per_million": 0.6
        },
        {
          "id": "openai/gpt-5",
          "maker": "openai",
          "context": 400000,
          "input_cost_per_million": 1.25,
          "output_cost_per_million": 10
        },
        {
          "id": "openai/gpt-5-codex",
          "maker": "openai",
          "context": 400000,
          "input_cost_per_million": 1.25,
          "output_cost_per_million": 10
        },
        {
          "id": "openai/gpt-5-mini",
          "maker": "openai",
          "context": 400000,
          "input_cost_per_million": 0.25,
          "output_cost_per_million": 2
        },
        {
          "id": "openai/gpt-5-nano",
          "maker": "openai",
          "context": 400000,
          "input_cost_per_million": 0.05,
          "output_cost_per_million": 0.4
        },
        {
          "id": "openai/gpt-5-pro",
          "maker": "openai",
          "context": 400000,
          "input_cost_per_million": 15,
          "output_cost_per_million": 120
        },
        {
          "id": "openai/o1",
          "maker": "openai",
          "context": 200000,
          "input_cost_per_million": 15,
          "output_cost_per_million": 60
        },
        {
          "id": "openai/o3",
          "maker": "openai",
          "context": 200000,
          "input_cost_per_million": 2,
          "output_cost_per_million": 8
        },
        {
          "id": "openai/o3-deep-research",
          "maker": "openai",
          "context": 200000,
          "input_cost_per_million": 10,
          "output_cost_per_million": 40
        },
        {
          "id": "openai/o3-mini",
          "maker": "openai",
          "context": 200000,
          "input_cost_per_million": 1.1,
          "output_cost_per_million": 4.4
        },
        {
          "id": "openai/o4-mini",
          "maker": "openai",
          "context": 200000,
          "input_cost_per_million": 1.1,
          "output_cost_per_million": 4.4
        },
        {
          "id": "openai/text-embedding-3-large",
          "maker": "openai",
          "context": null,
          "input_cost_per_million": 0.13,
          "output_cost_per_million": null
        },
        {
          "id": "openai/text-embedding-3-small",
          "maker": "openai",
          "context": null,
          "input_cost_per_million": 0.02,
          "output_cost_per_million": null
        },
        {
          "id": "openai/text-embedding-ada-002",
          "maker": "openai",
          "context": null,
          "input_cost_per_million": 0.1,
          "output_cost_per_million": null
        }
      ]
    },
    {
      "id": "openrouter",
      "name": "OpenRouter",
      "icon": "@/assets/icons/openrouter.svg",
      "website": "https://openrouter.ai/",
      "requires_api_key": "OPENROUTER_API_KEY",
      "api_key_register_url": "https://openrouter.ai/keys",
      "description": "Unified API for multiple AI models",
      "provider_type": "api"
    },
    {
      "id": "parasail",
      "name": "Parasail",
      "icon": "@/assets/icons/parasail.svg",
      "website": "https://parasail.ai/",
      "requires_api_key": "PARASAIL_API_KEY",
      "api_key_register_url": "https://app.parasail.ai/",
      "description": "Low-latency API access to leading open-source models",
      "provider_type": "api",
      "models": [
        {
          "id": "deepseek/deepseek-r1",
          "maker": "deepseek",
          "context": 164000,
          "input_cost_per_million": 0.79,
          "output_cost_per_million": 4
        },
        {
          "id": "moonshotai/kimi-k2",
          "maker": "moonshotai",
          "context": 131000,
          "input_cost_per_million": 0.5,
          "output_cost_per_million": 2
        },
        {
          "id": "openai/gpt-oss-120b",
          "maker": "openai",
          "context": 131000,
          "input_cost_per_million": 0.1,
          "output_cost_per_million": 0.5
        }
      ]
    },
    {
      "id": "perplexity",
      "name": "Perplexity",
      "icon": "@/assets/icons/perplexity.svg",
      "website": "https://www.perplexity.ai/api",
      "requires_api_key": "PERPLEXITY_API_KEY",
      "api_key_register_url": "https://www.perplexity.ai/hub",
      "description": "Perplexity's conversational search and answer API",
      "provider_type": "api",
      "models": [
        {
          "id": "perplexity/sonar",
          "maker": "perplexity",
          "context": 127000,
          "input_cost_per_million": 1,
          "output_cost_per_million": 1
        },
        {
          "id": "perplexity/sonar-pro",
          "maker": "perplexity",
          "context": 200000,
          "input_cost_per_million": 3,
          "output_cost_per_million": 15
        },
        {
          "id": "perplexity/sonar-reasoning",
          "maker": "perplexity",
          "context": 127000,
          "input_cost_per_million": 1,
          "output_cost_per_million": 5
        },
        {
          "id": "perplexity/sonar-reasoning-pro",
          "maker": "perplexity",
          "context": 127000,
          "input_cost_per_million": 2,
          "output_cost_per_million": 8
        }
      ]
    },
    {
      "id": "vertex-ai",
      "name": "Vertex AI",
      "icon": "@/assets/icons/vertexai.svg",
      "website": "https://cloud.google.com/vertex-ai",
      "requires_api_key": "GOOGLE_APPLICATION_CREDENTIALS",
      "api_key_register_url": "https://cloud.google.com/vertex-ai",
      "description": "Google Cloud's AI platform",
      "provider_type": "cloud",
      "models": [
        {
          "id": "anthropic/claude-3-haiku",
          "maker": "anthropic",
          "context": 200000,
          "input_cost_per_million": 0.25,
          "output_cost_per_million": 1.25
        },
        {
          "id": "anthropic/claude-3-opus",
          "maker": "anthropic",
          "context": 200000,
          "input_cost_per_million": 15,
          "output_cost_per_million": 75
        },
        {
          "id": "anthropic/claude-3.5-haiku",
          "maker": "anthropic",
          "context": 200000,
          "input_cost_per_million": 0.8,
          "output_cost_per_million": 4
        },
        {
          "id": "anthropic/claude-3.5-sonnet",
          "maker": "anthropic",
          "context": 200000,
          "input_cost_per_million": 3,
          "output_cost_per_million": 15
        },
        {
          "id": "anthropic/claude-3.5-sonnet-20240620",
          "maker": "anthropic",
          "context": 200000,
          "input_cost_per_million": 3,
          "output_cost_per_million": 15
        },
        {
          "id": "anthropic/claude-3.7-sonnet",
          "maker": "anthropic",
          "context": 200000,
          "input_cost_per_million": 3,
          "output_cost_per_million": 15
        },
        {
          "id": "anthropic/claude-haiku-4.5",
          "maker": "anthropic",
          "context": 200000,
          "input_cost_per_million": 1,
          "output_cost_per_million": 5
        },
        {
          "id": "anthropic/claude-opus-4",
          "maker": "anthropic",
          "context": 200000,
          "input_cost_per_million": 15,
          "output_cost_per_million": 75
        },
        {
          "id": "anthropic/claude-opus-4.1",
          "maker": "anthropic",
          "context": 200000,
          "input_cost_per_million": 15,
          "output_cost_per_million": 75
        },
        {
          "id": "anthropic/claude-sonnet-4",
          "maker": "anthropic",
          "context": 200000,
          "input_cost_per_million": 3,
          "output_cost_per_million": 15
        },
        {
          "id": "anthropic/claude-sonnet-4.5",
          "maker": "anthropic",
          "context": 200000,
          "input_cost_per_million": 3,
          "output_cost_per_million": 15
        },
        {
          "id": "google/gemini-2.0-flash",
          "maker": "google",
          "context": 1000000,
          "input_cost_per_million": 0.1,
          "output_cost_per_million": 0.4
        },
        {
          "id": "google/gemini-2.0-flash-lite",
          "maker": "google",
          "context": 1000000,
          "input_cost_per_million": 0.07,
          "output_cost_per_million": 0.3
        },
        {
          "id": "google/gemini-2.5-flash",
          "maker": "google",
          "context": 1000000,
          "input_cost_per_million": 0.3,
          "output_cost_per_million": 2.5
        },
        {
          "id": "google/gemini-2.5-flash-image",
          "maker": "google",
          "context": 33000,
          "input_cost_per_million": 0.3,
          "output_cost_per_million": 2.5
        },
        {
          "id": "google/gemini-2.5-flash-image-preview",
          "maker": "google",
          "context": 33000,
          "input_cost_per_million": 0.3,
          "output_cost_per_million": 2.5
        },
        {
          "id": "google/gemini-2.5-flash-lite",
          "maker": "google",
          "context": 1000000,
          "input_cost_per_million": 0.1,
          "output_cost_per_million": 0.4
        },
        {
          "id": "google/gemini-2.5-pro",
          "maker": "google",
          "context": 1000000,
          "input_cost_per_million": 1.25,
          "output_cost_per_million": 10
        },
        {
          "id": "google/gemini-embedding-001",
          "maker": "google",
          "context": null,
          "input_cost_per_million": 0.15,
          "output_cost_per_million": null
        },
        {
          "id": "google/text-embedding-005",
          "maker": "google",
          "context": null,
          "input_cost_per_million": 0.03,
          "output_cost_per_million": null
        },
        {
          "id": "google/text-multilingual-embedding-002",
          "maker": "google",
          "context": null,
          "input_cost_per_million": 0.03,
          "output_cost_per_million": null
        },
        {
          "id": "meta/llama-4-maverick",
          "maker": "meta",
          "context": 1300000,
          "input_cost_per_million": 0.15,
          "output_cost_per_million": 0.6
        },
        {
          "id": "meta/llama-4-scout",
          "maker": "meta",
          "context": 128000,
          "input_cost_per_million": 0.08,
          "output_cost_per_million": 0.3
        }
      ]
    },
    {
      "id": "voyage",
      "name": "Voyage AI",
      "icon": "@/assets/icons/voyage.svg",
      "website": "https://www.voyageai.com/",
      "requires_api_key": "VOYAGE_API_KEY",
      "api_key_register_url": "https://dash.voyageai.com/",
      "description": "Domain-specialized embedding APIs from Voyage AI",
      "provider_type": "api",
      "models": [
        {
          "id": "voyage/voyage-3.5-lite",
          "maker": "voyage",
          "context": null,
          "input_cost_per_million": 0.02,
          "output_cost_per_million": null
        },
        {
          "id": "voyage/voyage-3.5",
          "maker": "voyage",
          "context": null,
          "input_cost_per_million": 0.06,
          "output_cost_per_million": null
        },
        {
          "id": "voyage/voyage-3-large",
          "maker": "voyage",
          "context": null,
          "input_cost_per_million": 0.18,
          "output_cost_per_million": null
        },
        {
          "id": "voyage/voyage-law-2",
          "maker": "voyage",
          "context": null,
          "input_cost_per_million": 0.12,
          "output_cost_per_million": null
        },
        {
          "id": "voyage/voyage-code-3",
          "maker": "voyage",
          "context": null,
          "input_cost_per_million": 0.18,
          "output_cost_per_million": null
        },
        {
          "id": "voyage/voyage-code-2",
          "maker": "voyage",
          "context": null,
          "input_cost_per_million": 0.12,
          "output_cost_per_million": null
        },
        {
          "id": "voyage/voyage-finance-2",
          "maker": "voyage",
          "context": null,
          "input_cost_per_million": 0.12,
          "output_cost_per_million": null
        }
      ]
    },
    {
      "id": "xai",
      "name": "xAI",
      "icon": "@/assets/icons/xai.svg",
      "website": "https://x.ai/api",
      "requires_api_key": "XAI_API_KEY",
      "api_key_register_url": "https://x.ai/api",
      "description": "Official xAI API for Grok models",
      "provider_type": "api",
      "models": [
        {
          "id": "xai/grok-2",
          "maker": "xai",
          "context": 131000,
          "input_cost_per_million": 2,
          "output_cost_per_million": 10
        },
        {
          "id": "xai/grok-2-vision",
          "maker": "xai",
          "context": 33000,
          "input_cost_per_million": 2,
          "output_cost_per_million": 10
        },
        {
          "id": "xai/grok-3",
          "maker": "xai",
          "context": 131000,
          "input_cost_per_million": 3,
          "output_cost_per_million": 15
        },
        {
          "id": "xai/grok-3-fast",
          "maker": "xai",
          "context": 131000,
          "input_cost_per_million": 5,
          "output_cost_per_million": 25
        },
        {
          "id": "xai/grok-3-mini",
          "maker": "xai",
          "context": 131000,
          "input_cost_per_million": 0.3,
          "output_cost_per_million": 0.5
        },
        {
          "id": "xai/grok-3-mini-fast",
          "maker": "xai",
          "context": 131000,
          "input_cost_per_million": 0.6,
          "output_cost_per_million": 4
        },
        {
          "id": "xai/grok-4",
          "maker": "xai",
          "context": 256000,
          "input_cost_per_million": 3,
          "output_cost_per_million": 15
        },
        {
          "id": "xai/grok-4-fast-non-reasoning",
          "maker": "xai",
          "context": 2000000,
          "input_cost_per_million": 0.2,
          "output_cost_per_million": 0.5
        },
        {
          "id": "xai/grok-4-fast-reasoning",
          "maker": "xai",
          "context": 2000000,
          "input_cost_per_million": 0.2,
          "output_cost_per_million": 0.5
        },
        {
          "id": "xai/grok-code-fast-1",
          "maker": "xai",
          "context": 256000,
          "input_cost_per_million": 0.2,
          "output_cost_per_million": 1.5
        }
      ]
    },
    {
      "id": "zai",
      "name": "Z.ai",
      "icon": "@/assets/icons/zai.svg",
      "website": "https://z.ai/",
      "requires_api_key": "ZAI_API_KEY",
      "api_key_register_url": "https://z.ai/",
      "description": "Z.ai's unified API for leading frontier models",
      "provider_type": "api",
      "models": [
        {
          "id": "zai/glm-4.5",
          "maker": "zai",
          "context": 128000,
          "input_cost_per_million": 0.6,
          "output_cost_per_million": 2.2
        },
        {
          "id": "zai/glm-4.5-air",
          "maker": "zai",
          "context": 128000,
          "input_cost_per_million": 0.2,
          "output_cost_per_million": 1.1
        },
        {
          "id": "zai/glm-4.5v",
          "maker": "zai",
          "context": 66000,
          "input_cost_per_million": 0.6,
          "output_cost_per_million": 1.8
        },
        {
          "id": "zai/glm-4.6",
          "maker": "zai",
          "context": 200000,
          "input_cost_per_million": 0.45,
          "output_cost_per_million": 1.8
        }
      ]
    }
  ]
}
